{
  "hash": "552cbeb3c0494fcbb55aafb42120b3ed",
  "result": {
    "engine": "knitr",
    "markdown": "# Quantifying Results: p-Value {-}\n\nIn addition to computing the range of likely results from the model, statisticians also typically provide a quantification of the likelihood of the observed result given the hypothesized model. This quantification is referred to as a *p*-value (the *p* stands for probability). \n\nTo compute a *p*-value, you count the number of results that are at least as extreme as the observed result, and divide this by the total number of results. \n\n$$\np = \\frac{\\mathrm{number~of~results~at~least~as~extreme~as~observed~result}}{\\mathrm{total~number~of~simulated~results}}\n$$\n\nThis value is then reported as a decimal value. It quantifies the probability of observing a result at least as extreme as the observed result under the hypothesized model.\n\nTo illustrate this, we will re-examine simulation results from the *Sleep Deprivation* study. Recall in that activity, the observed data had a difference in means of 15.9. Below is a plot of 100 differences in means simulated under the \"no-effect\" model. A vertical line is shown at the observed difference of 15.9.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/sleep-deprivation-simulation.png){fig-align='center' fig-alt='Plot of results from the sleep deprivation study.' width=70%}\n:::\n:::\n\n\n\n\nSince 15.9 is to the right of 0 (i.e., it is on the right-hand side of the plot) results that are *more extreme than the observed result* are to the right of 15.9. (If the observed result was to the left of 0, more extreme results would be those more negative than the observed result.) Here there are two simulated results out of 100 that are at least as extreme as 15.9 ($\\geq 15.9$). We would report the *p*-value as 0.02.\n\n<br />\n\n\n## Adjustment for Simulation Results {-}\n\nIn simulation studies, we make one small adjustment to the *p*-value computation; we add 1 to both the numerator and denominator:\n\n$$\np = \\frac{\\mathrm{number~of~results~more~extreme~than~observed} + 1}{\\mathrm{total~number~of~simulated~results} + 1}\n$$\n\nThis adjustment assures that we never get a *p*-value of 0. Consider the *p*-value if our observed result would have been 18 (instead of 15.9). There are 0 results that are at least as extreme as 18 ($\\geq 18$). Without making the simulation adjustment, we would report a *p*-value of 0. This implies that seeing a result at least as extreme as 18 under the \"no-effect\" model is impossible. The problem is that we only ran 100 trials of the simulation. If we had run this simulation for all possible randomizations of the data, we would have seen results $\\geq 18$. So, to report a *p*-value of 0 is misleading. The *p*-value we report should be,\n\n$$\np = \\frac{0 + 1}{100 + 1} = 0.0099\n$$\n\nAfter the adjustment, the *p*-value is still quite small, indicating that had we seen an observed result of 18, we would say that it is inconsistent with the model of \"no-effect\". In fact it is in the outer 0.01 (1%) of the results simulated from the hypothesized model.\n\nGoing back to the *p*-value computed from the observed value of 15.9,\n\n$$\np = \\frac{2 + 1}{100 + 1} = 0.0297\n$$\n\nWe can interpret the *p*-value of 0.030 as indicating that the observed difference of 15.9 is in the outer 0.03 (3%) of results simulated from the hypothesized model. It is quite unlikely that we would see a result as extreme as 15.9, or more extreme, under the hypothesized model of \"no effect\". \n\n<br />\n\n\n## p-Values as Evidence {-}\n\nLarge *p*-values indicate that the observed data are more compatible with the results from the model, while small *p*-values indicate that the observed data are not very compatible with the results from the model. As researchers, our goal is often to then translate this quantitative evidence into support for the hypothesized model. For example, in the *Sleep Deprivation* study, we obtained a *p*-value of 0.03. This suggests a low degree of compatibility between the observed data (our empirical evidence) and the hypothesized model of \"no effect\". \n\nThe broader scientific question about whether sleep deprivation has a harmful effect on learning, however is difficult to ascertain from a *p*-value. There are no hard-and-fast rules for gauging how strong the evidence is against the hypothesized model because what counts as evidence in one scientific discipline or context may not count in another scientific discipline or context. And, even if there were rules about the degree of evidence needed, you would still need to evaluate other criteria such as internal and external validity evidence, and sample size. Even then, the results from a single study are not typically convincing enough for most scientists to draw definitive conclusions. It is only through consistent findings that emerge from multiple studies (which may take decades) that we may have convincing evidence about the answer to the broader scientific question.\n\n<br />\n\n\n## Six Principles about p-Values {-}\n\nBecause they are so ubiquitous in the research literature for any field, and because they are often mis-interpreted (even by PhDs, researchers, and math teachers) it is important to be aware of what a *p*-value tells you, and more importantly, what it doesn't tell you. To this end, the American Statistical Association released a [statement on *p*-values](http://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1154108#.Vt2XIOaE2MN) in which it listed six principles:^[Yaddanapudi (2016) [published a paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5187603/) in the *Journal of Anaesthesiology, Clinical Pharmacology* in which she explains each of these six principles for practicing physician-scientists using an example of treatment efficacy for a drug.]\n\n- **Principle 1:** *p*-values can indicate how incompatible the data are with a specified statistical model.\n- **Principle 2:** *p*-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\n- **Principle 3:** Scientific conclusions and business or policy decisions should not be based only on whether a *p*-value passes a specific threshold.\n- **Principle 4:** Proper inference requires full reporting and transparency.\n- **Principle 5:** A *p*-value does not measure the size of an effect or the importance of a result.\n- **Principle 6:** By itself, a *p*-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n<!-- Principle 3 is especially poignant, as many reserachers use the criterion of 0.05 when evaluating *p*-values to indicate whether a result is statistically significant ($p \\leq 0.05$) or not. In fact, this use was so pervasive and problematic, that the journal *Basic and Applied Social Psychology* [banned the use of *p*-values](http://www.nature.com/news/psychology-journal-bans-p-values-1.17001) in its published articles. It also led to many jokes about *p*-values, including this [XKCD](https://xkcd.com) comic. -->\n\n<!-- <center> -->\n<!-- ![](https://imgs.xkcd.com/comics/p_values.png) -->\n<!-- </center> -->\n\n<br />\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}