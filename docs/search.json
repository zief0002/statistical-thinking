[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Thinking",
    "section": "",
    "text": "Front Matter\nThis website is the textbook for EPSY 3264: Basic and Applied Statistics.\n\n\n\n\n\n\n\n\nLicensing and Attribution\nCopyright © 2025 Catalysts for Change\nPUBLISHED BY CATALYST PRESS\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License. You are free to share, remix, and make commercial use of the work under the condition that you provide proper attribution. To reference this work, use:\n\nZieffler, A., & Catalysts for Change. (2025). Statistical Thinking: A simulation approach to uncertainty (4.5th ed.). Minneapolis, MN: Catalyst Press. http://zief0002.github.io/statistical-thinking/\n\nThe work to create the material appearing in the book was made possible by the National Science Foundation (DUE–0814433).\nThe material on this website and in the lab manual is a direct reflection of the ideas, work, and effort of several Catalysts for Change. They include (alphabetically): Ethan Brown, Jonathan Brown, Dan Butler, Tony Casci, Beth Chance, George Cobb, Robert delMas, Katherine Edwards, Michelle Everson, Jeffrey Finholm, Chris Fiscus, Elizabeth Fry, Joan Garfield, Theresa Gieschen, Meg Goerdt, Robert Gould, Adam Gust, Melissa Hanson, John Holcomb, Michael Huberty, Rebekah Isaak, Kari Johnson, Nicola Justice, Laura Le, Chelsey Legacy, Regina Lisinker, Suzanne Loch, Matthew Mullenbach, Michael Nguyen, Amy Okan, Vimal Rao, Allan Rossman, Anelise Sabbag, Andrew Zieffler, and Laura Ziegler.\nAdditionally, some of the activities presented in the lab manual were originally developed by Beth Chance, George Cobb, John Holcomb, and Allan Rossman as part of their NSF-funded project Concepts of Statistical Inference: A Randomization-Based Curriculum (NSF CCLI- DUE-0633349).\n\n\n\nColophon\nThe book is typeset using Atkinson Hyperlegible. The color palette was generated using coolors.co.\nIcons used on the website are:\n\nKey by Iconic from the Noun Project",
    "crumbs": [
      "Front Matter"
    ]
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Course Material\nLearning statistics is sexy. Hal Varian, Google’s chief economist, believes this. During an interview in McKinsey Quarterly, Varian stated, “I keep saying the sexy job in the next ten years will be statisticians. People think I’m joking, but who would’ve guessed that computer engineers would’ve been the sexy job of the 1990s?” Varian is not the only person to express this sentiment either. Hans Rosling in the 2010 BBC documentary Joy of Stats1 referred to statistics as the “sexiest subject around”.\nWhether you believe it is the sexiest subject or not, it is incontrovertible that the use of statistics and data are prevalent in today’s information age. Almost every person on earth will benefit from learning some foundational ideas of statistics. This is true because statistics forms the basis of our everyday world just as much as do science, technology, and politics. Google, Netflix, Twitter, Facebook, OKCupid, Match.com, Amazon, iTunes, and the Federal Government are just a handful of the companies and organizations that use statistics on a daily basis. Journalism, political science, biology, sociology, psychology, graphic design, economics, sports science, and dance are all disciplines that have made use of statistical methodology.\nThe content in this textbook and in the lab manual will introduce you to the seminal ideas underlying the discipline of statistics. In addition, they have been designed with your learning in mind. For example, many of the class activities were developed using pedagogical principles, such as small group activities and discussion, that have been shown in research to improve student learning.\nCourse readings should be completed outside of class and are intended to help you learn and extend the ideas, skills, and concepts you learn in the classroom. The readings themselves are not all “traditional” readings in the sense of words written on the screen, but instead often link to video clips, blogs and other multimedia material.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01-introduction.html#course-material",
    "href": "01-introduction.html#course-material",
    "title": "Introduction",
    "section": "",
    "text": "PROTIP\nBecause the textbook includes background reading and information that will help you complete the activities from the Lab Manual, it is most helpful if you complete the textbook readings prior to coming to class.\n\n\n\nTinkerPlots 3™ Software\nMuch of the material presented in the lab manual requires the use of TinkerPlots 3™. This software can be downloaded (for Mac or PC), and a license can be purchased from [http://www.tinkerplots.com/]. You will need to do two things to obtain the TinkerPlots 3 software: (1) download the software to your computer and install it, and (2) purchase a license for it.\n\nFYI\nTinkerPlots can only be installed on a Mac or Windows computer. You cannot install it on a ChromeBook!\n\n\n\nDownload TinkerPlots 3 from the TinkerPlots website. Click the “Get TinkerPlots” link on the left-hand side of the website. Then agree to the license agreement, and select the link that corresponds to your computer’s operating system. Make sure you are clicking this link in the TinkerPlots 3.0 box and not the TinkerPlots 2.3 box!\nWhile that is downloading, purchase a license (further down the same page). You can do this with PayPal or a credit card. If you are using a credit card, note that the company that runs TinkerPlots is in Australia. Depending on your credit card, you may need to contact your financial institution to okay the international transaction.\n\nAfter you have downloaded the TinkerPlots installer, double-click it to open the installer. Follow the instructions to get it installed on your computer. Once it is installed you can delete the installer. Finally open the TinkerPlots 3 application and enter your license number when prompted.\n\n\n\nLab Manual\nYou will work from the lab manual every day in class. As such, you will need to bring a copy of the lab manual (physical or electronic) with you to class every day. To download a PDF copy of the lab manual, click this link: https://osf.io/download/683741827891878874a4ddc1/.\n\n\n\n\n\n\n\n\n\n\n\n\nTinkerPlots Data Sets\nThere are several data sets used in the lab manual, as well as in EPsy 3264 individual exams and group quizzes. You can download a ZIP file of these TinkerPlots data sets by clicking this link: https://osf.io/download/683745fa3b7d1cff5b85b5ac/.\nExtracting a ZIP File: After you download the ZIP file (which will have the file extension .zip), you will need to extract (i.e., unzip) it. (A ZIP file is simply a file that has been compressed to reduce the size of the file.) How you extract or unzip the file depends on the type of computer you are using:\n\nWindows: Right-click the ZIP file, Select Extract All..., Choose a location to extract the files (put it inside your Documents folder, or somewhere where you can easily access it), and click Extract. Once you have extracted the contents, you can delete the .zip file.\nMac: Double-click the ZIP file. This will extract the folder. Delete the .zip file, and then move the extracted folder to a location where you can easily access it (e.g., your Documents folder).\n\n\n\n\nParticipation in the Learning Process\nThe lab manual, instructors, and teaching assistants are all resources that are at your disposal to help you learn the material. In the end, however, you will have to do all of the hard work associated with actually learning that material. To successfully navigate this process, it is vital that you be an active participant in the learning process. Coming to class, participating in the activities and discussions, reading, completing the assignments, and asking questions are essential to successful learning.\nLearning anything new takes time and effort and this is especially true of learning statistics, as you are not just learning a set of methods, but rather a disciplined way of thinking about the world. Changing your habits of mind will take continual practice. It will also take a great deal of patience and persistence.\nAs you engage in and use the skills, concepts and ideas introduced in the material, you will find yourself thinking about data and evidence in a different way. This may lead you to make different decisions or choices. But, even if this course does not change your world overnight, you will at the very least be able to critically think about inferences and conclusions drawn from data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01-introduction.html#footnotes",
    "href": "01-introduction.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "Watch Joy of Stats online at http://www.gapminder.org/videos/the-joy-of-stats/↩︎",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "02-00-modeling-and-simulation.html",
    "href": "02-00-modeling-and-simulation.html",
    "title": "Modeling & Simulation",
    "section": "",
    "text": "Goals of Unit 1\nModeling is one of the most important topics you may ever learn. It is used in microbiology, macroeconomics, urban studies, sociology, psychology, public health, computer science, and of course, statistics. In fact, modeling is a method that is used in almost every discipline. Many think that it is an important skill to learn because it is so pervasive. While this is true, even more important is how closely the skills of modeling tie to the more general skills of problem solving. Starfield, Smith, and Bleloch (1994) summed this sentiment up nicely when they wrote, “learning to model is bound up with learning to solve problems and to think imaginatively and purposefully” (p. x).2\nA model is a simplified representation of a system that can be used to promote an understanding of a more complex system. For example, meteorologists use computers to build models of the climate to understand and predict the weather. The computer model includes behaviors or properties which correspond, in some way, to the particular real-world system of climate. The computer models, however, do not include every possible detail about climate. All models leave things out and get some things—many things—wrong. This is because all models are simplifications of reality. Since all models are simplifications of reality there is always a trade-off as to what level of detail is included in the model. If too little detail is included in the model one runs the risk of missing relevant interactions and the resultant model does not promote understanding. If too much detail is included in the model, the model may become overly complicated and actually preclude the development of understanding.\nModels have many purposes, but are primarily used to better understand phenomena in the real-world. Common uses of models are for description, exploration, prediction, and classification. For example, Google builds models to understand and predict peoples’ internet searching tendencies. These models are then used to help Google carry out more efficient and better searches of information. As another example, Netflix builds models to understand the characteristics of movies that their customers have rated highly so that they can then recommend other movies that the person may enjoy. Amazon and Apple iTunes both use models in similar manners.\nIn this unit, you will begin by learning about data and how we use TinkerPlots to make plots and summarize data to help us make sense of it. You will then explore ideas of randomness, which permeates, and is, in fact, fundamental to statistics. You will learn how to use TinkerPlots to model several random processes and generate outcomes from those models. By generating data from different models, you will gain experience in considering the variation in outcomes that is produced by these random processes. This consideration will help you understand and overcome many misleading human intuitions about randomness.\nYou will also be introduced to the Monte Carlo simulation process and learn how to carry out a Monte Carlo simulation using TinkerPlots™. This process allows you to quickly generate multiple data sets from a model in order to carry out hypothetical experiments. For example, we could ask the question: How likely is it to rain three out of the five days on my vacation given a particular forecast? By modeling the forecast and repeatedly generating data for the five days of vacation, we can then answer this question.\nAs you progress through the unit, remember that the modeling process is a creative process that can often be very challenging. At times, this might lead to frustration as you are learning and practicing some of the material. But, as Mosteller et al. (1973) remind us, it is also a profitable experience since, “modeling is not only a technique of statistics…it is a method of study which can be applied in many other fields as well” (p. xii).3",
    "crumbs": [
      "Modeling &amp; Simulation"
    ]
  },
  {
    "objectID": "02-00-modeling-and-simulation.html#footnotes",
    "href": "02-00-modeling-and-simulation.html#footnotes",
    "title": "Modeling & Simulation",
    "section": "",
    "text": "Hartmann, S. (2005). The world as a process: Simulations in the natural and social sciences. http://philsci-archive.pitt.edu/2412/↩︎\nStarfield, A. M., Smith, K. A., & Bleloch, A. L. (1994). How to model it: Problem solving for the computer age. Edina, MN: Burgess International Group, Inc.↩︎\nMosteller, F., Kruskal, W. H., Link, R. F., Pieters, R. S., & Rising, G. R. (1973). Statistics by example: Finding models. Reading, MA: Addison–Wesley.↩︎",
    "crumbs": [
      "Modeling &amp; Simulation"
    ]
  },
  {
    "objectID": "02-01-cases-and-attributes.html",
    "href": "02-01-cases-and-attributes.html",
    "title": "Cases and Attributes",
    "section": "",
    "text": "Questions to Ask about Every Data Set\nConsider the following data (mn-colleges-default-rates.tp3) which include information on the three-year loan default rates from 2015, 2016, and 2017 for 83 post-secondary institutions in Minnesota. Take a minute to look familiarize yourself with these data.\nThese data have a tabular structure, that is, it is organized into rows and columns; we refer to this as a case table.. In tabular data, the rows represent cases (also sometimes referred to as units) and the columns represent attributes (also referred to as variables).\nIn these data, there are 83 cases, and each case is a post-secondary institution in Minnesota. That is, each row represents a different post-secondary institution in Minnesota. There are five attributes in these data. Attributes are the information or characteristics that are collected for each of the cases. The attributes in these data are:\nAside from identifying the cases and attributes, here are some other questions about collected data that you should answer (Gould & Ryan, 2013):\nThe answers to these questions give us information about the data’s context, which is key to drawing reasonable conclusions and interpreting any statistical analyses. If you are the one collecting the data, you should record all of this information so others have access to it. For our data here are the answers to these questions:",
    "crumbs": [
      "Modeling &amp; Simulation",
      "Cases and Attributes"
    ]
  },
  {
    "objectID": "02-01-cases-and-attributes.html#questions-to-ask-about-every-data-set",
    "href": "02-01-cases-and-attributes.html#questions-to-ask-about-every-data-set",
    "title": "Cases and Attributes",
    "section": "",
    "text": "Who collected the data?\nHow and where were the data collected?\nWhy were the data collected?\n\n\n\nWho collected the data? These data are collected by the Minnesota Office of Higher Education. Learn more here.\nWhy were the data collected? These data are collected for monitoring debt trends and evaluating state financial aid policies. Colleges and universities with consistently high student loan default rates over a period of three years may be denied participation in federal and state financial aid programs for their students.\nHow and where were the data collected? The data three-year default rates were computed based on date provided by the U.S. Department of Education that includes informarion on official cohort default rates once per year. Institutions report this information to the U.S. Department of Education annually.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "Cases and Attributes"
    ]
  },
  {
    "objectID": "02-01-cases-and-attributes.html#classifying-attributes",
    "href": "02-01-cases-and-attributes.html#classifying-attributes",
    "title": "Cases and Attributes",
    "section": "Classifying Attributes",
    "text": "Classifying Attributes\nOur ultimate goal is often to analyze the data we have to learn from it. For example, in our MN post-secondary institution data, we may be interested in knowing what the average three-year default rate is for Minnesota post-secondary institution. Or, we may want to know whether borrowers from some institution types are more likely to default than borrowers from other institution types. The type of analyses we can do, however, depend on the type of attributes we have.\nWe typically classify attributes as either categorical attributes or quantitative attributes. These classifications are based on the type of information (data) in the attribute. A categorical attribute has values that represent categorical (or qualitative) differences between the cases, whereas a quantitative attribute represents numerical (or quantitative) differences between cases. For example, in our data, the following attributes are categorical:\n\nInstitution name (Institution)\nClassification of the institution (Type)\n\nTypically attributes that have numerical values are quantitative. (Sometimes numbers are used to represent different categories for a categorical attribute. For example, public institutions could be coded as “1” and private institutions could be coded as “0”. Although the values in this coded attribute would be numeric, these numbers actually are meaningless other than representing different categories.) One check of whether an attribute is actually quantitative is whether operations such as “less than” or “greater than” are meaningful when comparing two data values in the attribute. If the answer is “yes”, than the attribute is quantitative. If those operations are not meaningful, then the numeric values are simply categories and the attribute is categorical.\nConsider the loan default rates in our data. In this case, the operations of “less than” and “greater than” have meaning in that by comparing two values tell us that an instituiton has a lower or higher rate of borrowers who default on their loans. For example by comparing the 2017 default rates of Aveda Arts & Sciences Institute Minneapolis (12.06) and University Of Minnesota - Twin Cities (2.27), we can say that the borrowers from the University Of Minnesota - Twin Cities are less likely to default on their loans than borrowers from Aveda Arts & Sciences Institute Minneapolis. Because of this, we can confirm that the attribute DefaultRate2017 is a quantitative attribute.\nIn contrast, consider our propsed attribute in which public institutions were coded as “1” and private institutions were coded as “0”. If you compared Macalester College (a private university with a value of 0) to the University Of Minnesota - Twin Cities (a public university with a value of 1), the operations of “less than” and “greater than” are meaningless. It doesn’t make sense to say that a public university is greater than or less than a private unversity (at least in the mathematical sense). Because of this, our hypethetical attribute would be a categorical attribute.\nIn our data, we have three quantitative attributes:\n\nThe three-year default rate for fiscal years between October 01, 2013–September 30, 2015 (DefaultRate2015)\nThe three-year default rate for fiscal years between October 01, 2014–September 30, 2016 (DefaultRate2016)\nThe three-year default rate for fiscal years between October 01, 2015–September 30, 2017 (DefaultRate2017)",
    "crumbs": [
      "Modeling &amp; Simulation",
      "Cases and Attributes"
    ]
  },
  {
    "objectID": "02-01-cases-and-attributes.html#references",
    "href": "02-01-cases-and-attributes.html#references",
    "title": "Cases and Attributes",
    "section": "References",
    "text": "References\n\n\n\n\nGould, R., & Ryan, C. (2013). Introductory statistics: Exploring the world through data. Pearson.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "Cases and Attributes"
    ]
  },
  {
    "objectID": "02-02-tinkerplots-101-importing-data-and-plotting.html",
    "href": "02-02-tinkerplots-101-importing-data-and-plotting.html",
    "title": "TinkerPlots 101: Importing Data and Plotting",
    "section": "",
    "text": "Importing Data\nTinkerPlots is a software tool originally developed for use by middle school students. Its data visualization and modeling capabilities make it an excellent choice for introductory statistics courses. Aside from it functionality to carry out simulations and data analysis, it has many features that help students actually learn and reason about statistics. In this reading, you will learn how to use some of the basic features of TinkerPlots.\nOne of the first things you need to learn is how to import data into TinkerPlots. TinkerPlots 3 files have the file extension .tp3. You can import these files into TinkerPlots in one of two manners:\nFigure 1: TinkerPlots workspace showing a case table of the imported data.\nAfter importing the data, you should see a case table of the imported data in your TinkerPlots workspace.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Importing Data and Plotting"
    ]
  },
  {
    "objectID": "02-02-tinkerplots-101-importing-data-and-plotting.html#importing-data",
    "href": "02-02-tinkerplots-101-importing-data-and-plotting.html#importing-data",
    "title": "TinkerPlots 101: Importing Data and Plotting",
    "section": "",
    "text": "Double-clicking on the .tp3 file.\nOpening TinkerPlots, and selecting File &gt; Open. You can then navigate to the .tp3 file you want to import and select it.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Importing Data and Plotting"
    ]
  },
  {
    "objectID": "02-02-tinkerplots-101-importing-data-and-plotting.html#creating-a-dot-plot",
    "href": "02-02-tinkerplots-101-importing-data-and-plotting.html#creating-a-dot-plot",
    "title": "TinkerPlots 101: Importing Data and Plotting",
    "section": "Creating a Dot Plot",
    "text": "Creating a Dot Plot\nA dot plot is a simple data visualization that consists of plotting each case’s data value as a dot on a graph. To create a dot plot using TinkerPlots, we will:\n\nClick on the Plot icon in the main toolbar and drag a plot into the TinkerPlots workspace.\nClick on the name of the attribute in the case-table we want to plot, and drag that to to the x-axis of the plot. (When you do that, and get to the right place in the plot, you should see a rectangle appear along the x-axis).\n\n\n\n\n\n\n\n\n\nFigure 2: LEFT: When you drag the attribute to the correct place on the x-axis, a box will show up in the plot. RIGHT: A plot sperated into two bins and showing a bin line.\n\n\n\n\n\nThis should create a semi-organized plot of the data. One thing you will notice is that the data is binned. That is, there is at least one vertical line splitting up the data values into different categories (if you are plotting a categorical attribute) or ranges (if you are plotting a quantitative attribute). Every time we plot data in TinkerPlots, we want to be sure that the data in the plot is fully separated. This means that for categorical data, we want each category in its own bin, and for quantitative data we want no bins. To fully seperate the data in a plot,\n\nClick on a dot in the plot and keep dragging it to the right. You can let go when each category is in a unique bin (if you have categorical data) or all the bin lines disappear (quantitative data).\n\nAnother thing you will notice is that the dots representing the data are “floating” in the middle (vertically) of the plot. This makes it hard to compare counts of different values. To fix this problem, we will vertically stack the dots. To do this:\n\nClick the Stack icon in the plot toolbar associated with vertical dots. (It has three vertically stacked dots on it.)\n\n\n\n\n\n\n\n\n\nFigure 3: The plot toolbar has many options for adding lines, summaries, or separating the data in the plot.\n\n\n\n\n\n\nPROTIP\nIf you don’t see the plot toolbar, click on the plot to highlight it. That should bring up the plot toolbar.\n\n\nNow you should have a fully separated, vertically stacked dot plot.\n\n\n\n\n\n\n\n\nFigure 4: Two plots that are fully separated and vertically stacked. LEFT: The data in the plot are quantitative so there should be no bin lines after the cases are fully separated. RIGHT: The data in the plot are categorical so there will be bin lines after the cases are fully separated, but each category is in its own bin.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Importing Data and Plotting"
    ]
  },
  {
    "objectID": "02-02-tinkerplots-101-importing-data-and-plotting.html#changing-the-limits-on-the-x-axis",
    "href": "02-02-tinkerplots-101-importing-data-and-plotting.html#changing-the-limits-on-the-x-axis",
    "title": "TinkerPlots 101: Importing Data and Plotting",
    "section": "Changing the Limits on the x-Axis",
    "text": "Changing the Limits on the x-Axis\nWhen TinkerPlots creates a dotplot of a quantitative attribute, it picks the minimum and maximum value displayed on the axis based on the data values. Sometimes it is necessary to change these. To do this:\n\nDouble-click the box enclosing either the minimum or maximum axis value. You can then indicate the value you want the axis to end at.\n\n\n\n\n\n\n\n\n\nFigure 5: How to change the limits on the x-axis. The maximum value of the x-axis in this plot has been changed to 30.\n\n\n\n\n\n\nPROTIP\nYou can also change the bin width by double-clicking on eithere the minimum or maximum axis value. this. A fully seperated plot should have the bin width set to 0. This is an alternative method to fully seperate the cases in a plot.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Importing Data and Plotting"
    ]
  },
  {
    "objectID": "02-02-tinkerplots-101-importing-data-and-plotting.html#adding-summaries-to-the-plot",
    "href": "02-02-tinkerplots-101-importing-data-and-plotting.html#adding-summaries-to-the-plot",
    "title": "TinkerPlots 101: Importing Data and Plotting",
    "section": "Adding Summaries to the Plot",
    "text": "Adding Summaries to the Plot\nA fully separated and stacked plot of the data is useful for describing the data’s distribution. Another thing that is useful is to compute numerical summaries. For categorical data, we typically want to compute the count (or percent) of cases in each category, and for quantitative data it it typical to compute the mean (or median) of the values. To compute summaries:\n\nMake sure the plot is highlighted so that you have access to the plot toolbar.\nClick on the summary icon in the plot toolbar that you want to display on the plot.\n\nFor example, to add the mean to our previously created plot we would select the Mean icon from the plot toolbar.\n\n\n\n\n\n\n\n\nFigure 6: Adding the mean to a plot.\n\n\n\n\n\nIf you summarize the mean or median, you will also likely want to see what the numerical value of that summary is. To do that:\n\nClick on the Average Options (upside-down triangle next to the averages) and select Show Numeric Value.\n\n\n\n\n\n\n\n\n\nFigure 7: Adding the numeric value of the mean to a plot.\n\n\n\n\n\n\nPROTIP\nAny time you see and upside-down triangle in TinkerPlots, it is associated with additional options that you can select.\n\nWith categorical data, we summarize with counts and percentages. To do this, with the plot highlighted,\n\nClick on the n (counts) or % icon in the plot toolbar that you want to display on the plot.\n\n\n\n\n\n\n\n\n\nFigure 8: Adding bin counts to a plot.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Importing Data and Plotting"
    ]
  },
  {
    "objectID": "02-02-tinkerplots-101-importing-data-and-plotting.html#adding-a-divider-to-the-plot",
    "href": "02-02-tinkerplots-101-importing-data-and-plotting.html#adding-a-divider-to-the-plot",
    "title": "TinkerPlots 101: Importing Data and Plotting",
    "section": "Adding a Divider to the Plot",
    "text": "Adding a Divider to the Plot\nSometimes it is useful to divide you plot into different parts. To do this we highlight the plot, and:\n\nClick on the Divider icon in the plot toolbar.\n\n\n\n\n\n\n\n\n\nFigure 9: Adding a divider to a plot.\n\n\n\n\n\nBy default, the plot will be divided into three distinct parts: a shaded middle part and two unshaded ends. You can change where the division is by dragging the ends of the shaded area. To do this:\n\nClick on the white boxes at the top of the divider in the plot and drag the end to where you want it.\nYou can also double-click on the white boxes at the top of the divider and enter in a specific value you want the divider at.\n\nIt can also be useful to add counts or percentages to the plot after you have added a divider. This will tell you the number or percentage of cases in each of the three sections. To do this, highlight the plot, and click on the n or % in the plot toolbar.\n\n\n\n\n\n\n\n\nFigure 10: In this plot the divider has been moved so the shaded area covers most of the cases. Percentages have also been added by clicking the % icon in the plot toolbar.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Importing Data and Plotting"
    ]
  },
  {
    "objectID": "02-02-tinkerplots-101-importing-data-and-plotting.html#adding-a-reference-line-to-the-plot",
    "href": "02-02-tinkerplots-101-importing-data-and-plotting.html#adding-a-reference-line-to-the-plot",
    "title": "TinkerPlots 101: Importing Data and Plotting",
    "section": "Adding a Reference Line to the Plot",
    "text": "Adding a Reference Line to the Plot\nA vertical reference line can be added to the plot by clicking on the vertical reference line icon in the plot toolbar. This is useful when you want to indicate a particular value on your plot.\n\n\n\n\n\n\n\n\nFigure 11: Adding a vertical reference line to the plot. In this plot the reference line has been moved to the value of 20.\n\n\n\n\n\nYou can move the vertical reference line by:\n\nClicking on the red box at the top of the reference line (in the plot) and dragging the line to wherever you want it.\nYou can also double-click on the red box at the top of the reference line and enter in a specific value you want the reference line at.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Importing Data and Plotting"
    ]
  },
  {
    "objectID": "02-03-randomness.html",
    "href": "02-03-randomness.html",
    "title": "Randomness",
    "section": "",
    "text": "One critical component of simulation is the random process used to generate data. To help you begin to understand randomness, watch Stephan Dreiseitl’s TEDxMCInnsbruck talk: Math and the Art of Describing Randomness\nWhat is Random",
    "crumbs": [
      "Modeling &amp; Simulation",
      "Randomness"
    ]
  },
  {
    "objectID": "02-04-tinkerplots-101-generating-simulated-data.html",
    "href": "02-04-tinkerplots-101-generating-simulated-data.html",
    "title": "TinkerPlots 101: Generating Simulated Data",
    "section": "",
    "text": "Creating a Sampler\nSocial scientists are increasingly using simulation methods to help them understand the social processes they study. At the heart of simulation methods is using a model to generate random samples of simulated data. Most of the models you will encounter in this course are referred to as probability models. That is just a fancy way of associating probabilities with different events, or outcomes, in a model. For example, the model of flipping a “fair” coin is a probability model. There are two events/outcomes in the model: heads and tails. Each of these outcomes has a probability of 0.5 associated with it. (Note that although we could say 50%, that probabilities are on the scale from 0 to 1, so are defined using decimal values.) To generate simulated data using TinkerPlots, we use a sampler to specify characteristics of the model. The sampler is composed of one or more sampling devices that use a random process to generate data from the prespecified model.\nTo create a sampler, you will:\nFigure 1: Default TinkerPlots sampler showing a mixer with three elements.\nThe default sampler is a mixer with three balls in it. The mixer is one of six different sampling devices we can use in our sampler. You can see the icons for other potential sampling devices at the bottom of the sampler.\nFigure 2: Six potential sampling devices we can use in a TinkerPlots sampler.\nThe three sampling devices we will use most often in this class are: spinners, mixers, and stacks. While all three of these devices are similar in that they generate data by randomly sampling from aset of discrete elements, they have some subtle diffeences that make some devices more appropriate in certain situations. To illustrate these differences, consider a situation in which you are randomly generating animals from a population that includes:\nFigure 3: A mixer (LEFT), spinner (CENTER), and stacks (RIGHT) sampling device for randomly generating animals from a population that includes three goats, four cows, ten chickens and one sheep.\nJust seeing these different sampling devices might bring to mind some advantages and disadvantages of using these devices. For example, the mixer requires us to record each individual animal on a seperate ball. While we can easily see each ball, we need to double-check the counts (which are not shown in the sampling device). The spinner requires us to indicate the probability (or proportion) for each type of animal rather than the counts. The stacks device seems to be the best device for this scenario as it allows us to easily see the counts of each animal. (Often the stacks device is useful when you have many repeats of the same element.) In other modeling scenarios, the mixer or spinner might be a better device to use.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Generating Simulated Data"
    ]
  },
  {
    "objectID": "02-04-tinkerplots-101-generating-simulated-data.html#creating-a-sampler",
    "href": "02-04-tinkerplots-101-generating-simulated-data.html#creating-a-sampler",
    "title": "TinkerPlots 101: Generating Simulated Data",
    "section": "",
    "text": "Click on the Sampler icon in the TinkerPlots toolbar and drag a sampler from into your workspace.\n\n\n\n\n\n\nThree goats\nFour cows\nTen chickens\nOne sheep\n\n\n\n\nFYI\nThroughout the class, you will gain a sense for when one device might be better to use than another device.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Generating Simulated Data"
    ]
  },
  {
    "objectID": "02-04-tinkerplots-101-generating-simulated-data.html#changing-the-sampling-device",
    "href": "02-04-tinkerplots-101-generating-simulated-data.html#changing-the-sampling-device",
    "title": "TinkerPlots 101: Generating Simulated Data",
    "section": "Changing the Sampling Device",
    "text": "Changing the Sampling Device\nTo change the sampling device from the default mixer:\n\nClick on the icon of the sampling device you want and drag it into the sampler.\n\nWhen you drag the new sampling device over the old sampler, you will see two pink dots appear. These are locations where you can drop the new sampling device. You will also see a black outline of a rectangle appear in the sampler. Which pink dot you drop the device on depends on whether you want to replace the old device, or whether you want to add a second device to the sampler.\n\n\n\n\n\n\n\n\nFigure 4: Dragging a stacks sampling device into the sampler to either replace the default mixer or add it as a second device in the sampler.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Generating Simulated Data"
    ]
  },
  {
    "objectID": "02-04-tinkerplots-101-generating-simulated-data.html#adding-or-removing-elements",
    "href": "02-04-tinkerplots-101-generating-simulated-data.html#adding-or-removing-elements",
    "title": "TinkerPlots 101: Generating Simulated Data",
    "section": "Adding or Removing Elements",
    "text": "Adding or Removing Elements\nLet’s set up a stacks device to include the 18 animals in our scenario. AFter dragging a sampler into the workspace, and then changing the sampling device to stacks, you should have a device that includes two different stacks: one stack includes two “a” elemnts, and the other stack includes one “b” element. There are four icons below the sampling device. These icons allow us to add or remove elements in the sampling device, and access the options for that sampling device.\n\n\n\n\n\n\n\n\nFigure 5: The four icons below the sampling device are used to add or remove elements in the sampling device, and access the options for that sampling device.\n\n\n\n\n\nIn our scenario, we have four unique elements: goat, cow, chicken, and sheep. Since our current sampler already has two stacks, we need to add two additional stacks by clicking the “+” icon below the sampling device twice. (You should now see four stacks in the sampling device.) Click on the name of each stack (e.g., on “a”) and change each element’s name to reflect the four animals.\n\n\n\n\n\n\n\n\nFigure 6: Clicking the name of each element allows you to edit the element name.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Generating Simulated Data"
    ]
  },
  {
    "objectID": "02-04-tinkerplots-101-generating-simulated-data.html#sampling-device-options",
    "href": "02-04-tinkerplots-101-generating-simulated-data.html#sampling-device-options",
    "title": "TinkerPlots 101: Generating Simulated Data",
    "section": "Sampling Device Options",
    "text": "Sampling Device Options\nAfter we have identified our four unique animals, we need to change the height of each stack to reflect the number of each type of animal in our population. We do this by accessing the device options menu for the sampling device. Click the upside-down triangle below the sampling device to bring up the device options menu. There are several options available there, but we want to Show Count. Selecting this will bring include the count for each of our four animal types. Edit these to reflect the number of each type of animal in the population:\n\nThree goats\nFour cows\nTen chickens\nOne sheep\n\n\n\n\n\n\n\n\n\nFigure 7: Access the device options to show the counts for each stack. These counts can be edited by clicking on the count value and changing its value.\n\n\n\n\n\n\nFYI\nDifferent sampling devices will have different options.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Generating Simulated Data"
    ]
  },
  {
    "objectID": "02-04-tinkerplots-101-generating-simulated-data.html#sampler-settings-draw-and-repeat",
    "href": "02-04-tinkerplots-101-generating-simulated-data.html#sampler-settings-draw-and-repeat",
    "title": "TinkerPlots 101: Generating Simulated Data",
    "section": "Sampler Settings: Draw and Repeat",
    "text": "Sampler Settings: Draw and Repeat\nThe Draw and Repeat values are two settings that we need to specify when we set up a simulation. The Repeat value is always set to the number of cases that you want to generate in a single trial (i.e., run) of the the simulation. For example, say we wanted to randomly generate seven animals from our population of animals. In that case, we would set the Repeat value to 7.\n\n\n\n\n\n\n\n\nFigure 8: Setting the Repeat and Draw values in a sampler.\n\n\n\n\n\nThe Draw value is always set to the number of sampling devices included in the sampler. In our example, there is only a single sampling device included in the sampler. Because of this, the the Draw value should be set to 1. Later in the course, you will be setting up samplers with multiple sampling devices. In those cases, you will need to change the Draw value to reflect however many sampling devices are included in your sampler.\n\n\n\n\n\n\n\n\nFigure 9: This sampler includes two sampling devices, so the Draw value is set to 2.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Generating Simulated Data"
    ]
  },
  {
    "objectID": "02-04-tinkerplots-101-generating-simulated-data.html#naming-the-sampling-device",
    "href": "02-04-tinkerplots-101-generating-simulated-data.html#naming-the-sampling-device",
    "title": "TinkerPlots 101: Generating Simulated Data",
    "section": "Naming the Sampling Device",
    "text": "Naming the Sampling Device\nLastly, you will want to name the sampling device. The default name “Attr1” does not describe what is being generated from the sampling device. In our scenario, a better name might be “Animal”.\n\n\n\n\n\n\n\n\nFigure 10: Edit the name of the sampling device to reflect the data that will be generated in the simulation.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Generating Simulated Data"
    ]
  },
  {
    "objectID": "02-04-tinkerplots-101-generating-simulated-data.html#running-the-simulation",
    "href": "02-04-tinkerplots-101-generating-simulated-data.html#running-the-simulation",
    "title": "TinkerPlots 101: Generating Simulated Data",
    "section": "Running the Simulation",
    "text": "Running the Simulation\nOnce we have completely set up our sampler, we can run the simulation. To do this, click the Run button in the top-left of the sampler. This will start the simulation, which was set up to randomly generate seven animals from our population. These generated outcomes are entered into a case table in the TinkerPlots workspace.\n\n\n\n\n\n\n\n\nFigure 11: Clicking the Run button executes the simulation. The randomly generated outcomes are entered in a case table.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Generating Simulated Data"
    ]
  },
  {
    "objectID": "02-06-monte-carlo-simulation.html",
    "href": "02-06-monte-carlo-simulation.html",
    "title": "Monte Carlo Simulation",
    "section": "",
    "text": "Example of a Monte Carlo Simulation Study\nMonte Carlo simulation is one method that statisticians use to understand real-world phenomena. In Monte Carlo simulation, a model is used to generate multiple (sometimes millions) of data sets. By examining the data sets produced (or summaries of the data sets produced), researchers can draw insight about and predict what might happen in the real-world under a given set of circumstances. You can read about the fascinating origins of Monte Carlo simulation in the following article:\nCarsey & Harden (2014) define Monte Carlo simulation as,\nThis definition of a Monte Carlo simulation implies that the process for carrying out such a simulation encompasses:\nOr, in simpler terms, (1) model, (2) simulate, and (3) evaluate.\nIn 1978, China introduced the “one-child” policy in order to alleviate social, economic, and environmental problems in China. According to Wikipedia (2015)\nAlthough the Chinese government has suggested that the policy has prevented more than 250 million births from its implementation to 2000, the policy is controversial both within and outside of China because of the manner in which the policy has been implemented. There have also been concerns raised about potential negative economic and social consequences, in part because many families were determined to have a son. Scholars have wondered how things would change if instead of a one-child policy, a country adopted a “one son” policy. A “one son” policy would allow families to keep having children until they had a son. If a family’s first child is a boy, they would be restricted from having more children. If, however, the first child was a daughter, the family could continue having children until a son was born. For example, they might ask the question,",
    "crumbs": [
      "Modeling &amp; Simulation",
      "Monte Carlo Simulation"
    ]
  },
  {
    "objectID": "02-06-monte-carlo-simulation.html#example-of-a-monte-carlo-simulation-study",
    "href": "02-06-monte-carlo-simulation.html#example-of-a-monte-carlo-simulation-study",
    "title": "Monte Carlo Simulation",
    "section": "",
    "text": "The policy officially restricts the number of children married urban couples can have to one, although it allows exemptions for several cases, including rural couples, ethnic minorities, and parents without any siblings themselves. A spokesperson of the Committee on the One-Child Policy has said that approximately 35.9% of China’s population is currently subject to the one-child restriction.\n\n\n\n\n\nIf China adopted a “one son” policy, how would the policy affect the average number of children per family, which is currently 1.6 (Central Intelligence Agency, 2025)?\n\n\n\n\n\nModel\nOne way in which this question could be studied (without actually implementing the policy) would be to conduct a simulation study by modeling this situation and generating many data sets from the model. Consider for a minute how you might model the number of children a particular family would have.\nOne way to model this is to write the word “boy” on one index card and the word “girl” on another index card and to place those two index cards in a hat. You could also use a TinkerPlots sampler to set up a similar model.\n\n\n\n\n\n\n\n\nFigure 1: TinkerPlots sampler to model the question posed in the ‘one son’ simulation.\n\n\n\n\n\n\n\n\nSimulate\nAfter you have a model, the next step in a simulation study is to use the model to generate data. For example, in our index card model, you could mix up the index cards and draw a single card from the hat. If the card has the word “boy” written on it, the simulated “family” would be reported to have one child. If the card has the word “girl” written on it, a tally mark could be recorded and the index card would be replaced in the hat. The cards could then be remixed and another card would be drawn. If the second card drawn has the word “boy” written on it, the simulated “family” would be reported to have two children. If the second card has the word “girl” written on it, another tally mark could be recorded and the index card would again be replaced in the hat. This process would continue until the “boy” card was drawn. The process would be the same if you were using TinkerPlots. But, rather than mixing the index cards and dealing one out, you would click the Run button to generate a child.\nThe table below shows the results after carrying out this process for three simulated families.\n\n\n\n\nThe recorded number of girls, boys, and total children for three simulated families.\n\n\nFamily\nGirl\nBoy\nChildren\n\n\n\n\nFamily #1\n✔\n✔\n2\n\n\nFamily #2\n\n✔\n1\n\n\nFamily #3\n✔✔\n✔\n3\n\n\n\n\n\nOne thing to note is that, not only do we need to record the data that was simulated, but we also need to summarize that data in a way that helps us answer our question of interest. Here the summary we used was the total number of children in the simulated family. This will be true in every simulation study you undertake in the course.\n\nVOCABULARY\nEach repetition of the simulation experiment is referred to as a trial of the simulation. In each trial, we compute a summary measure to summarize the outcomes from that trial into a single number.\n\n\n\n\nEvaluate\nIn practice, we would carry out many trials of this simulation, say running it for 500 families. We would then use the results (the 500 summary measures) to provide an answer to the research question. For example, we could average the 500 summary measures to determine the mean number of children in a family. We could then compare whether this is smaller, equal to, or bigger than 1.6. This comparison would help answer whether the one-son policy impacts the average number of children per family (which is currently 1.6).",
    "crumbs": [
      "Modeling &amp; Simulation",
      "Monte Carlo Simulation"
    ]
  },
  {
    "objectID": "02-06-monte-carlo-simulation.html#monte-carlo-simulation-assumptions",
    "href": "02-06-monte-carlo-simulation.html#monte-carlo-simulation-assumptions",
    "title": "Monte Carlo Simulation",
    "section": "Monte Carlo Simulation Assumptions",
    "text": "Monte Carlo Simulation Assumptions\n“Wait,” you say. “Even if I carried out this simulation, I still would not be able to provide an answer to the research question! It doesn’t reflect reality! Some families may not want to have any children, while others might be happy to stop after a girl was born. What about multiple births?”\nMaybe you are even questioning whether the probability of having a boy or having a girl is really 50:50. These are all valid points, and all would likely affect the results of the simulation, which in turn affects the inferences and conclusions that are drawn.\nWhile the model used in the “one son” example is overly simplistic for drawing any sorts of meaningful conclusions about implementing such a policy in China, it could still provide a useful starting point for introducing additional complexity. Even in the most enormously complicated modeling problem, researchers often make many simplifying assumptions. (Remember that all models—even those that seem quite complex—are simplifications of reality and get many things wrong.) With enough simplification, a model can be constructed and studied. The model is evaluated and often revised or updated as certain assumptions are deemed tenable and others are not. Because of this process, simulation studies are generally iterative in their development. This iteration process continues until an adequate level of understanding is developed and the research question can be answered.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "Monte Carlo Simulation"
    ]
  },
  {
    "objectID": "02-06-monte-carlo-simulation.html#monte-carlo-simulation-in-practice",
    "href": "02-06-monte-carlo-simulation.html#monte-carlo-simulation-in-practice",
    "title": "Monte Carlo Simulation",
    "section": "Monte Carlo Simulation in Practice",
    "text": "Monte Carlo Simulation in Practice\nIn practice, statisticians often use incredibly complex models to generate their data. As an example, Electronic Arts, the video game company behind titles such as Madden, NHL and FIFA, uses game telemetry (the transmission of data from a game executable for recording and analysis) to model the gameplay patterns of players and identify the elements of their games that are highly correlated with player retention (Mateas & Jhala, 2011).\nBy understanding the behavior of players and the common patterns that are used, Electronic Arts game developers can focus their attention on more relevant features in future iterations of the game and ultimately reduce production costs. For example, in their examination of Madden NFL 11, Electronic Arts used 46 features to model players’ preferences, including their control usage, performance, and play-calling style. This is but one example of using simulation in video games.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "Monte Carlo Simulation"
    ]
  },
  {
    "objectID": "02-06-monte-carlo-simulation.html#references",
    "href": "02-06-monte-carlo-simulation.html#references",
    "title": "Monte Carlo Simulation",
    "section": "References",
    "text": "References\n\n\n\n\nCarsey, T. M., & Harden, J. J. (2014). Monte Carlo simulation and resampling methods for social science. Sage.\n\n\nCentral Intelligence Agency. (2025). The world factbook. https://www.cia.gov/the-world-factbook/\n\n\nMateas, M., & Jhala, A. (2011). Modeling player retention in Madden NFL 11. Presented at Innovative Applications of Artificial Intelligence. http://users.soe.ucsc.edu/~bweber/pubs/madden11retention.pdf\n\n\nWikipedia. (2015). One-child policy. http://en.wikipedia.org",
    "crumbs": [
      "Modeling &amp; Simulation",
      "Monte Carlo Simulation"
    ]
  },
  {
    "objectID": "02-07-tinkerplots-101-collecting-summaries.html",
    "href": "02-07-tinkerplots-101-collecting-summaries.html",
    "title": "TinkerPlots 101: Collecting Summary Measures",
    "section": "",
    "text": "Example Simulation Study\nTo use results from simulation studies, we need to have the results from many, many trials. In practice, statisticians might carry out a million or more trials. The process of manually recording the results from each trial and entering those into a case table would be onerous, if not impossible. Because of this, we have the software record and keep track of the result from each trial of the simulation. In TinkerPlots, this is referred to as “collecting” the summary measure from each trial.\nTo illustrate how to have TinkerPlots record and collect the summary measure from each of the trials, consider the following scenario:\nDumbledore has set up the following TinkerPlots sampler to carry out his simulation study.\nFigure 1: TinkerPlots sampler showing a spinner with four types of pets and their respective probabilities in the wizarding world.\nHe has also run a single trial and has plotted the results. He has also summarized the results by computing the case counts in each pet bin.\nFigure 2: A case table including the 25 outcomes from the first trial. These outcomes are also plotted and summarized with case counts.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Collecting Summary Measures"
    ]
  },
  {
    "objectID": "02-07-tinkerplots-101-collecting-summaries.html#example-simulation-study",
    "href": "02-07-tinkerplots-101-collecting-summaries.html#example-simulation-study",
    "title": "TinkerPlots 101: Collecting Summary Measures",
    "section": "",
    "text": "Dumbledore wants to find out how many Hogwarts students out of 25 first-years are likely to have a owl. He know that in the larger magical population about 30% of wizards have owls, 20% have rats, 40% have cats, and 10% have toads. He also took a statistics course, so he would like to conduct a simulation to account for sampling variation in his estimate.\n\n\n\n\n\n\nFYI\nIn order to have TinkerPlots collect the results from your trials, you need to run the first trial of the simulation, plot the outcomes, and add a summary measure to the plot.\n\n\n\nCollecting Summary Measures\nWe collect the summary measure from the plot of results from our first trial. To do this:\n\nRight-click the value of the summary measure in your plot.\nSelect Collect Statistic from the pop-up menu.\n\n\n\n\n\n\n\n\n\nFigure 3: Collecting a summary measure from the plot of the first trial.\n\n\n\n\n\n\nFYI\nIt is important that you right-click on the actual value of the summary measure in the plot since you want TinkerPlots to collect the value. For example, in the plot displayed above, you would right-click on the value 6 to collect the number of owls.\n\nThis will add the summary measure from the pl0t to a case table. You can carry out additional trials and collect the summary measure from those trials by changing the value in the collect box from 1 to another value, say 99. This will carry out 99 additional trials and collect the same summary measure from those trials as you did in the first trial. After changing the value, click the Collect button.\n\n\n\n\n\n\n\n\nFigure 4: Collecting summary measures from 99 additional trials.\n\n\n\n\n\nYou should see an additional 99 summary measures added to your case table. (When it finishes, you should have a total of 100 summary measures in the case table! The summary from the first trial, and the additional 99 summary measures from the trials you just collected.) These summary measures can be plotted and summarized the same as any other data we have in a case table.\n\n\n\nTurning Off the Animation: Faster Collecting\nWhile the collection in TinkerPlots is faster than doing the simulation by hand, we can speed up the simulation by turning off the animation. To do this, click in the Collect Options (the upside-down triangle in the collect case table) and select History Options.... In the new menu, uncheck the Animation On option. The speed of the collection should now be much faster.\n\n\n\n\n\n\n\n\nFigure 5: Turning off the animation can speed up the collection of summary measures.",
    "crumbs": [
      "Modeling &amp; Simulation",
      "TinkerPlots 101: Collecting Summary Measures"
    ]
  },
  {
    "objectID": "03-00-evaluating-hypotheses.html",
    "href": "03-00-evaluating-hypotheses.html",
    "title": "Evaluating Hypotheses",
    "section": "",
    "text": "In the course activities and homework assignments, you have been using probability models to generate random outcomes. You have also learned how to use Monte Carlo simulation to generate many data sets from a given model. This is the same kind of process that researchers, scientists, and statisticians engage in when they evaluate (or test) hypotheses about the world.\nFor example, consider evaluating whether the admission rate for public universities is higher then 0.50. Imagine you collected data from 15 public universities and found that the admission rate for those 15 schools was 0.62. Based on this evidence, we could say that the average admission rate for our sample of 15 universities was higher than 0.50. But, is this true when we grow our sample to ALL public universities? Is the mean admission rate for ALL public universities higher than 0.50?\nDrawing conclusions beyond the data we have is called inference, and the associated methods that allow researchers to allows us to learn from incomplete or imperfect data are referred to as statistical inference (Gelman & Hill, 2007). One thing that makes inference difficult is sampling variation. Sampling variation is the variation in summary measures that arises because of having different samples from the model (or population). You have experienced this in the previous unit in all the simulations that you have carried out—you have seen that we get variation in the results (summary measures) just because of chance (randomness).\n\n\nGoals of Unit 2\nIn this unit, you will learn the language statisticians use to describe distributions. You will also explore the process of evaluating statistical hypotheses. As part of that you will be introduced to how to write testable statisitical hypotheses. You will also use TinkerPlots to generate simulated data to account for sampling variation in the results that would be expected under these hypotheses.\nYou will also learn how to formally quantify the variation in a distribution. This is helpful as we evaluate whether a particular result in observed data is compatible with results produced from the given model. Lastly, you will learn about common misconceptions regarding model evaluation (e.g., we can never say a model produced the data, only that it produces results compatible with the data), and how to use probabilistic language when providing an “answer” to a research question.\n\n\n\n\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.",
    "crumbs": [
      "Evaluating Hypotheses"
    ]
  },
  {
    "objectID": "03-02-describing-distributions.html",
    "href": "03-02-describing-distributions.html",
    "title": "Describing Distributions",
    "section": "",
    "text": "Shape\nOne of the important steps in any statistical analysis is that of summarizing data. It is good practice to examine both a graphical and a numerical summarization of your data. These summaries are often part of the evidence that researchers use to support any conclusions drawn from the data. They also allow researchers to discover structure that might have otherwise been overlooked in the raw data that was actually collected. Lastly, both graphical and numerical summaries of the data often point to other analyses that may be undertaken with the data.\nOnce raw data has been collected in a study, it can be overwhelming to pull any kind of meaning out of it. For example, it is not uncommon for Google to be dealing with millions of cases. How can Google—or any researcher for that matter—go from all of that raw data to something that can help them answer their research questions?\nRather than examining all of those cases individually, researchers examine the data collectively, often by plotting it. This is what is meant by a graphical summary of the data; it is quite literally, a picture of the distribution.\nThere are many, many different types of plots that have been created to graphically summarize data. Each can provide a slightly different representation of the data. Metaphorically, you can imagine each of these different plot types as a different photo taken of the exact same person. Some may be color, others black and white. Some may be taken from different perspectives, angles or distances. While all photographs “summarize” the same person, you may notice characteristics of that person in some photos that are not evident in others. Many of the photos, however, will show the same thing.\nThe dot plot that TinkerPlots™ provides is a very useful plot.1 It allows us to summarize the shape of the distribution very easily. Shape is used to describe a distribution’s symmetry. As you might expect, symmetric distributions are shaped the same on either side of the center. (Another way of thinking about this is that if you folded the distribution at the center, the folded half of the distribution would align pretty well on top of the other half.) For example, “bell-shaped” (“approximately normal”) distributions are symmetric.\nWhen a distribution is asymmetric, it is referred to as a skewed distribution. The distribution shown in Figure 1 is a skewed distribution. In this distribution, there appears to be a longer tail on the right side of the distribution. Because the tail is on the right side of the distribution, statisticians would say it is “skewed to the right” or “positively skewed”. In a similar way, a distribution that tails to the left is “skewed to the left” or “negatively skewed”.\nFigure 1: This distribution is skewed to the right, or positively skewed.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Describing Distributions"
    ]
  },
  {
    "objectID": "03-02-describing-distributions.html#location",
    "href": "03-02-describing-distributions.html#location",
    "title": "Describing Distributions",
    "section": "Location",
    "text": "Location\nAside from the overall shape of the distribution, it is also useful to summarize the location of the distribution. The location of the distribution provides a summarization of a so-called “typical” value for the data. A “typical” value can be estimated from the plot of the distribution. You can also use more formally calculated summaries of the location such as the mean, median, or mode. These values are easily calculated using TinkerPlots™.\nWhen looking at a plot of a distribution, data analysts often consider the number of modes or “humps” that are seen in a plot of the distribution. Here, the concept of mode is slightly different (although related) to the concept of mode that you may have learned in previous mathematics or statistics courses. The mode of a distribution gives a general sense of the values or measurements that occur frequently. This may be a single number, but many times is not. For example, the first hump of the distribution shown in the figure below suggests that values around nine are very common. The actual value of nine, however, may only show up once or twice in the data.\n\n\n\n\n\n\n\n\n\nFigure 2: A bimodal distribution showing two modes. One mode is around 9, and the other is near 12.\n\n\n\n\n\n\nA distribution can be unimodal (one mode), bimodal (two modes), multimodal (many modes), or uniform (no modes). The distribution shown above is bimodal—notice there are two humps. Uniform distributions have roughly the same frequency for all possible values (they look essentially flat) and thus have no modes.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Describing Distributions"
    ]
  },
  {
    "objectID": "03-02-describing-distributions.html#variation",
    "href": "03-02-describing-distributions.html#variation",
    "title": "Describing Distributions",
    "section": "Variation",
    "text": "Variation\nA third characteristic of a distribution that should be summarized is the variation. Summarizing the variation gives an indication of how variable the data are. One method of numerically summarizing the variability in the data is to quantify how close the observations are relative to the “typical” value on average. Are the observations for the most part close to the “typical” value? Far away from the “typical” value? How close?\nIt turns out, that the shape of the distribution also helps describe the variation in the data. For example, “bell-shaped” distributions have most observations close to the “typical” value, and more extreme observations show up both below and above the “typical” value (the variation is the same on both sides of the “typical” value). Whereas skewed distributions have many observations near the “typical” value, but extreme values only deviate from this value in one direction (there is more variation in the data on one side of the “typical” value than the other).\n\n\n\n\n\n\n\n\n\nFigure 3: Most of the observations in this distribution are clustered between 0 and 2. There are some observations greater than 2 (up to 10), although these are rare.\n\n\n\n\n\n\nOne thing that affects the variation, and should be described is whether there are observations that stand out from the other observations. Often these observations have extremely large or small values relative to the other observations. These observations are referred to as potential outliers, or extreme cases. For example, in the positively skewed distribution shown previously, the observation that has a value near 10 would likely be considered a potential outlier.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Describing Distributions"
    ]
  },
  {
    "objectID": "03-02-describing-distributions.html#putting-it-all-together",
    "href": "03-02-describing-distributions.html#putting-it-all-together",
    "title": "Describing Distributions",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nRotten Tomatoes is a website which aggregates movie critics’ reviews of films. The website marks each review as either positive or negative and then gives the film a score based on the percentage of positive reviews. In addition to the critics’ score, each film is also given a score based on reviews from the general public using the same methodology (reviews are tabulated so that the score represents the percentage of positive reviews from the general public). The plot shown below shows a dot plot of the distribution of the general public’s scores for 134 movies released in 2009.\n\n\n\n\n\n\n\n\n\nFigure 4: The scores for 134 movies released in 2009 based on the general public’s reviews. The scores represent the percentage of positive reviews for each movie.\n\n\n\n\n\n\nA written description of the distribution might read as follows:\n\nThe distribution of scores for this sample of 134 movies is fairly symmetric. The median score for these movies is near 60, indicating that a typical movie released in 2009 is positively reviewed by about 60% of the public. The distribution also indicates that there is variation in the movies’ scores. Most of the movies in the sample have a score between 35 and 85, suggesting large differences in the public’s opinion of the quality of these movies.\n\nNotice that the description includes a description of the distribution’s shape, location, and variation. It also incorporates the context of the data, in this case film scores. This helps a reader to interpret the description.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Describing Distributions"
    ]
  },
  {
    "objectID": "03-02-describing-distributions.html#footnotes",
    "href": "03-02-describing-distributions.html#footnotes",
    "title": "Describing Distributions",
    "section": "",
    "text": "TinkerPlots™ also provides other types of plots, including the box plot (sometimes called the box-and-whiskers plot) and the hat plot (a variation of the box plot).↩︎",
    "crumbs": [
      "Evaluating Hypotheses",
      "Describing Distributions"
    ]
  },
  {
    "objectID": "03-01-simulation-process.html",
    "href": "03-01-simulation-process.html",
    "title": "Simulation Process for Evaluating Hypotheses",
    "section": "",
    "text": "Statistical Hypotheses\nTo illustrate the ideas behind statistical hypothesis testing, imagine you had flipped a coin 100 times and it had produced heads in 64 of those flips. This proportion of .64 (or 64%) heads made you suspicious about whether the coin was fair and you now want to evaluate whether it is fair, or whether it is producing heads too often.\nTo determine whether the coin is fair or not, we will write and evaluate a set of hypotheses about the population. Hypotheses are mathematical statements about population parameters which are often formed based on prior knowledge and theory in the area of content. In the social sciences, we typically write out two hypotheses about the population parameters: the null hypothesis (\\(H_0\\)), often referred to as a statement of no effect, and the alternative hypothesis (\\(H_A\\)), often termed the research hypothesis. For example, here are a set of potential hypotheses about our coin:\n\\[\n\\begin{split}\nH_0:~ &\\text{The proportion of heads when flipping the coin is equal to 0.50.}\\\\\nH_A:~ &\\text{The proportion of heads when flipping the coin is greater than 0.50.}\n\\end{split}\n\\]\nThere are a few things to notice about these hypotheses:\nAdditionally, statisticians often use the language of mathematics to express these hypotheses. The same hypotheses expressed via the language of mathematics are:\n\\[\n\\begin{split}\nH_0:~ &\\pi_{\\mathrm{Heads}} = 0.50\\\\\nH_A:~ &\\pi_{\\mathrm{Heads}} &gt; 0.50\n\\end{split}\n\\]\nThe Greek letter pi (\\(\\pi\\)) denotes a population proportion or probability.1 (In symbolic notation for statistical hypotheses, pi is not the mathematical constant of 3.14.) The subscript of “Heads” indicates that we are considering the proportion of heads in all flips of the coin.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Simulation Process for Evaluating Hypotheses"
    ]
  },
  {
    "objectID": "03-01-simulation-process.html#statistical-hypotheses",
    "href": "03-01-simulation-process.html#statistical-hypotheses",
    "title": "Simulation Process for Evaluating Hypotheses",
    "section": "",
    "text": "The statements are about the proportion of heads (i.e., about a summary measure).\nThe statements are about the population (all flips of the coin), not the sample.\nThe null hypothesis (\\(H_0\\)) is a statement of equality (is equal to).\nThe alternative hypothesis often indicates the researcher’s belief about the population summary (e.g., we think the coin is producing more heads than tails).\n\n\n\n\n\nYou can always write hypotheses descriptively, without resorting to the symbolic notation. If you are comfortable with the mathematical symbols, feel free to use it; the mathematical notation acts as a shorthand to quickly state a hypothesis and define the model used. As you read research articles or take other courses, you will see statistical hypotheses stated in many ways, so it is good to understand that there are many ways to express the same thing.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Simulation Process for Evaluating Hypotheses"
    ]
  },
  {
    "objectID": "03-01-simulation-process.html#modelsimulateevaluate",
    "href": "03-01-simulation-process.html#modelsimulateevaluate",
    "title": "Simulation Process for Evaluating Hypotheses",
    "section": "Model–Simulate–Evaluate",
    "text": "Model–Simulate–Evaluate\nThe process we will use for evaluating a hypothesis is:\n\nCreate a model that conforms to the null hypothesis.\nUse the selected model to generate many, many sets of data (Monte Carlo simulation). The results you collect and pool together from these trials will give a picture of the variation you would expect if the null hypothesis is true.\nEvaluate whether the results observed in the actual data (not the simulated data) are compatible with the expected results produced from the null hypothese. This acts as evidence of support (or nonsupport) for the null hypothesis.\n\nTo help you remember this process, you can use the more simplistic mnemonic:\n\nModel\nSimulate\nEvaluate\n\nThis may sound like a straight-forward process, but in practice it can actually be quite complex—especially as you are reading research articles and trying to interpret the findings. First off, the model that is selected is often not provided, nor described, explicitly within most research articles. It is often left to the reader to figure out what the assumed model was. At first, this may be quite difficult, but like most tasks, as you gain experience in this course and as you read more research, you find that there are a common set of models that are typically used by researchers.\n\n\nModel\nThe model that you use in the Monte Carlo simulation is directly related to the null hypothesis. In our coin flip example, recall that the null hypothesis was:\n\\[\nH_0:~ \\pi_{\\mathrm{Heads}} = 0.50\n\\]\nWe can set up a TinkerPlots sampler based on this hypothesis. In our sampler, we need a device that has two potential outcomes (heads and tails). The heads outcome needs to have a probability of 0.50. We then set the repeat value to 100 to mimic our sample observed data in which we flipped the coin 100 times.\n\n\n\n\n\n\n\n\nFigure 1: TinkerPlots sampler to model the null hypothesis that a coin if fair and produces heads with a probability of 0.50.\n\n\n\n\n\n\n\n\nSimulate\nWe can now use the model to produce the proportion of heads we would expect if we flipped the coin 100 times. To do this, we will run the simulation to obtain the outcomes from our first trial (i.e., 100 flips). After plotting those outcomes and summarizing them with the percentage of heads and tails, we could collect the percentage of heads. Then using the collect function, we could carry out an additional 499 trials (500 total trials).\n\n\n\n\n\n\n\n\nFigure 2: The first trial produced 100 coin flips that we plot and summarize. We then collect the percentage of heads and carry out 499 additional trials.\n\n\n\n\n\n\n\n\nEvaluate\nTo evaluate the hypotheses, we plot the summary measures from our 500 trials of the siumulation. This allows us to understand the sampling variation we expect if the null hypothesis is true.\n\n\n\n\n\n\n\n\nFigure 3: The plot of summary measures (percentage of heads) for the 500 trials. The divider shows the range where most of the results are.\n\n\n\n\n\nWe can then use this plot to make a statement about which summary values we expect to see because of this sampling variation. In our example that might be the following:\n\nIf the null hypothesis is true, in 100 flips of a coin we expect to see between 38% and 62% of the flips to be heads.\n\nThat is, if we see that between 38% and 62% of the 100 flips to be heads it is consistent with the coin being fair. But, what we observed was that 64% of the 100 flips were heads. This is a higher proportion of heads than we expect if the coin were fair. Because our observed result of 64% heads was more extreme than what we expect if the null hypothesis is true, it acts as evidence against the null hypothsis. Because of this we would reject the null hypothesis that the coin is fair and conclude that the coin is indeed producing a higher percentage of heads than it should even after we account for the sampling variation.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Simulation Process for Evaluating Hypotheses"
    ]
  },
  {
    "objectID": "03-01-simulation-process.html#footnotes",
    "href": "03-01-simulation-process.html#footnotes",
    "title": "Simulation Process for Evaluating Hypotheses",
    "section": "",
    "text": "In general Greek letters represent population parameters while Roman letters represent sample statistics.↩︎",
    "crumbs": [
      "Evaluating Hypotheses",
      "Simulation Process for Evaluating Hypotheses"
    ]
  },
  {
    "objectID": "03-03-standard-deviation.html",
    "href": "03-03-standard-deviation.html",
    "title": "Quantifying Variation: The Standard Deviation",
    "section": "",
    "text": "Example Data\nRecall that the mean is a single number that can be used to summarize the data; it is a summary of the typical value in the data. Of course, not every observation in a distribution is at the typical value (in fact all of them might be different from the typical value). Thus, it is also useful to have a summary measure of how different the data tends to be from this typical value. This summary is what statisticians refer to as the standard deviation. This measure quantifies variability by determining how far data cases typically deviate from the mean value.\nConsider the following data which give the admission rates for 15 public universities. The plot of the data suggests that the distribution of admission rates is unimodal and roughly symmetric. The mean admission rate for these universities is 0.782. There is also variability in the admission rates, with most univerisites in the sample having an admission rate between 0.7 and 0.9.\nFigure 1: Data and plot showing the admission rates for 15 public universities.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Quantifying Variation: The Standard Deviation"
    ]
  },
  {
    "objectID": "03-03-standard-deviation.html#calculating-the-standard-deviation",
    "href": "03-03-standard-deviation.html#calculating-the-standard-deviation",
    "title": "Quantifying Variation: The Standard Deviation",
    "section": "Calculating the Standard Deviation",
    "text": "Calculating the Standard Deviation\nThe formula for the standard deviation is:\n\\[\n\\mathrm{SD} = \\sqrt{\\frac{\\sum(x-\\bar{x})^2}{n-1}}\n\\]\nwhile this may look ominous it really isn’t. The key is to break it down into its different steps.\n\nCalculate the deviations from the mean\nSquare those deviations\nSum the squared deviations\nDivide that sum by \\(n-1\\)\nTake the square root\n\nLet’s walk through this with our admissions data!\n\n\nStep 1: Calculate the deviations from the mean\nOur first step is to calculate the deviations from the mean. To do this we take each data point and subtract the mean. For example, the deviation for the first admission rate is calculatedas:\n\\[\n0.98 - 0.782 = 0.198\n\\]\nThe positive value of the deviation tells us that this admission rate is higher than the mean admission rate by 0.198.\n\n\n\nThe admission rates and their deviations from the mean of 0.782.\n\n\nAdm_Rate\nDeviation\n\n\n\n\n0.98\n0.198\n\n\n0.81\n0.028\n\n\n0.74\n-0.042\n\n\n0.65\n-0.132\n\n\n0.80\n0.018\n\n\n0.62\n-0.162\n\n\n0.63\n-0.152\n\n\n0.85\n0.068\n\n\n0.80\n0.018\n\n\n0.77\n-0.012\n\n\n0.68\n-0.102\n\n\n0.79\n0.008\n\n\n0.88\n0.098\n\n\n0.81\n0.028\n\n\n0.92\n0.138\n\n\n\n\n\n\n\n\nStep 2: Square those deviations\nIn the next step we square the deviations we just calculated.\n\n\n\nThe admission rates, their deviations from the mean of 0.782, and their squared deviations.\n\n\nAdm_Rate\nDeviation\nSquared.Deviation\n\n\n\n\n0.98\n0.198\n0.039204\n\n\n0.81\n0.028\n0.000784\n\n\n0.74\n-0.042\n0.001764\n\n\n0.65\n-0.132\n0.017424\n\n\n0.80\n0.018\n0.000324\n\n\n0.62\n-0.162\n0.026244\n\n\n0.63\n-0.152\n0.023104\n\n\n0.85\n0.068\n0.004624\n\n\n0.80\n0.018\n0.000324\n\n\n0.77\n-0.012\n0.000144\n\n\n0.68\n-0.102\n0.010404\n\n\n0.79\n0.008\n0.000064\n\n\n0.88\n0.098\n0.009604\n\n\n0.81\n0.028\n0.000784\n\n\n0.92\n0.138\n0.019044\n\n\n\n\n\n\n\n\nStep 3: Sum the squared deviations\nIn Step 3, we sum all of the squared deviations.\n\\[\n\\begin{split}\n0.15384 = 0.039204 &+ 0.000784 + 0.001764 + 0.017424 + 0.000324 + 0.026244 +\\\\\n0.023104 &+ 0.004624 + 0.000324 + 0.000144 + 0.010404 + 0.000064 +\\\\\n0.009604 &+ 0.000784 + 0.019044\n\\end{split}\n\\]\n\n\n\nStep 4: Divide that sum by \\(n-1\\)\nIn our sample we have 15 data points, so \\(n = 15\\). We are now dividing our sum of squared deviations by \\(n-1\\), or in our example, by 14.\n\\[\n\\frac{0.15384}{14} = 0.01098857\n\\]\n\n\n\nStep 5: Take the square root\nFinally we are going to take the square root.\n\\[\n\\sqrt{0.01098857} = 0.1048264\n\\]  \n\n\nInterpreting the Standard Deviation\nTo understand how we interpret this value, let’s again look back at the formula:\n\\[\n\\mathrm{SD} = \\sqrt{\\frac{\\sum(x-\\bar{x})^2}{n-1}}\n\\]\nNotice that the square root essentially removes the squaring that we did. If we omit the squaring and square root, that essentially leaves us with adding deviations and then essentially dividing by how many deviations there are. This is like the formula for average. So the standard deviation is essentially computing the average deviation from the mean. That is, it is indicating how far away from the mean value observations in the distribution are, on average.\nIn our example, the standard deviation was 0.1048. This tells us that, on average, the admission rates in our distribution are 0.1048 away from the mean. Another way to think about this is that most observations in the distribution are within 0.1048 units of the mean. Since the mean is 0.782, that tells us that most admission rates are in the range of \\(0.782 \\pm 0.1048\\), or are between 0.6772 and 0.8868.\n\nFYI\nFrom statistical theory, we know that most of the observations in a distribution are within one standard deviation of the mean.\n\nFrom the formula, we can also derive two other truths about the standard deviation.\n\nThe standard deviation cannot be a negative value. While the deviations from the mean can be nagative, once we square these, they become positive. Also, remember that the standard deviation is measuring the average distance from the mean; distances are non-negative.\nThe only way that the standard deviation ban be 0 is if every deviation is 0. That implies that every observation has the value of the mean. Another way to think about that is that every observation has to havethe same value—there is no variation.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Quantifying Variation: The Standard Deviation"
    ]
  },
  {
    "objectID": "03-03-standard-deviation.html#tinkerplots-101-computing-the-standard-deviation",
    "href": "03-03-standard-deviation.html#tinkerplots-101-computing-the-standard-deviation",
    "title": "Quantifying Variation: The Standard Deviation",
    "section": "Tinkerplots 101: Computing the Standard Deviation",
    "text": "Tinkerplots 101: Computing the Standard Deviation\nIn practice, we will not use the formula to compute the standard deviation, but instead will use TinkerPlots to do the computation. Unlike other summary measures that we have computed (which are computed in the plot), the standard deviation is computed in the case table. To compute the standard deviation for the data in a case table, we first create a new attribute called “SD” in our case table.\n\nClick on the attribute name &lt;New&gt; and edit the text to SD\nRight-click the SD (attribute name) and select Edit Formula\n\nThis will open the formula editor. In the formula editor window type stdDev() which is the TinkerPlots formula for standard deviation. Inside the parentheses, we need to include the exact name of the attribute that you want to compute the standard deviation for. In our example, the name of that attribute is AdmRate. So the entire formula would read: stdDev(AdmRate).\n\nAfter you have entered your formula click the Apply button. This should populate the SD attribute in the case table with the value of the standard deviation.\nClick OK to close the formula editor.\n\n\n\n\n\n\n\n\n\nFigure 2: LEFT: Right-click the SD attribute name and select ‘Edit Formula’. RIGHT: Write the formula for computing the standard deviation in the formula editor.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Quantifying Variation: The Standard Deviation"
    ]
  },
  {
    "objectID": "03-03-standard-deviation.html#standard-deviation-and-the-normal-distribution",
    "href": "03-03-standard-deviation.html#standard-deviation-and-the-normal-distribution",
    "title": "Quantifying Variation: The Standard Deviation",
    "section": "Standard Deviation and the Normal Distribution",
    "text": "Standard Deviation and the Normal Distribution\nEarlier, it was pointed out that most of the observations in a distribution are within one standard deviation of the mean. For specific distributions we can be much more specific about the percantage of observations that are within one standard deviation of the mean. One of those distributions is the normal distribution. In the normal disribution 68% of the observations are within one standard deviation of the mean. We also know that 95% of the observations in a normal distribution are within two standard deviations of the mean, and 99% of the observations are within three standard deviations of the mean.\n\n\n\n\n\n\n\n\nFigure 3: In a normal distribution 68% of the observations are within one standard deviation of the mean, 95% of the observations are within two standard deviations of the mean, and 99% of the observations are within three standard deviations of the mean.\n\n\n\n\n\n\nFWIW\nThis distribution of observations in the normal distribution is referred to as the Empirical Rule, or the 68–95–99 Rule.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Quantifying Variation: The Standard Deviation"
    ]
  },
  {
    "objectID": "03-04-range-of-likely-values.html",
    "href": "03-04-range-of-likely-values.html",
    "title": "Range of Likely Values",
    "section": "",
    "text": "Expected Value\nRecall that to evaluate a hypothesis, we build a model under the assumption that the null hypothesis is true, and then use that model to generate data from whih we produce summary measures. We then create a range of likely values for those summary measures that we use to evaluate the summary measure from the actual observed data. Up until now, you have been eyeballing the distribution of summary measures and creating that range of likely values subjectively. We are now going to introduce a more formal method for creating that range of likely values. To formally create the range of likely values, we need two pieces of information:\nTo help us understand this, we will use the coin flip example introduced in Simulation Process for Evaluating Hypotheses. In that example, recall that we were evaluating whether or not the coin was fair, and the statistical hypotheses were:\n\\[\n\\begin{split}\nH_0:~ &\\text{The proportion of heads when flipping the coin is equal to 0.50.}\\\\\nH_A:~ &\\text{The proportion of heads when flipping the coin is greater than 0.50.}\n\\end{split}\n\\]\nThe expected value is the value for the summary measure that we expect under the null hypothesis. In our example, if the null hypothesis is true, we expect the proportion of heads to be 0.50.\n\\[\n\\text{Expected Value} = 0.50\n\\]\nThe expected value is a theoretical concept in that it is the mean value of the summary measure if we carried out the simulation an infinite number of times. If we look at the distribution of the proportion of heads in our 500 trials, you will notice that the mean is close to 0.50.\nFigure 1: The plot of summary measures (percentage of heads) for the 500 trials. The mean if the 500 trials is 50.018% (which corresponds to 0.50018). The expected value is the mean we expect if we carried out an infinite number of trials. In this example, the expected value is 0.50.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Range of Likely Values"
    ]
  },
  {
    "objectID": "03-04-range-of-likely-values.html#expected-value",
    "href": "03-04-range-of-likely-values.html#expected-value",
    "title": "Range of Likely Values",
    "section": "",
    "text": "PROTIP\nThe mean of the collected summary measures in your simulation should be close to the expected value from the null hypothesis. If it isn’t, you may want to check that you carried out the simulation correctly. One common issue is that you collected the wrong summary measure (e.g., counts rather than percents).",
    "crumbs": [
      "Evaluating Hypotheses",
      "Range of Likely Values"
    ]
  },
  {
    "objectID": "03-04-range-of-likely-values.html#standard-error",
    "href": "03-04-range-of-likely-values.html#standard-error",
    "title": "Range of Likely Values",
    "section": "Standard Error",
    "text": "Standard Error\nThe standard error is a measure of the sampling variation we expect in a summary measure—a quantification of how much variation we would get simply because the summary measure is from a different sample. In practice, the standard error is the standard deviation we compute from the collection of summary measures in our simulation. In our example,\n\\[\n\\text{Standard Error} = 5.13\\%\n\\]\nOr as a proportion,\n\\[\n\\text{Standard Error} = .0513\n\\]\n\n\n\n\n\n\n\n\nFigure 2: The standard error is the standard deviation of the 500 summary measures we collected in our simulation.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Range of Likely Values"
    ]
  },
  {
    "objectID": "03-04-range-of-likely-values.html#range-of-likely-values",
    "href": "03-04-range-of-likely-values.html#range-of-likely-values",
    "title": "Range of Likely Values",
    "section": "Range of Likely Values",
    "text": "Range of Likely Values\nTo compute our range of likely values we are going to combine the expected value and the standard error. Most statisticians define likely results as those that are within two standard errors of the expected value. (This is based on the Empirical Rule which states that in a normal distribution about 95% of the potential summary measures would be within two standard errors of the expected value.) The formula for the range of likely values is thus:\n\\[\n\\text{Range of Likely Values} = \\text{Expected Value} \\pm 2(\\text{Standard Error})\n\\]\nIn our example, the range of likely values (for percent) would be:\n\\[\n\\begin{split}\n50\\% &\\pm 2(5.13\\%) \\\\\n50\\% &\\pm10.26\\% \\\\\n39.74&\\% \\text{ to } 60.26\\%\n\\end{split}\n\\]\nOr, the range of likely values (for proportion) would be:\n\\[\n\\begin{split}\n0.50 &\\pm 2(0.0513) \\\\\n0.50 &\\pm0.1026 \\\\\n0.3974& \\text{ to } 0.6026\n\\end{split}\n\\]\nWe can then use this to complete our sentence about what we expect under the null hypothesis:\n\nIf the null hypothesis is true (the coin is fair), out of 100 flips we expect to see a proportion of heads between 0.3974 and 0.6026.\n\nWe can then evaluate the actual observed data based on this range. Recall that we observed that 0.64 of the flips were heads in the actual data. This is outside the range of likely values—we observed a higher proportion of heads than we expect if the coin is fair. Because of this, we reject the null hypothesis which suggests that the coin may not be fair.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Range of Likely Values"
    ]
  },
  {
    "objectID": "03-05-generalization.html",
    "href": "03-05-generalization.html",
    "title": "Generalization and Random Sampling",
    "section": "",
    "text": "Statistical Bias\nThe goal in many studies is to provide information about some characteristic of a population. For example, you may want to say something about the percentage of Americans who would support a particular piece of legislation. Or, you may want to provide information about the average amount of time University of Minnesota students take to graduate. One potential solution to obtain such information would be to collect the necessary data from every member of the target population.\nIn many studies, however, it may not be feasible given time and money constraints to collect data from each member of the population. In these cases it is only possible to consider data collected for a smaller subset, or sample from that population. In these cases, the characteristic of the population would be estimated from the sample data and inferences would be drawn about the population. The key is then to carefully select the sample so that the results estimated from the sample are representative of the characteristic in the larger population. Drawing conclusions about the larger population based on information from a sample is called statistical inference.\nIn statistical inference, generalization refers to the approipriateness of using sample data to draw conclusions about the larger population from which the sample was drawn. In practice, statisticians compute a summary measure from their sample (called a statistic) and use it as as estimate for that same summary measure in the population (called a parameter).\nWhether that sample statistic is a statistically good estimate of the population parameter depends on whether the sample is statistically “representative” of the population. In order for the sample to be statistically representative of the population, the sampling units (i.e., cases) in the sample need to have been chosen using an unbiased sampling method—that is, the selection of sample cases has not introduced statistical bias. Statistical bias is when sample statistics differ systematically from the population parameter. The key here is the word “systematically”. This implies that there is something in the underlying process (aside from random variation) that is affecting the estimation process.\nTo help you think about bias, imagine a person, Arthur Dent, has lost his keys. The actual location of the keys, the Library, is akin to the population parameter. Arthur believes he lost his keys at the Supermarket and searches several places around the Supermarket. The locations where Arthur searches are like sample statistics.\nFigure 1: This figure is a metaphor for statistical bias.\nFigure 1 is a metaphor for the concept of statistical bias. Arthur’s search locations (sample statistics) are systematically in the wrong place. On average, where Arthur searched (the middle of the yellow circle) is not the actual location of the keys. Compare this with the search locations in Figure 1.\nFigure 2: This figure is a metaphor for unbiasedness.\nFigure 1 is a metaphor for unbiasedness. On average, where Arthur searched is the location of the keys. There are a couple of other concepts that this metaphor can help us think about.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Generalization and Random Sampling"
    ]
  },
  {
    "objectID": "03-05-generalization.html#statistical-bias",
    "href": "03-05-generalization.html#statistical-bias",
    "title": "Generalization and Random Sampling",
    "section": "",
    "text": "Even in Figure 1, none of the actual search locations were right at the keys. Some of the locations were too far to the left of the keys, and others were too far to the right of the keys. However, ON AVERAGE, the search locations “found” the keys. The way we define unbiased is that the AVERAGE of the statistics is at the population parameter.\nAverage has nothing to do with the size of the yellow circle. (The size of the circle is related to the amount of sampling variation, a concept we will deal with in Unit 4.) The two figures in Figure 3 (below) also illustrate unbiasedness (left) and bias (right), despite the size of the yellow circle.\nThe last concept about bias to point out is that bias (or unbiasedness) is a property of the sampling method. The reason the search locations were not in the right place is because the method Arthur used to pick the search locations was biased. He thought he lost his keys in the Supermarket, so that is where he looked.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Unbiased sample with the a lot of sampling variation.\n\n\n\n\n\n\n\n\n\n\n\n(b) Biased sample with the very little sampling variation.\n\n\n\n\n\n\n\nFigure 3: Bias and sampling variation are two different ideas.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Generalization and Random Sampling"
    ]
  },
  {
    "objectID": "03-05-generalization.html#unbiased-sampling-method",
    "href": "03-05-generalization.html#unbiased-sampling-method",
    "title": "Generalization and Random Sampling",
    "section": "Unbiased Sampling Method",
    "text": "Unbiased Sampling Method\nThe key to whether study results generalize to the larger population is whether the sample cases were selected from that population using an unbiased sampling method. One unbiased sampling method is random sampling. Random sampling uses chance to select the sampling units (participants) from the larger population. When random sampling has been employed in a study, the unbiasedness of the sampling method is strong evidence for generalization; we have a much higher belief in generalizations to the larger population.\nThere are many random sampling methods employed by researchers. For example:\n\nSimple random sampling;\nStratified random sampling; and\nCluster random sampling.\n\nThe most elementary of these methods is simple random sampling. To draw a simple random sample we need a list of EVERY member of the population. This list is called the sampling frame. (Obtaining a sampling frame can be very difficult. Try obtaining a list of everyone who lives in the United States!) Then we employ randomness to draw out sampling units, with the caveat that each unit in the sampling frame has an equal chance of being drawn.\nThis is akin to drawing names out of a hat. Everyone’s name is written down (the sampling frame) and added to the hat. Then a chance mechanism is employed to draw out a name. Ideally, there would be no sytematic bias in the selection of names from the hat (e.g., all the pieces of paper would be the same size; the person drawing would be equally likely to draw a name from the top of the hat as the bottom of the hat).\n\nKEY QUESTION FOR GENERALIZATION\nOne key question to ask when you are considering whether generalizations are appropriate is: How were the sample cases selected? If the answer is they were selected using an unbiased sampling method like simple random sampling, then your sample findings do generalize to the population. If the answer is that they were not selected using an unbiased sampling method, then the sample findings probably do NOT generalize to the population.",
    "crumbs": [
      "Evaluating Hypotheses",
      "Generalization and Random Sampling"
    ]
  },
  {
    "objectID": "04-00-comparing-groups.html",
    "href": "04-00-comparing-groups.html",
    "title": "Comparing Groups",
    "section": "",
    "text": "Scientific Experiments: The Gold Standard of Group Comparisons\nGroup comparisons are at the heart of many interesting questions addressed by psychologists, physicians, scientists, teachers, and engineers. For example, data scientists at OKCupid, an online dating site, examined whether frequent tweeters (users of Twitter) have shorter real-life relationships than others.1 In another example, researchers from Vanderbilt University asked the question of whether students who enter college with A.P. credits graduate with less student debt than their peers who do not have A.P. credits.2\nLiao (2002) summed up group comparisons in the following manner: “The nature of doing science, be it natural or social, inevitably calls for comparison. Statistical methods are at the heart of such comparison, for they not only help us gain understanding of the world around us but often define how our research is to be carried out.”\nQuestions about group differences are often studied through scientific experiments. When considering a scientific experiment to examine group differences, the design of the study plays a very important role. To help understand this, think about a researcher who is studying the efficacy of a new cold medication. Let’s say that the researcher has 100 people (each with a cold) who volunteer to be a part of her study. Let’s consider how she might design her study.\nAll three designs have been used, and are still used, in research studies. There are pros and cons to each of the designs, and all are useful depending on what you want to know.\nIn Design 1, it is hard to judge the efficacy of the medication. For example, what if 60 of the volunteers had no cold symptoms after four days? Did the medication work? You might be thinking, “what would have happened if they hadn’t received any medication?” That is a great question. In this design, we don’t know.\nDesign 2 gives the researcher a comparison group. She can compare the number of volunteers in each group who have no cold symptoms after four days. This is a better design than Design 1 for examining efficacy. But, what if she found that after four days, 35 of the volunteers who got the medication had no symptoms, while only 25 of the volunteers who didn’t receive medication had no symptoms. Is this enough evidence for her to say the cold medication is effective? Probably not. Maybe most of the volunteers in the treatment group were already in later stages of their colds. Maybe they had more robust immune systems to begin with (e.g., due to differing exercise or nutrition habits) than the control group. You can imagine many such reasons that the treatment group would show quicker improvement than the control group.\nDesign 3 has the same comparison group advantage as Design 2. The big difference, however, is that the volunteers were put into the groups at random. By assigning participants at random, the researcher “equalizes” the treatment and control groups. What this means is that the groups have, on average, the SAME nutritional habits, the SAME exercise habits, and the SAME everything-else. That means that the only thing that is different between the two groups is that the treatment group got the cold medication and the control group didn’t. If the researcher uses this type of design, she can draw much stronger inferences about WHY the treatment group improved: it was because of the cold medication!",
    "crumbs": [
      "Comparing Groups"
    ]
  },
  {
    "objectID": "04-00-comparing-groups.html#scientific-experiments-the-gold-standard-of-group-comparisons",
    "href": "04-00-comparing-groups.html#scientific-experiments-the-gold-standard-of-group-comparisons",
    "title": "Comparing Groups",
    "section": "",
    "text": "Design 1: She gives the cold medicine to all 100 volunteers.\nDesign 2: She gives the cold medicine to the first 50 volunteers (treatment group) and nothing to the other 50 volunteers (control group).\nDesign 3: She randomly picks 50 of the volunteers to whom she gives the cold medicine (treatment group), and she gives nothing to the other 50 volunteers (control group).\n\n\n\n\n\n\nKEY COMPONENTS TO SCIENTIFIC EXPERIMENTS\nThe two key components of a scientific experiment are that there is a control and a treatment group , and participants are randomly asdsigned to either the treatment or control group. In the control group participants do not receive the treatment. This allows the control group to act as a reference group for our comparison. The random assignment makes the treatment and control groups statistically equivalent on all other factors so that we fan attribute any differences to the fact that the treatment group got the treatment and the control group did not.",
    "crumbs": [
      "Comparing Groups"
    ]
  },
  {
    "objectID": "04-00-comparing-groups.html#goals-of-unit-3",
    "href": "04-00-comparing-groups.html#goals-of-unit-3",
    "title": "Comparing Groups",
    "section": "Goals of Unit 3",
    "text": "Goals of Unit 3\nIn this unit, you will learn about experimental variation (the variation that we get just because of random assignment to groups). You will explore how to account for experimental variation and model that variation using a randomization test (a Monte Carlo method for evaluating whether an observed result in compatible with experimental variation from a hypothesized model). You will also learn how to carry out a randomization test using TinkerPlots. Finally, you will also learn about the importance of random assignment in drawing causal conclusions about the effect of a treatment in group comparisons.",
    "crumbs": [
      "Comparing Groups"
    ]
  },
  {
    "objectID": "04-00-comparing-groups.html#references",
    "href": "04-00-comparing-groups.html#references",
    "title": "Comparing Groups",
    "section": "References",
    "text": "References\n\n\n\n\nLiao, T. F. (2002). Statistical group comparison. Wiley.",
    "crumbs": [
      "Comparing Groups"
    ]
  },
  {
    "objectID": "04-00-comparing-groups.html#footnotes",
    "href": "04-00-comparing-groups.html#footnotes",
    "title": "Comparing Groups",
    "section": "",
    "text": "The website OKTrends includes an answer to this question, as well as many others.↩︎\nThey found students with 10 or more AP credits have $1,000 less debt, on average, than those who don’t. Those students aslo tend to graduate earlier, are more likely to double major, and attend graduate school (see here).↩︎",
    "crumbs": [
      "Comparing Groups"
    ]
  },
  {
    "objectID": "04-01-experimental-variation.html",
    "href": "04-01-experimental-variation.html",
    "title": "Experimental Variation",
    "section": "",
    "text": "Treatments and Effects\nIn group comparisons we are interested in comparing the summary measure for one group to the same summary measure in another group. When we do this, before we can draw any conclusions about whether one group’s summary measure is higher or lower than the other’s, we need to account for experimental variation. Experimental variation is the ides that in group comparisons, the summary measure for each group is impacted by which participants are assigned to which group. To help understand this, consider the following example:\nDr. Bunsen Honeydew wants to evaluate the effects of a magic bean that he hypothesizes will help people grow faster. He plans to evaluate this by carrying out a statistical experiment in which he will randomly assign six subjects he has recruited to two groups. The people in the first group will be given the magic bean (treatment group) and the people in the second group will not be given the magic bean (control group). After a year, he will measure the heights of people in both groups and compare their means to evaluate whether the magic bean impacted the treatment group’s heights.\nFigure 1 shows the random assignment of the six people in the sample to the treatment and control groups and the results of Dr. Honeydew’s study. The people in the treatment group grew on average 3.70 inches over the course of the year and those in the control group grew on average 2.63 inches over the course of the year.\nThe people in the treatment group who consumed the magic bean grew 1.07 inches more than the people in the control group did. However, this is not enough evidence to conclude that the magic bean is effective. To see why this is the case, we have to understand how statisticians think about the effects of treatments.\nLet’s consider a person in the treatment group, say Ariel. Their growth after one year was 5.0 inches. If the magic bean is effective, then part of Ariel’s growth is due to eating the magic bean, and part is just growth that would have occurred anyway (even if Ariel didn’t eat the magic bean). Mathmatically, we could write Ariel’s growth as:\n\\[\n5.0 = T + e\n\\]\nwhere T is the growth attributed to the treatment (in this case, the magic bean) and e is the growth that would have occurred anyway without the bean. We could write such an equation for everyoe in the tretament group.\n\\[\n\\begin{split}\n\\text{Ariel:}~~~5.0 &= T + e_{\\mathrm{Ariel}}\\\\\n\\text{Belle:}~~~2.9 &= T + e_{\\mathrm{Belle}}\\\\\n\\text{Jasmine:}~~~3.2 &= T + e_{\\mathrm{Jasmine}}\\\\\n\\end{split}\n\\]\nHere we add a subscript to the e value just to indicate that the amount of growth that naturally occurs over the course of the year can be different for each person in the treatment group.1\nWe can also write a similar equation for everyone in the control group. The difference is that nobody in the control group ate the magic bean, so it couldn’t have impacted their growth. Mathematically, that implies that \\(T=0\\) for those equations. As an example, take Aurora’s growth, which was 2.7 inches:\n\\[\n\\begin{split}\n2.7 &= T + e_{\\mathrm{Aurora}}\\\\\n2.7 &= 0 + e_{\\mathrm{Aurora}}\\\\\n2.7 &= e_{\\mathrm{Aurora}}\n\\end{split}\n\\]\nWe can write this equation for everyone in the control group:\n\\[\n\\begin{split}\n\\text{Aurora:}~~~2.7 &= e_{\\mathrm{Aurora}}\\\\\n\\text{Cinderella:}~~~2.1 &= e_{\\mathrm{Cinderella}}\\\\\n\\text{Moana:}~~~3.1 &= e_{\\mathrm{Moana}}\\\\\n\\end{split}\n\\]\nIn this case all of growth for people in our control group is just a function of the growth that occurred over that year.",
    "crumbs": [
      "Comparing Groups",
      "Experimental Variation"
    ]
  },
  {
    "objectID": "04-01-experimental-variation.html#no-effect-of-treatment",
    "href": "04-01-experimental-variation.html#no-effect-of-treatment",
    "title": "Experimental Variation",
    "section": "No Effect of Treatment",
    "text": "No Effect of Treatment\nNow consider a scenario where the magic bean does not impact growth at all; that is the treatment has no effect on growth. In this scenario, \\(T=0\\) for not just those people in the control group, but also for everyone in the treatment group:\n\\[\n\\begin{split}\n\\text{Ariel:}~~~5.0 &= e_{\\mathrm{Ariel}}\\\\\n\\text{Belle:}~~~2.9 &= e_{\\mathrm{Belle}}\\\\\n\\text{Jasmine:}~~~3.2 &= e_{\\mathrm{Jasmine}}\\\\\n\\text{Aurora:}~~~2.7 &= e_{\\mathrm{Aurora}}\\\\\n\\text{Cinderella:}~~~2.1 &= e_{\\mathrm{Cinderella}}\\\\\n\\text{Moana:}~~~3.1 &= e_{\\mathrm{Moana}}\\\\\n\\end{split}\n\\]\nIf there is no effect of treatment, everyone’s growth is just what naturally occurs over the course of a year. Another way to think about this is that regardless of what group people were assigned to at the beginning of the study, their growth would have been exactly the same as what it was. For example, had Moana been randomlly assigned to the treatment group and eaten the magic bean, her growth still would have been 3.1 inches since the bean has no effect on growth. Similarly, Ariel’s growth would have been 5.0 inches, even if she had been assigned to the control group!",
    "crumbs": [
      "Comparing Groups",
      "Experimental Variation"
    ]
  },
  {
    "objectID": "04-01-experimental-variation.html#variation-in-groups-due-to-random-assignment",
    "href": "04-01-experimental-variation.html#variation-in-groups-due-to-random-assignment",
    "title": "Experimental Variation",
    "section": "Variation in Groups due to Random Assignment",
    "text": "Variation in Groups due to Random Assignment\nLet’s continue assuming there is no effect of treatment. But now, let’s go back to the point in the study where Dr. Honeydew carried out the random assignment. What if the random assignment had come out differently? There are, after all, many other ways the six people could have been randomly assigned to two groups.\nFigure 2 shows another possible random assignment of the six people in the sample to two groups. In this random assignment the mean growth for the people in the treatment group was 3.27 inches and that for the control group was 3.07 inches. Remember that there is no effect of the magic bean, so the difference we are seeing in the two means ( a difference of 0.20 inches in favor of the treatment group) is just a function of who was assigned to the treatment group and who was assigned to the control group.\n\n\n\n\n\n\n\n\nFigure 2: Another way the six people in the sample could have been randomly assigned to two groups. The mean growth for people in the treatment group was 3.27 inches and that for the control group was 3.07 inches. If there is no effect of the magic bean, the difference in mean growth is just a function of who is in each group.\n\n\n\n\n\nFigure 3 shows yet another possible random assignment of the six people in the sample to two groups. In this random assignment the mean growth for the people in the treatment group was 3.67 inches and that for the control group was 2.67 inches. Again, since there is no effect of the magic bean, the difference we are seeing in the two means (a difference of 1.00 inches in favor of the treatment group) is just a function of who was assigned to the treatment group and who was assigned to the control group.\n\n\n\n\n\n\n\n\nFigure 3: Yet another way the six people in the sample could have been randomly assigned to two groups. The mean growth for people in the treatment group was 3.67 inches and that for the control group was 2.67 inches. If there is no effect of the magic bean, the difference in mean growth is just a function of who is in each group.\n\n\n\n\n\nThe key thing to notice is that different random assignments produce different differences in the means. And, under the scenario that there is no effect of treatment, these differences are all a function of who was in the group…not because the treatment actually worked. This variation in the differences in our summary measures between the two groups (which is all just a fucntion of the random assignment and not related to the treatment at all) is what statisticians call experimental variation.\nExperimental variation means that we cannot just look at the original results of Dr. Honeydew’s experiment and claim that the because the treatment group grew 1.07 inches more than the control group did, on average, that the magic beans worked. This difference might completely be due to experiemental variation. In order to evaluate whether the magic beans increased people’s height, we need to evaluate whether our difference of 1.07 inches is more than we expect because of experimental variation.",
    "crumbs": [
      "Comparing Groups",
      "Experimental Variation"
    ]
  },
  {
    "objectID": "04-01-experimental-variation.html#collecting-the-differences-in-means",
    "href": "04-01-experimental-variation.html#collecting-the-differences-in-means",
    "title": "Experimental Variation",
    "section": "Collecting the Differences in Means",
    "text": "Collecting the Differences in Means\nTo evaluate the amount of experimental variation we expect if there is no impact of treatment, we could continue our process of:\n\nRandomly assign people to treatment and control groups;\nCompute the mean growth for both groups; and\nFind the difference in means by taking the control group mean and subtracting it from the treatment group mean.\n\nWe would repeat this many times. Figure 4 shows the plot of these differences in means for roughly 100 trials of this process. Similar to what we did in the previous unit, we can qauntify the experimental variation by computing the standard error of these differences and then finding the range of likely results.\n\n\n\n\n\n\n\n\nFigure 4: A plot of the difference in mean for many potential random assignments of people to treatment and control groups.",
    "crumbs": [
      "Comparing Groups",
      "Experimental Variation"
    ]
  },
  {
    "objectID": "04-01-experimental-variation.html#statistical-hypotheses-for-comparing-groups",
    "href": "04-01-experimental-variation.html#statistical-hypotheses-for-comparing-groups",
    "title": "Experimental Variation",
    "section": "Statistical Hypotheses for Comparing Groups",
    "text": "Statistical Hypotheses for Comparing Groups\nThe two explanations we have for people’s growth constitute the statistical hypotheses, namely that (1) the treatment adds to peoples natural growth, or (2) it doesn’t. Writing these hypotheses:\n\\[\n\\begin{split}\nH_0:&~ \\text{There is no effect of treatment (magic bean) on peoples’ growth.}\\\\\nH_A:&~ \\text{There is an effect of treatment (magic bean) on peoples’ growth.}\n\\end{split}\n\\]\n\nPROTIP\nThe null hypothesis is always the “no effect of treatment” hypothesis\n\nOne really important thing to note about the differences in Figure 4 is that, on average, the difference between the mean treatment growth and the mean control growth is 0. This implies that if there is no effect of treatment, then on average, the difference between the means for the two groups is 0. So another way we can write the statistical hypothesis is in terms of this difference:\n\\[\n\\begin{split}\nH_0:~&\\text{There is no difference in the average growth between people in the treatment}\\\\\n&\\text{group and those in the control group.}\\\\[1em]\nH_A:~&\\text{There is a positive difference in the average growth between people in the}\\\\\n&\\text{treatment group and those in the control group.}\n\\end{split}\n\\]\nHere, in the alternative hypothesis we say the difference will be positive since Dr. Honeydew’s research hypothesis was that the magic bean would add to people’s natural growth, which would imply that the mean growth for the people in the treatment group should be HIGHER than the mean growth fo the people in the control group. We could also use mathematical notation to write these hypotheses about the difference in means:\n\\[\n\\begin{split}\nH_0:&~ \\mu_t - \\mu_c = 0\\\\\nH_A:&~ \\mu_t - \\mu_c &gt; 0\n\\end{split}\n\\]\nRemember statistical hypotheses always use Greek letters! Here the Greek letter mu is what we use to indicate a mean.",
    "crumbs": [
      "Comparing Groups",
      "Experimental Variation"
    ]
  },
  {
    "objectID": "04-01-experimental-variation.html#footnotes",
    "href": "04-01-experimental-variation.html#footnotes",
    "title": "Experimental Variation",
    "section": "",
    "text": "Note that the T does not have a subscript. This implies that the impact of treatment on growth is exactly the same for everyone in the treatment group. This is a simplifying assumption that we use to make the problem easier; typically we think about this as the average impact of the treatment.↩︎",
    "crumbs": [
      "Comparing Groups",
      "Experimental Variation"
    ]
  },
  {
    "objectID": "04-02-tinkerplots-101-randomization-test.html",
    "href": "04-02-tinkerplots-101-randomization-test.html",
    "title": "TinkerPlots 101: Randomization Test",
    "section": "",
    "text": "Observed Data\nThe hypothesis test we perform to evaluate group differences is referred to as a randomization test. In this test, we are going to model experimental variation assuming the null hypothesis that there is no effect of treatment on the outcome is true. Using that model, we will replicate the random assignment of subjects (and their outcome value) to the two conditions/groups. Based on that re-randomization to groups we will calculate and record a measure of how different the conditions are on the outcome (e.g., calculate the difference in means). We will repeat this re-randomization and summarization a large number of times. This will give us a sense for the experiemental variation (i.e., the variation that is expected in the differences just because of random assignment).\nTo illustrate how to use TinkerPlots to carry out a randomization test, we will again visit our example from the previous reading:\nFigure 1 shows the random assignment of the six people in the sample to the treatment and control groups and the results of Dr. Honeydew’s study. The people in the treatment group grew on average 3.70 inches over the course of the year and those in the control group grew on average 2.63 inches over the course of the year.\nFigure 1: The six people in the sample are randomly assigned to two groups. The mean growth for people in the treatment group was 3.70 inches and that for the control group was 2.63 inches.\nThe people in the treatment group who consumed the magic bean grew 1.07 inches more than the people in the control group did. To get that we used:\n\\[\n\\begin{split}\n\\text{Difference} &= \\bar{x}_{t} - \\bar{x}_c\\\\\n&= 3.70 - 2.63 \\\\\n&= 1.07\n\\end{split}\n\\]\nIn this equation, \\(\\bar{x}_t\\) is the mean growth for the treatment group and \\(\\bar{x}_c\\) is the mean growth for the control group.",
    "crumbs": [
      "Comparing Groups",
      "TinkerPlots 101: Randomization Test"
    ]
  },
  {
    "objectID": "04-02-tinkerplots-101-randomization-test.html#statistical-hypotheses-for-comparing-groups",
    "href": "04-02-tinkerplots-101-randomization-test.html#statistical-hypotheses-for-comparing-groups",
    "title": "TinkerPlots 101: Randomization Test",
    "section": "Statistical Hypotheses for Comparing Groups",
    "text": "Statistical Hypotheses for Comparing Groups\nThe two explanations we have for people’s growth constitute the statistical hypotheses, namely that (1) the treatment adds to peoples natural growth, or (2) it doesn’t. Writing these hypotheses:\n\\[\n\\begin{split}\nH_0:&~ \\text{There is no effect of treatment (magic bean) on peoples’ growth.}\\\\\nH_A:&~ \\text{There is an effect of treatment (magic bean) on peoples’ growth.}\n\\end{split}\n\\]\nRemember that if there is no effect of treatment, then on average, the difference between the means for the two groups is 0. So another way we can write the statistical hypothesis is in terms of this difference. Here we write the hypotheses using the language of mathematics:\n\\[\n\\begin{split}\nH_0:&~ \\mu_t - \\mu_c = 0\\\\\nH_A:&~ \\mu_t - \\mu_c &gt; 0\n\\end{split}\n\\]",
    "crumbs": [
      "Comparing Groups",
      "TinkerPlots 101: Randomization Test"
    ]
  },
  {
    "objectID": "04-02-tinkerplots-101-randomization-test.html#model",
    "href": "04-02-tinkerplots-101-randomization-test.html#model",
    "title": "TinkerPlots 101: Randomization Test",
    "section": "Model",
    "text": "Model\nIn order to carry out a randomization test using TinkerPlots, you need to include two sampling devices in the sampler. The first sampling device will include the observed response data for all of the subjects. The second device will contain the observed grouping data (i.e., the conditions) for all of the subjects. In our example, the reponses are the amount of growth each person exhibited, and the group/condition data are the conditions of “Bean” (treatment) and “No Bean” (control).\n\nResponses Data: 5.0, 2.9, 3.2, 2.7, 2.1, 3.1\nGrouping Data: Bean, Bean, Bean, No Bean, No Bean, No Bean\n\nNote that since we need to have a response and group for each participant, there should be 6 total responses and 6 total groups. In TinkerPlots, we will begin by setting up a sampler that includes a Mixer containing the response data for the 6 subjects. We will also change how the mixer samples elements. All of our sampling devices to date in this class have been sampling with replacement—that is the exact same element can be sampled over and over. This is appropriate when we are modeling sampling variation. But when modeling experimental variation, we are trying to mimic the random assignment to different groups, and people are only assinged to a group one time. To mimic this we need to sample without replacement.\n\nYou can change how the sampling device samples elements by clicking on the Device Options button for the sampling device (upside-down triangle below the device).\nTo sample with replacement select Replacement &gt; Without Replacement.\n\n\n\n\n\n\n\n\n\nFigure 2: A TinkerPlots sampler including a mixer containing the six responses (i.e., growth values). To mimic random assignment we will sample elements without replacement.\n\n\n\n\n\n\nPROTIP\nTo tell whether you are sampling with or without replacement, you can look at the top of the sampling device. If the top of the sampling device is capped, then the device is sampling with replacement. If the top of the sampling device is open, then the device is sampling without replacement.\n\n\n\n\n\n\n\n\nFigure 3: If the top of the sampling device is capped, then the device is sampling with replacement. If the top of the sampling device is open, then the device is sampling without replacement.\n\n\n\n\n\n\nIn addition to randomly generating the response values, we also need to randomly generate the group that the value will be randomly assigned to. To do this, we will link a second sampling device to the mixer. (This is similar to the sampler you worked with in the Pet Factory activity which had multiple linked samplers.) A good sampling device to use for the groups is a Stacks device. To add a linked Stacks sampling device to your sampler:\n\nDrag a Stacks sampling device from the device menu to the right-hand side of the existing mixer. The sampler should now contain two devices linked by a grey line.\nAdd the names of the possible conditions/groups as elements. (In our example there are two elements–“Bean” and “No Bean”)\nClick on Stacks Device Options (upside-down triangle below the sampling device) and select Show Count. Change the count value for the “Bean” label to reflect the number of participants originally assigned to the magic bean condition (in our example this is 3). Change the count value for the “No Bean” label to reflect the number of participants originally assigned to the control condition (again, this is 3 in our example).\nChange the device to sample values without replacement.\n\n\n\n\n\n\n\n\n\nFigure 4: A TinkerPlots sampler including a mixer containing the six responses (i.e., growth values) and a linked stacks device containing the six group labels. To mimic random assignment we sample elements without replacement in both devices.\n\n\n\n\n\nTo model the random assignment of the treatment condition labels that might have occurred, you need to produce simulated data from another model that generates labels of JFK and JFKC. To do this you will use the Stacks sampling device. We also need to include this sampling device in the same Sampler as the outcomes. To do this, you link multiple sampling devices in the same sampler the same way you did in the Cat Factory course activity.\n\nFYI\nWhen you add a second sampling device, the Draw value will automatically change to the number of devices included in the sampler. Double-check that it is 2 when you have two sampling devices.",
    "crumbs": [
      "Comparing Groups",
      "TinkerPlots 101: Randomization Test"
    ]
  },
  {
    "objectID": "04-02-tinkerplots-101-randomization-test.html#simulate-first-trial",
    "href": "04-02-tinkerplots-101-randomization-test.html#simulate-first-trial",
    "title": "TinkerPlots 101: Randomization Test",
    "section": "Simulate: First Trial",
    "text": "Simulate: First Trial\nOnce the sampler has been set up, the Run Button can be clicked to carry out the first simulated random assignment. The outcomes from both linked devices are recorded in the case table, each in their own attribute. In addition, an attribute called Join is also created that includes the outcomes of both linked devices separated by a comma. (We will ignore this column!)\nAs with every simulation we have done, we will plot the simulated data from this first trial and compute a summary measure, which for our example is the difference in means. To plot the data we will drag the response attribute (growth values) to the x-axis of the plot. This should be fully separated and stacked. Then, the group/condition attribute can be dragged to the y-axis of the plot. Finally, the averages can be added to the plot along with their numeric values in the same manner as in previous simulations.\n\n\n\n\n\n\n\n\nFigure 5: The plot of the simulated data from the first trial. Averages and their numeric values are also displayed on the plot.\n\n\n\n\n\nAt this point we could calculate the difference in means (subtracting in the same order as we did for the observed data), but we want to have TinkerPlots automate this, so we are going to collect both means. Remember, to collect a mean we right-click on the mean triangle and select Collect Statistic. Once the mean has been added to the case table, collect the other mean as well. Now you should have both the “Bean” and “No Bean” means in a case table o in the same row (but in different columns).\nTo compute the difference in means we will use the Formula Editor. To do this:\n\nCreate a third attribute (column) in the case table by clicking the column name, &lt;new&gt;. Rename this attribute “Difference”\nSelect the “Difference” attribute to highlight it and then right-click the attribute and select Edit Formula.\nSelect the Attribute triangle to display the names of the case table’s attributes in the Formula Editor.\nDouble-click the attribute for the “Bean” groups mean value. Then click the subtraction key (–) in the Formula Editor calculator. Finally, double-click the attribute for the “No Bean” group’s mean value.\nClick the Apply button and then click OK.\n\n\n\n\n\n\n\n\n\nFigure 6: LEFT: The formula editor showing the subtraction to find the difference in means. RIGHT: The case table where the means were collected also now includes the difference in means.\n\n\n\n\n\nBased on this simulated random assignment,, the difference in means is \\(-1.2\\).",
    "crumbs": [
      "Comparing Groups",
      "TinkerPlots 101: Randomization Test"
    ]
  },
  {
    "objectID": "04-02-tinkerplots-101-randomization-test.html#simulate-collect-results-from-additional-trials",
    "href": "04-02-tinkerplots-101-randomization-test.html#simulate-collect-results-from-additional-trials",
    "title": "TinkerPlots 101: Randomization Test",
    "section": "Simulate: Collect Results from Additional Trials",
    "text": "Simulate: Collect Results from Additional Trials\nOnce you have collected the means and used the Formula Editor to compute the difference, click the Run button on your sampler again. This should add another set of means and their difference to your case table. Click Run a couple times to be sure that the simulation is working. Once you are convinced the simulation is working properly, collect the difference in means for additional trials so tyhat you have 500 total differences.\nOnce you have the 500 simulated differences, we can plot them and also compute the standard error to quantify the amount of experimental variation. After plotting them, we should do a spot check that the average is near 0. (Recall that 0 is the expected value of the difference underthe null hypothesis and that the plot should be centered at the expected value.)\nIn our example, the standard error is 0.7868. We can use this to compue the range of likely values:\n\\[\n\\begin{split}\n\\text{Range of Likely Values} &= \\text{Expected Value} \\pm 2(SE) \\\\\n&= 0 \\pm 2(0.7868) \\\\\n&= -1.57~\\text{to}~1.57\n\\end{split}\n\\]\nThat is:\n\nIf the null hypothesis is true (and there is no effect of the magic bean), we expect to see a difference in means of between \\(-1.57\\) and 1.57.\n\nWe can also display this range of likely values on our plot using the divider tool.\n\n\n\n\n\n\n\n\nFigure 7: LEFT: The formula editor is used to calculate the standard error. RIGHT: The plot of simulated differences is centered at 0 and the range of likely values is added with a divider.",
    "crumbs": [
      "Comparing Groups",
      "TinkerPlots 101: Randomization Test"
    ]
  },
  {
    "objectID": "04-02-tinkerplots-101-randomization-test.html#evaluate",
    "href": "04-02-tinkerplots-101-randomization-test.html#evaluate",
    "title": "TinkerPlots 101: Randomization Test",
    "section": "Evaluate",
    "text": "Evaluate\nFinally, we can evaluate our observed difference in means (which was 1.07) using our range of likely values to make a decision about the null hypothesis and answer our research question. In our example, the observed difference of 1.07 is within the range of likely values which implies we would fail to reject the null hypothesis; we conclude that there is no difference in the mean growth between the magic bean group and the no magic bean group. In other words, there is no evidence to support that the magic bean is contributing to people’s growth.",
    "crumbs": [
      "Comparing Groups",
      "TinkerPlots 101: Randomization Test"
    ]
  },
  {
    "objectID": "04-03-tinkerplots-101-data-structure-for-comparing-groups.html",
    "href": "04-03-tinkerplots-101-data-structure-for-comparing-groups.html",
    "title": "TinkerPlots 101: Data Structure For Comparing Groups",
    "section": "",
    "text": "Adding Data to the Sampling Device\nThe data for comparing groups may be presented in a variety of ways, depending on how it was collected and recorded. For example, one common structure that group data is recorded is the data for each group is recorded in a seperate column.\nWhile this structure for the data may be useful for recording the data, it is not the structure we need when we use statistical software (including TinkerPlots) to analyze the data. Most software requires that we have all of the response data for both groups in a single column. Then a second column indicates the group associated with these responses.\nAside from needing the data in this structure to create a plot that allows us to compare the groups and obtain the group means in TinkerPlots, there is one other big advantage to this structure. When we create our sampler for the randomization test, since all of the response data is in a single column in the case table, we can copy the entire attribute and paste the results into our mixer (rather than having to record each data point on a mixer element one-at-a-time).\nTo do this, after your data are entered into a case table in the appropriate structure, click on the attribute name to highlight the column with the response values. Then use key commands to copy the attribute. (On a Mac the key command for copy is Command-C, and on a PC it is Control-C.) Then in the mixer, use the remove elements icon (the - below the sampling device) to remove all of the elements in the mixer until it is empty. Click on the empty mixer and then use key commands to paste the response values into the mixer. (On a Mac the key command for paste is Command-V, and on a PC it is Control-V.)\nFigure 1: LEFT: Click on the attribute name to highlight the column and copy this using key commands. RIGHT: Remove all of the elements from the mixer so it is empty. Click on the empty mixer and then use key commands to paste.",
    "crumbs": [
      "Comparing Groups",
      "TinkerPlots 101: Data Structure For Comparing Groups"
    ]
  },
  {
    "objectID": "04-03-tinkerplots-101-data-structure-for-comparing-groups.html#adding-data-to-the-sampling-device",
    "href": "04-03-tinkerplots-101-data-structure-for-comparing-groups.html#adding-data-to-the-sampling-device",
    "title": "TinkerPlots 101: Data Structure For Comparing Groups",
    "section": "",
    "text": "FYI\nIt is easiest to enter the group data in the stacks device manually.",
    "crumbs": [
      "Comparing Groups",
      "TinkerPlots 101: Data Structure For Comparing Groups"
    ]
  },
  {
    "objectID": "04-04-cause-and-effect.html",
    "href": "04-04-cause-and-effect.html",
    "title": "Cause-and-Effect Inferences",
    "section": "",
    "text": "Statistical Experiments: The Key to Cause-and-Effect Inference\nOne of the most powerful type of inferences that researchers can draw is cause-and-effect. This type of inference allows researchers to attribute any group differences to the treatment that was given to the treatment group. For example, in the memorization activity we did in class, a cause-and-effect inference would allow us to attribute the improvement in average score for the familiar chunking group to the familiar chunking. That is, the reason that there is improved scores (the effect) is because of the familiar chunking (the cause).\nCause-and-effect inferences are useful in a variety of disciplines. For example, medical researchers may be interested in showing that a drug helps improve people’s health (the cause of improvement is the drug), while educational researchers may be interested in showing a curricular innovation improves students’ learning (the curricular innovation causes improved learning). However, it is often difficult to draw cause-and-effect inferences. To attribute a causal relationship, there are three criteria a researcher needs to establish:\nBecause of this third criteria, attributing a cause-and-effect relationship is very difficult. (You can read more about each of these criteria at the Web Center for Social Research Methods.)\nStudies that employ a statistical experiment are the key to making cause-and-effect inferences. This is because to rule out ALL other possible explanations for the effect, the control group and the treatment group need to be “identical” with respect to every possible characteristic (aside from the treatment) that could explain differences. This way the only characteristic that will be different is that the treatment group gets the treatment and the control group doesn’t. If there are differences in the outcome, then it must be attributable to the treatment, because the other possible explanations are ruled out.\nSo, in order to make causal inferences, we need to make the control and treatment groups “identical” when we create them. One thing that makes this task (slightly) easier is that they don’t have to be exactly identical, only probabilistically equivalent. This means, for example, that if you were matching groups on age that you don’t need the two groups to have identical age distributions; they would only need to have roughly the same AVERAGE age. Here roughly means “the average ages should be the same within what we expect because of sampling error.”\nNow we just need to create the groups so that they have, on average, the same characteristics … for EVERY POSSIBLE CHARCTERISTIC that could explain differences in the outcome. Zoinks!1\nIt turns out that creating probabilistically equivalent groups is a really difficult problem. But, one method that works pretty well for doing this is to randomly assign participants to the groups. This works best when you have large sample sizes, but even with small sample sizes random assignment has the advantage of at least removing the systematic bias between the two groups (any differences are due to chance and will probably even out between the groups). As Wikipedia’s page on random assignment points out,",
    "crumbs": [
      "Comparing Groups",
      "Cause-and-Effect Inferences"
    ]
  },
  {
    "objectID": "04-04-cause-and-effect.html#statistical-experiments-the-key-to-cause-and-effect-inference",
    "href": "04-04-cause-and-effect.html#statistical-experiments-the-key-to-cause-and-effect-inference",
    "title": "Cause-and-Effect Inferences",
    "section": "",
    "text": "Random assignment of participants helps to ensure that any differences between and within the groups are not systematic at the outset of the experiment. Thus, any differences between groups recorded at the end of the experiment can be more confidently attributed to the experimental procedures or treatment. … Random assignment does not guarantee that the groups are matched or equivalent. The groups may still differ on some preexisting attribute due to chance. The use of random assignment cannot eliminate this possibility, but it greatly reduces it.\n\n\nKEY QUESTION FOR CAUSE-AND_EFFECT INFERENCE\nOne key question to ask when you are considering whether cause-and-effect inference is appropriate is: How were the sample cases assigned to groups? If the answer is they were assigned to groups randomly, then you can make a causal attribution about the group differences to the treatment. If the answer is that they were not assigned to groups randomly, then you cannot ake a causal attribution because there may be some other systematic difference between the groups that may be causing the differences.",
    "crumbs": [
      "Comparing Groups",
      "Cause-and-Effect Inferences"
    ]
  },
  {
    "objectID": "04-04-cause-and-effect.html#footnotes",
    "href": "04-04-cause-and-effect.html#footnotes",
    "title": "Cause-and-Effect Inferences",
    "section": "",
    "text": "According to Wiktionary the earliest usage of the work “zoinks” was by Norville “Shaggy” Rogers on the show Scooby-Doo.↩︎",
    "crumbs": [
      "Comparing Groups",
      "Cause-and-Effect Inferences"
    ]
  },
  {
    "objectID": "04-05-bootstrap-test.html",
    "href": "04-05-bootstrap-test.html",
    "title": "Bootstrap Test: Modeling Sampling Variation",
    "section": "",
    "text": "Bootstrapping: Modeling Sampling Variation when Comparing Groups\nIn some studies, researchers do not assign study participants to groups/conditions. As an example, imagine that Dr. Bunsen Honeydew wants to study whether Comedians or Musicians have more followers on Instagram (IG). He collects data by selecting 5 of his favorite comedians and 5 of his favorite musicians and then getting the number of followers from IG. These data are shownin Table 1.\nIn this study, the two groups being compared are comedians and musicians. The 10 subjects in the data, of course, were not assigned by Dr. Honeydew to these groups — they “self-selected” into the groups based on whether they chose to become a comedian or a musician. When the study participants are not assigned to conditions by a researcher the study is referred to as an observational study.\nIn observational studies, we often think about the underlying variation that we need to account for to make statistical inferences differently than we do when participants have been assigned to groups by the researcher. In the latter situation (i.e., statistical experiments), we model and account for the experimental variation that arises due to random assignment. In observational studies, since there is no random assignment, we instead model and account for sampling variation, similar to when we were trying to account for random sampling.\nTo analyze data from an observational study, we need to adapt our randomization test to account for sampling variation rather than experimental variation. To account for sampling variation in the randomization test, we change replacement option for the sampling device producing the response/outcome attribute so that the it is being sampled with replacement. (Note that the group labels should still be sampled without replacement since we want to model the same number of participants in each group as in any observational data.) Sampling the outcome values with replacement is called bootstrapping and the subsequent test to compare groups is referred to as a bootstrap test.\nFigure 1 shows the TinkerPlots sampler to carry out a bootstrap test using Dr. Honeydew’s data.\nFigure 1: The sampler for carrying out a bootstrap test to model sampling variation. In this sampler the responses are being sampled with replacement and the groups are being sampled without replacement.",
    "crumbs": [
      "Comparing Groups",
      "Bootstrap Test: Modeling Sampling Variation"
    ]
  },
  {
    "objectID": "04-06-modeling-variation-to-compare-groups.html",
    "href": "04-06-modeling-variation-to-compare-groups.html",
    "title": "Modeling Variation to Compare Groups",
    "section": "",
    "text": "When we compare groups we now need to consider two different sources of variation that impact the differences in summary measures: experimental variation and sampling variation. In practice, these sources of variation are a function of the study design—how participants are selected from the larger population, and how they are assigned to groups.\n\n\n\n\n\n\n\n\nFigure 1: The two key questions for determining the source of variation that needs to be modeled are: (1) How are people selected into the sample from the population? and (2) How are people assigned to groups?. For both questions was it random or not?\n\n\n\n\n\nIf we dichotomize the answers to these two key questions as either “randomly” or “not randomly”, this results in four potential study designs. These designs are listed in Table 1 along with the source of variation that we need to model for each of them.\n\n\n\n\nTable 1: Four potential study designs based on whether the answers to the two key questions are ‘random’ or ‘not random’. The source of variation we model in practice for each study design is also indicated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow are people selected into the sample from the population?\nHow are people assigned to groups?\nVariation to be Modeled\n\n\n\n\nRandom selection\nRandom assignment\nSampling variation(from selection from the population)\n\n\nNon-random selection\nRandom assignment\nExperimental variation(from assignment to groups)\n\n\nRandom selection\nNon-random assignment\nSampling variation(from selection from the population)\n\n\nNon-random selection\nNon-random assignment\nExperimental variation(from assignment to groups)\n\n\n\n\n\n\n\n\n\n\nLastly, we indicate the test and the corresponding TinkerPlots sampler for modeling these sources of variation in Table 2.\n\n\n\n\nTable 2: How to model different sources of variation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariation Modeled\nTest\nTinkerPlots Sampler\n\n\n\n\nExperimental variation(from assignment to groups)\nRandomization\nTwo sampling devices:   (1) outcomes (sampled without replacement); and   (2) group labels (sampled without replacement)\n\n\nSampling variation(from selection from the population)\nBootstrap\nTwo sampling devices:   (1) outcomes (sampled with replacement); and   (2) group labels (sampled without replacement)",
    "crumbs": [
      "Comparing Groups",
      "Modeling Variation to Compare Groups"
    ]
  },
  {
    "objectID": "04-07-dummy-coding.html",
    "href": "04-07-dummy-coding.html",
    "title": "Working with Categorical Responses",
    "section": "",
    "text": "Summarizing Group Differences\nOften, the responses that researchers are comparing are categorical attributes. For example, a researcher may be interested in evaluating whether one group reads more than another group. To evaluate this they ask everyone in both groups whether they read a book in the previous week and collect the response which is either “Yes” or “No”. The response attribute is then the “Yes” and “No” responses from the participants. If we wanted to evaluate whetehr one group read more than another, how would we summarize the groups reading? And how does this change what we do in the randomization test to evaluate the observed difference?\nContinuing with our reading example, imagine you had the following response data from both 2-year and 4-year college students about whether they read a book in the previous week:\nTable 1: Response data from college students about whether they read a book in the previous week.\n\n\n\n\n\n\n\n\n\n2-Year College\n4-Year College\n\n\n\n\nYes\nNo\n\n\nNo\nYes\n\n\nNo\nYes\n\n\nYes\nYes\n\n\nYes\nNo\n\n\nNo\nYes\n\n\nNo\nYes\n\n\nYes\nYes\nOne way we can summarize categorical data is with counts. Here we find that 4 students from 2-year colleges and 3 students from 4-year colleges read a book in the previous week. When we present counts, we typically do so in a contingency table. In the contingency table, the rows and columns indicate the response attribute and the grouping attribute, respectively. Each cell represents the count for a particular combination of those two attributes.\nFigure 1: Contingency table for the example data.\nOne problem with counts for comparing groups if the group sizes are different is that they can be misleading. Here, if we used counts, we would say that students from 2-year colleges read more than students from 4-year colleges. The problem is that there were more 2-year college students in our sample than there were 4-year college students.\nInstead of counts, when comparing groups, statisticians use proportions. This alleviates the problem of unequal group sizes. In our data the proportion of 2-year college students that read a book in the previous week is 0.50 (4 out of 8), and the proportion of 4-year college students that read a book in the previous week is 0.75 (3 out of 4).",
    "crumbs": [
      "Comparing Groups",
      "Working with Categorical Responses"
    ]
  },
  {
    "objectID": "04-07-dummy-coding.html#statistical-hypotheses",
    "href": "04-07-dummy-coding.html#statistical-hypotheses",
    "title": "Working with Categorical Responses",
    "section": "Statistical Hypotheses",
    "text": "Statistical Hypotheses\nThe two statistical hypotheses are now about the proportion of responses. In our example, if the researcher hypthesized that a higher proportion of 4-year college students than 2-year college students read a book in the previous week, the statistical hypotheses would be:\n\\[\n\\begin{split}\nH_0:~&\\text{There is no difference in the proportion of 2-year and and 4-year}\\\\\n&\\text{college students who read a book in the previous week.}\\\\[1em]\nH_A:~&\\text{A higher proportion of 4-year college students}\\\\\n&\\text{than 2-year college students read a book in the previous week.}\n\\end{split}\n\\]\nWriting these mathematically:\n\\[\n\\begin{split}\nH_0:~&\\pi_{\\text{4-Year}} - \\pi_{\\text{2-Year}} = 0\\\\\nH_A:~&\\pi_{\\text{4-Year}} - \\pi_{\\text{2-Year}} &gt; 0\n\\end{split}\n\\]\nwhere the Greek letter \\(\\pi\\) is the symbol we use for proportion.",
    "crumbs": [
      "Comparing Groups",
      "Working with Categorical Responses"
    ]
  },
  {
    "objectID": "04-07-dummy-coding.html#dummy-coding",
    "href": "04-07-dummy-coding.html#dummy-coding",
    "title": "Working with Categorical Responses",
    "section": "Dummy Coding",
    "text": "Dummy Coding\nBefore we carry out our randomization test, we introduce the concept of dummy coding, a method that statisticians use to deal with categorical attributes. Dummy coding is a way to re-code categories into numbers, so that the outcome becomes “quantitative”. The idea of dummy coding is that each category of the outcome gets a numerical value of either “1” or “0”.\nFor example, consider the eight 2-year college students in our sample. We are going to dummy code their responses so that anyone who responded “Yes” gets a “1” and anyone who responded “No” gets “0”. Their data looks like this:\n\n\n\n\nTable 2: Response data from 2-year college students about whether they read a book in the previous week. These data have also been dummy coded.\n\n\n\n\n\n\n\n\n\n2-Year College\nRead_Book\n\n\n\n\nYes\n1\n\n\nNo\n0\n\n\nNo\n0\n\n\nYes\n1\n\n\nYes\n1\n\n\nNo\n0\n\n\nNo\n0\n\n\nYes\n1\n\n\n\n\n\n\n\n\n\n\n\nFYI\nThe name of the dummy coded attribute traditionally takes the name of the category that was re-coded as 1. In our example we re-coded “Yes: Read Book” to 1, so the name of the dummy coded variable is “Read_Book”.\n\nOnce the re-coded responses are quantitative, we can summarize the these values using a mean. Doing this for our 2-year college data we get:\n\\[\n\\begin{split}\n\\bar{x} &= \\frac{1 + 0 + 0 + 1 + 1 + 0 + 0 + 1}{8} \\\\\n&= \\frac{4}{8} \\\\\n&= 0.50\n\\end{split}\n\\]\nNote that the mean of the dummy coded atrribute is exactly the same as the proportion of 2-year college students who read a book in the last week! This is because once we dummy code, the numerator of the mean computation is essentially counting the number of “Yes” responses. And once we divide that by the number of students that gives us the proportion.\n\nFYI\nThe mean of a dummy coded attribute is the proportion of responses that were coded as 1.\n\nThe nice thing about using dummy coding is that we can continue to work with means and the difference in means in the randomization test. This implies we don’t have to change anything in the simulation process from what we did with quantitative responses. Figure 2 shows a sampler to carry out the randomization test for the example data.\n\n\n\n\n\n\n\n\nFigure 2: A sampler to carry out the randomization test for the example. The sampling device for the response attribute uses a stacks device to include the dummy coded values for the response attribute which is easier than a mixer when there are only two values for the response.\n\n\n\n\n\nThis sampler could then be used to generate data from simulated random assignments. The difference in means (i.e., difference in proportions) could be collected and the observed difference in means could be evaluated.\n\n\n\n\n\n\n\n\nFigure 3: Distribution of differences in proportions for 500 collected differences. The shaded area represents the range of likely values [-0.628, +0.628] based on the standard error of 0.314. The observed difference of 0.25 is displayed as a vertical reference line.\n\n\n\n\n\nIf the null hypothesis that there is no difference in the proportion of 2-year and and 4-year college students who read a book in the previous week is true, we would expect to see an observed difference in proportions between \\(-0.628\\) and 0.628. We actually saw an observed difference of 0.25. This is consistent with what we would have expected if the null hypothesis is true. Because of this we fail to reject the null hypotheis; it may be that there is no difference in the proportion of 2-year and and 4-year college students who read a book in the previous week.",
    "crumbs": [
      "Comparing Groups",
      "Working with Categorical Responses"
    ]
  },
  {
    "objectID": "04-08-p-value.html",
    "href": "04-08-p-value.html",
    "title": "Quantifying Evidence Against the Null Hypothesis: p-Value",
    "section": "",
    "text": "Computing a p-Value\nSo far in the course, we have been using the range of likely values to evaluate our observed summary measure. Another way that statisticians evaluate observed summary measures is to compute a measure of evidence called the p-value. The pvalue quantifies the probability that we would see the observed summary that we did (or a summary measure that is more extreme) if the null hypothesis is true. (Note: The “p” in p-value is short for probability).\nTo compute a p-value, you count the number of collected summary measures that are at least as extreme as the observed result, and divide this by the total number of results. This value is then reported as a decimal value.\n\\[\np = \\frac{\\bigg(\\text{number of collected summary measures at least as extreme as the observed summary}\\bigg) + 1}{\\bigg(\\text{total number of collected results in the simulation}\\bigg) + 1}\n\\]\nTo figure out the number of collected summary measures at least as extreme as the observed summary, we need to count the number of summary measures in the direction of the alternative hypothesis that are at least as extreme as the observed summary measure. For example, if the alternative hypothesis is “less than”, we count the number of summary measures less than or equal to our observed summary. On the other hand, if the alternative hypothesis is “greater than”, we count the number of summary measures greater than or equal to our observed summary.\nTo illustrate this, we will re-examine simulation results from the Sleep Deprivation activity Recall in that activity, the two statistical hypotheses (written in the language of mathematics) were:\n\\[\n\\begin{split}\nH_0:~ &\\mu_{\\text{Unrestricted Sleep}} - \\mu_{\\text{Sleep Deprived}} = 0\\\\\nH_A:~ &\\mu_{\\text{Unrestricted Sleep}} - \\mu_{\\text{Sleep Deprived}} &gt; 0\n\\end{split}\n\\]\nAlso recall that the in the observed data, we found a difference in means of 15.9. Figure 1 shows a plot of 100 differences in means simulated under the null hypothesis. A vertical line is shown at the observed difference of 15.9.\nFigure 1: Plot of 100 simulated differences in means from the sleep deprivation study assuming the null hypothesis is true. The observed difference in means of 15.9 is displayed as a vertical reference line. The shaded area indicates the simulated results that are at least as extreme as 15.9 (based on the alternative hypothesis).\nTo compute the p-value we need to count the number of collected summary measures that are at least as extreme as 15.9 (our observed difference). Since the alternative hypothesis was \\(\\mu_{\\text{Unrestricted Sleep}} - \\mu_{\\text{Sleep Deprived}} &gt; 0\\), we need to count the number of collected summary measures that are greater than or equal to 15.9. Here there are 2 simulated results that are at least as extreme as 15.9 (\\(\\geq 15.9\\)).\n\\[\n\\begin{split}\np &= \\frac{\\bigg(\\text{number of collected summary measures } \\geq 15.9\\bigg) + 1}{\\bigg(\\text{total number of collected results in the simulation}\\bigg) + 1} \\\\[1em]\n&= \\frac{2 + 1}{100 + 1} \\\\[1em]\n&= \\frac{3}{101} \\\\[1em]\n&= 0.0297\n\\end{split}\n\\]",
    "crumbs": [
      "Comparing Groups",
      "Quantifying Evidence Against the Null Hypothesis: p-Value"
    ]
  },
  {
    "objectID": "04-08-p-value.html#computing-a-p-value",
    "href": "04-08-p-value.html#computing-a-p-value",
    "title": "Quantifying Evidence Against the Null Hypothesis: p-Value",
    "section": "",
    "text": "FYI\nThe alternative hypothesis dictates the direction of more extreme values.\n\n\n\n\n\n\n\n\nFYI\nAdding one to the numerator and denominator in our p-value calculation assures that we never get a p-value of 0.\nConsider the p-value if our observed result would have been 18 (instead of 15.9). There are 0 results that are at least as extreme as 18 (\\(\\geq 18\\)). Without adding one to the numerator and denominator, we would report a p-value of \\(\\frac{0}{100} =0\\). This implies that seeing a result at least as extreme as 18 under the null hypothesis model is impossible. (remember the “p” stands for probability, and a probability of 0 implies something never occurs!) The problem is that we only ran 100 trials of the simulation. If we had run this simulation for all possible randomizations of the data, we would have seen results \\(\\geq 18\\). Adding one fixes this issue.",
    "crumbs": [
      "Comparing Groups",
      "Quantifying Evidence Against the Null Hypothesis: p-Value"
    ]
  },
  {
    "objectID": "04-08-p-value.html#interpreting-a-p-value",
    "href": "04-08-p-value.html#interpreting-a-p-value",
    "title": "Quantifying Evidence Against the Null Hypothesis: p-Value",
    "section": "Interpreting a p-Value",
    "text": "Interpreting a p-Value\nThe p-value quantifies the probability of seeing the observed summary that we did (or a summary measure that is more extreme) if the null hypothesis is true. So, interpreting our p-value of 0.0297, we would say:\n\nThe probability of seeing a difference in means at least as extreme as 15.9, if the null hypothesis is true that there is no difference in improvement between the unrestricted and sleep deprived groups, is 0.0297.",
    "crumbs": [
      "Comparing Groups",
      "Quantifying Evidence Against the Null Hypothesis: p-Value"
    ]
  },
  {
    "objectID": "04-08-p-value.html#using-the-p-value-to-evaluate-the-null-hypothesis",
    "href": "04-08-p-value.html#using-the-p-value-to-evaluate-the-null-hypothesis",
    "title": "Quantifying Evidence Against the Null Hypothesis: p-Value",
    "section": "Using the p-Value to Evaluate the Null Hypothesis",
    "text": "Using the p-Value to Evaluate the Null Hypothesis\nThis low probability implies that it is quite unlikely that we would see a result as extreme as 15.9, or more extreme, if the null hypothesis is true. Because of this, we would reject the null hypothesis.\nIn this example, it is pretty evident that we should reject the null hypothesis because 0.0297 is quite a low probability. How low does the p-value have to be to reject the null hypothesis? To answer this question, we need to go back to our range of likely values and the Empirical Rule. Remember that the formula for computing our range of likely values was:\n\\[\n\\text{Range of Likely Values} = \\text{Expected Value} \\pm 2(\\text{Standard Error})\n\\]\nAlso recall that the Empirical Rule stated that in a normal distribution 95% of the values in the distribution would be within two standard deviations. Using these two ideas, that implies that in our distribution of collected summaries, 95% of the collected summary measures will be in our range of likely values. Also remember that we rejected the null hypthesis when the observed summary was outside that range of likely values. That means that to reject the null hypothesis, our observed summary had to be in the most extreme 5% of the distribution!\nAnother way of thinking about the p-value is that it is actually indicating where in the distribution the observed summary lies. Because of this, we can evaluate the null hypothesis using the p-value by asking whether the observed difference is in the outer 5% of the distribution of collected summaries. In our example, the p-value of 0.0297 suggests that the observed difference of 15.9 lies in the most extreme 2.97% of the distribution. This is in the outer 5% of the distribution, so we reject the null hypothesis.\nThis evaluation essentially boils down to asking whether the p-value is less than 0.05. If it is, that implies the observed summary is in the outer 5% of the distribution of collected summaries and is quite unlikely if the null hypothesis is true causing us to reject the null hypothesis. If the p-value is greater than or equal to 0.05, that implies the observed summary is NOT in the outer 5% of the distribution of collected summaries and is consistent with a summary expected under the null hypothesis, which causues us to fail to reject the null hypothesis.\n\nVOCABULARY\nStatisticians use the term statistically significant to refer to findings where they reject the null hypothesis (i.e., they fins a p-value less than 0.05).",
    "crumbs": [
      "Comparing Groups",
      "Quantifying Evidence Against the Null Hypothesis: p-Value"
    ]
  },
  {
    "objectID": "04-08-p-value.html#statistical-significance-vs.-practical-significance",
    "href": "04-08-p-value.html#statistical-significance-vs.-practical-significance",
    "title": "Quantifying Evidence Against the Null Hypothesis: p-Value",
    "section": "Statistical Significance vs. Practical Significance",
    "text": "Statistical Significance vs. Practical Significance\nLarge p-values indicate that the observed data are more compatible with the results from the model, while small p-values indicate that the observed data are not very compatible with the results from the model. As researchers, our goal is often to then translate this quantitative evidence into support for the hypothesized model. For example, in the Sleep Deprivation study, we obtained a p-value of 0.0297. This suggests a low degree of compatibility between the observed data (our empirical evidence) and the null hypothesis. Because our p-value is less than 0.05, we might report the results as statistically significant.\nDon’t be fooled by the word “significant” in the phrase “statistically significant”. It does not mean important. All it means is that the observed summary measure is more extreme tahn we expect because of experimental variation if the null hypothesis is true. “Practically significant” results, on the other hand, indicate that the difference in means/proportions is meaningful and impactful in a real-world context. In our example the broader scientific question is about whether sleep deprivation has a harmful effect on learning. Is a difference of 15.9 a meaningful difference? This is a question that cannot be answered by a p-value.\n\nA statistically significant finding may be practically unimportant.\n\nTo know whether the difference is of practical importance, you need to go beyond the p-value to both your experiences and the domain literature. This will help you understand whether the magnitude of difference you observed is meaningful or not. As an example, one study of the impact of SAT coaching found that students who were coached on the SAT improved their SAT math score by an average of 29 points while a control group that was no coached improved their SAT math score by an average of 21 points. The researchers found that the observed difference in mean improcment (which was 8 points) was statistically significant and concluded that coaching improved scores. But, is an improvement of 8 points on the SAT (which is on an 800 point scale) meaningful?\nTwo other things that impact practical importance of findings are the ability to make: (1) generalizations, and (2) cause-and-effect inferences. These abilities, as you are aware, are not governed by the p-value, but by the design of the study: How were particiants selected from the larger population? How were participants assigned to groups? Don’t get too excited about small p-values. Ask whether they are subastantively meaningful, whether the results generalize, and whether a causal infernece can be made. Ron Wassertein, former president of the American Statistical Association reminds us about not getting overly-excited about small p-vlues when he stated:\n\nSmall p-values are like a right-swipe in Tinder. It means you have an interest. It doesn’t mean you’re ready to book the wedding venue.\n\n\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Comparing Groups",
      "Quantifying Evidence Against the Null Hypothesis: p-Value"
    ]
  },
  {
    "objectID": "04-08-p-value.html#six-principles-about-p-values",
    "href": "04-08-p-value.html#six-principles-about-p-values",
    "title": "Quantifying Evidence Against the Null Hypothesis: p-Value",
    "section": "Six Principles about p-Values",
    "text": "Six Principles about p-Values\nBecause they are so ubiquitous in the research literature for any field, and because they are often mis-interpreted (even by PhDs, researchers, and math teachers) it is important to be aware of what a p-value tells you, and more importantly, what it doesn’t tell you. To this end, the American Statistical Association released a statement on p-values in which it listed six principles:1\n\nPrinciple 1: p-values can indicate how incompatible the data are with a specified statistical model.\nPrinciple 2: p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nPrinciple 3: Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nPrinciple 4: Proper inference requires full reporting and transparency.\nPrinciple 5: A p-value does not measure the size of an effect or the importance of a result.\nPrinciple 6: By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.",
    "crumbs": [
      "Comparing Groups",
      "Quantifying Evidence Against the Null Hypothesis: p-Value"
    ]
  },
  {
    "objectID": "04-08-p-value.html#footnotes",
    "href": "04-08-p-value.html#footnotes",
    "title": "Quantifying Evidence Against the Null Hypothesis: p-Value",
    "section": "",
    "text": "Yaddanapudi (2016) published a paper in the Journal of Anaesthesiology, Clinical Pharmacology in which she explains each of these six principles for practicing physician-scientists using an example of treatment efficacy for a drug.↩︎",
    "crumbs": [
      "Comparing Groups",
      "Quantifying Evidence Against the Null Hypothesis: p-Value"
    ]
  },
  {
    "objectID": "05-00-estimating-uncertainty.html",
    "href": "05-00-estimating-uncertainty.html",
    "title": "Estimating Uncertainty",
    "section": "",
    "text": "Goals of Unit 4\nAside from hypothesis testing, one of the most common uses of statistical inference is the estimation of unknown parameters using sample data. Polling is one application where statistical estimation is used. For example, Gallup and the Pew Research Center are organizations that use statistical estimation to provide snapshots of public attitudes and opinions on topics from politics and the economy, to social awareness and health and well-being. The results of their polls are seen on a daily basis in almost every newspaper, news blog and website across the world.\nStatistical estimation is used by more than pollsters. Biologists, social scientists, and medical researchers use statistical estimation to quantify population characteristics. For example, each year the Minnesota Department of Natural Resources estimates the populations of various species of animal, bird, and fish. These estimates are used to help set hunting and fishing regulations, as well as to allocate resources.1\nIn this unit, you will learn about using bootstrapping to quantify the uncertainty in an observed estimate. You will learn how to incorporate that uncertainty into the estimate to produce a ccompatibility interval. You will explore how sample size impacts the amount of uncertainty in our compatibility interval. Finally, you will also learn about how compatibility intervals can be used to evaluate statistical hypotheses.",
    "crumbs": [
      "Estimating Uncertainty"
    ]
  },
  {
    "objectID": "05-00-estimating-uncertainty.html#footnotes",
    "href": "05-00-estimating-uncertainty.html#footnotes",
    "title": "Estimating Uncertainty",
    "section": "",
    "text": "Here is the Wolf Population report for 2016.↩︎",
    "crumbs": [
      "Estimating Uncertainty"
    ]
  },
  {
    "objectID": "05-01-compatibility-intervals.html",
    "href": "05-01-compatibility-intervals.html",
    "title": "Compatibility Intervals: Accounting for Uncertainty in a Statistical Estimate",
    "section": "",
    "text": "Statistical Error as Uncertainty\nThe Pew Research Center is an organization that… In 2023 they carried out a survey to better understand the views and experiences of public K-12 school teachers. The 2,531 teachers surveyed constitute a nationally representative panel of public K-12 school teachers. One question they asked was about challenges they faced in the classroom. Pew reported that 72% of high school teachers surveyed indicated that students being distracted by their cellphones in the classroom is a major problem.\nThis summary of the data (72%) is a statistical estimate of the percentage of public high school teachers in the nation who believe that students being distracted by their cellphones in the classroom is a major problem. When we use a sample summary (i.e., a statistic) as an estimate for the population summary (i.e., parameter), we are making a statistical estimate. In this case, the estimate is referred to as a point estimate. A point estimate is a single number estimate of the population parameter.\nBut, if there is one thing you have learned in this class, it is that had Pew chosen a different sample of teachers, they would have gotten a different point estimate. That is, our point estimate varies because of sampling error. Because of this, when statisticians report sample point estimates, they also typically provide a quantification of the amount of sampling error. In the accompanying technical report, Pew writes that the margin of error at 95% confidence level is +/- 2.4 percentage points. The margin of error is a commonly reported measure statisticians use to quantify the amount of sampling variation when reporting statistical estimates.\nOne way that we can view statistical error (e.g., sampling or experimental variation) is to think about it as measuring “uncertainty”. That is, when we report a statistical estimate from a sample, there is some uncertainty around using that value as a stand-in for the population parameter. The uncertainty comes from not only understanding that different samples would provide different point estimates, but also that a sample is only part of the population, so the estimate is being made from incomplete information about the population. If we take this view of statistical error as uncertainty, the statistical estimate from Pew has more nuance.",
    "crumbs": [
      "Estimating Uncertainty",
      "Compatibility Intervals: Accounting for Uncertainty in a Statistical Estimate"
    ]
  },
  {
    "objectID": "05-01-compatibility-intervals.html#statistical-error-as-uncertainty",
    "href": "05-01-compatibility-intervals.html#statistical-error-as-uncertainty",
    "title": "Compatibility Intervals: Accounting for Uncertainty in a Statistical Estimate",
    "section": "",
    "text": "What is the percentage of public high school teachers in the nation who believe that students being distracted by their cellphones in the classroom is a major problem? Our estimate for this percentage is 72% +/- 2.4%. That is, based on our sample, we believe taht 72% of public high school teachers in the nation believe that students being distracted by their cellphones in the classroom is a major problem. However, we have some uncertainty around that, so the true estimate is likely within 2.4% of that point estimate.",
    "crumbs": [
      "Estimating Uncertainty",
      "Compatibility Intervals: Accounting for Uncertainty in a Statistical Estimate"
    ]
  },
  {
    "objectID": "05-01-compatibility-intervals.html#compatibility-intervals-combining-the-point-estimate-and-margin-of-error",
    "href": "05-01-compatibility-intervals.html#compatibility-intervals-combining-the-point-estimate-and-margin-of-error",
    "title": "Compatibility Intervals: Accounting for Uncertainty in a Statistical Estimate",
    "section": "Compatibility Intervals: Combining the Point Estimate and Margin of Error",
    "text": "Compatibility Intervals: Combining the Point Estimate and Margin of Error\nThere are multiple ways to report the amount of uncertainty in a statistical estimate. One method is to simply report the point estimate and margin of error like Pew did: \\(72\\% \\pm 2.4\\%\\). Another way to report this is to actually carry out the mathematics and report the resulting interval:\n\\[\n\\begin{split}\n72\\% &\\pm 2.4\\%\\\\\n[72\\% - 2.4\\%,&~~72\\% + 2.4\\%]\\\\\n[69.6\\%,&~~74.4\\%]\n\\end{split}\n\\]\nThis is referred to as a compatibility interval (i.e., a confidence interval). A compatibility interval acknowledges the uncertainty in the estimate by giving a range of potential values for the estimate of the parameter.\n\nWe believe that the percentage of public high school teachers in the nation who believe that students being distracted by their cellphones in the classroom is a major problem is somewhere between 69.6% and 74.4%.\n\nAs you interpret compatibility intervals, there are a couple things to keep in mind.\n\nThe compatibility interval is being used to estimate the population parameter.\nThe compatibility interval gives a range of compatible values for the population parameter.\nEach value in the range is not equally compatible with the data (Values in the middle of the range are more compatible with the data than values at the ends of the range.) Moreover, the values outside the range are not incompatibile with the data; just far, far less compatible.",
    "crumbs": [
      "Estimating Uncertainty",
      "Compatibility Intervals: Accounting for Uncertainty in a Statistical Estimate"
    ]
  },
  {
    "objectID": "05-02-tinkerplots-101-bootstrap-uncertainty.html",
    "href": "05-02-tinkerplots-101-bootstrap-uncertainty.html",
    "title": "TinkerPlots 101: Estimating Uncertainty via Bootstrapping",
    "section": "",
    "text": "Point Estimate\nTo illustrate how to compute compatibility intervals using TinkerPlots, consider the following example: A recent survey of randomly selected Australian teenagers asked about mobile-phone ownership. The survey was administered to 189 teenagers between the ages of 12 and 14. Results indicated that 138 of them owned a mobile-phone. The goal is to estimate the percentage of ALL Australian teenagers between the ages of 12 and 14 who own a mobile phone.\nBased on the observed data, our point estimate for the the percentage of all Australian teenagers between the ages of 12 and 14 who own a mobile phone is 73% (\\(\\frac{138}{189} = 0.73\\)).",
    "crumbs": [
      "Estimating Uncertainty",
      "TinkerPlots 101: Estimating Uncertainty via Bootstrapping"
    ]
  },
  {
    "objectID": "05-02-tinkerplots-101-bootstrap-uncertainty.html#margin-of-error",
    "href": "05-02-tinkerplots-101-bootstrap-uncertainty.html#margin-of-error",
    "title": "TinkerPlots 101: Estimating Uncertainty via Bootstrapping",
    "section": "Margin of Error",
    "text": "Margin of Error\nWhen we use sample data to make a statistical estimate about the population, we need to incorporate the uncertainty due to sampling error into our point estimate. To do this we need to compute the margin of error, which is a measurement of the uncertainty in the point estimate. It turns out that we compute the margin of error similar to how we computed our range of likely values when we were testing hypotheses, namely:\n\\[\n\\mathrm{Margin~of~Error} = 2 \\times \\mathrm{Standard~Error}\n\\]\nThat means, to compute the margin of error we first need to compute the standard error.\n\n\nBootstrapping\nTo compute the standard error in order to ultimately compute our margin of error, we will use TinkerPlots to model the sampling variation using the observed data. That is, we will set up a TinkerPlots sampler to bootstrap the observed data and collect the percentage of teenagers who own a mobile phone. Then we can compute the standard error of the bootstrapped results in the same way we have been in previous units. Figure 1 shows a TinkerPlots sampler to bootstrap from the observed mobile phone ownership data.\n\n\n\n\n\n\n\n\nFigure 1: A TinkerPlots sampler to bootstrap from the observed mobile phone ownership data. The repeat value of 189 reflects the sample size. The sampling device reflects the responses in the observed data, namely 138 teenagers who own a mobile phone and 51 that do not own a mobile phone. To bootstrap, the sampling device is also set to sample with replacement, which models the sampling variation in the summary measure.\n\n\n\n\n\n\nIMPORTANT\nBecause we do not have different groups of teenagers that we are comparing, we do not have a second sampling device to indicate groups like we did when we were comparing groups.\n\nAfter running the first bootstrap trial, we can plot the simulated responses, and compute the percentage of teenagers who own a mobile phone. We can then collect an additional 499 percentages from this simulation. Figure 2 shows the distribution of the 500 bootstrapped percentages along with the computation of the standard error of these percentages.\n\n\n\n\n\n\n\n\nFigure 2: LEFT: The distribution of the 500 bootstrapped percentages from the simulation. This plot should be centered around the point estimate from the observed data. RIGHT: The case table of the 500 collected percentages and the computed standard error.\n\n\n\n\n\nBecause we are bootstrapping from the observed data (and not from a null model), the expected value is the point estimate from the observed data, namely 73%. (This is a check-in for you to be sure that the simulation was correctly set up; if the distribution of collected summary measures is not centered around the point estimate from the observed data—you did not set up the sampler correctly!)\nThe more important characteristic is to compute the standard error of the 500 bootstrapped percentages. We compute this the same as we always have, using the formula editor and the stdDev() function. Here the standard error is 3.37. This is a quantification of the sampling variation in our estimate. We can use this value to compute the margin of error:\n\\[\n\\begin{split}\n\\mathrm{Margin~of~Error} &= 2 \\times \\mathrm{Standard~Error} \\\\\n&= 2 \\times 3.37 \\\\\n&= 6.74\n\\end{split}\n\\]\nThe margin of error for our point estimate is 6.74%. Note that the standard error and margin of error are two different values. The margin of error is twice that of the standard error. Both the standard error and the margin of error measure the uncertainty due to sampling variation. But, similar to why we used twice the standard error in our computation of the range of likely values when we were hypothesis testing, we will always use twice the standard error (the margin of error) when we compute compatibility intervals. (Remember, this is based on the Empirical Rule!)\n\nLEARN MORE\nYou can learn more about the margin of error in What is a Margin of Error?, a chapter included in a short pamphlet put together by the American Statistical Association’s Section on Survey Research.",
    "crumbs": [
      "Estimating Uncertainty",
      "TinkerPlots 101: Estimating Uncertainty via Bootstrapping"
    ]
  },
  {
    "objectID": "05-02-tinkerplots-101-bootstrap-uncertainty.html#computing-the-compatibility-interval",
    "href": "05-02-tinkerplots-101-bootstrap-uncertainty.html#computing-the-compatibility-interval",
    "title": "TinkerPlots 101: Estimating Uncertainty via Bootstrapping",
    "section": "Computing the Compatibility Interval",
    "text": "Computing the Compatibility Interval\nOnce we have our point estimate (73%) and the margin of error (6.74%), we can put those together to compute the compatibility interval:\n\\[\n\\begin{split}\n\\text{Compatibility Interval} &= \\text{Point Estimate} \\pm \\text{Margin of Error}\\\\\n&= 73\\% \\pm 6.74\\% \\\\\n&= \\big[66.26\\%, ~~79.74\\%\\big]\n\\end{split}\n\\]\nRecall that a compatibility interval acknowledges the uncertainty in the estimate by giving a range of potential values for the estimate of the parameter. Interpreting our compatibility interval:\n\nWe believe that the percentage of Australian teenages between the ages of 12 and 14 who own a mobile phone is somewhere between 66.26% and 79.74%.\n\nRemember that our margin of error is based on two standard errors, and the Empirical Rule stated that because the distribution of bootstrapped percentages is roughly normal that roughly 95% of the bootstrapped percentages are within two standard errors of the point estimate (the expected value). Because of this, some statisticians embed this idea into their interpretation:\n\nWe are 95% confident that the percentage of Australian teenages between the ages of 12 and 14 who own a mobile phone is somewhere between 66.26% and 79.74%.\n\nThe “95% confident” part of the interpretation simply reflects the idea that we are measuring the amopunt of uncertainty by subtracting and adding two standard errors to our point estimate.",
    "crumbs": [
      "Estimating Uncertainty",
      "TinkerPlots 101: Estimating Uncertainty via Bootstrapping"
    ]
  },
  {
    "objectID": "05-03-unbiasedness-and-precision.html",
    "href": "05-03-unbiasedness-and-precision.html",
    "title": "Statistical Unbiasedness and Statistical Precision",
    "section": "",
    "text": "Statistical Unbiasedness\nAs you have seen in the readings and activities, the compatibility interval provides an estimate for the unknown population parameter by incorporating the point estimate and the uncertainty in that estimate due to sampling variation. There are two qualities of any compatibility interval that we need to take into consideration when we are interpreting these estimates: statistical unbiasedness and precision.\nStatistical unbiasedness is the idea that the expected value of the distribution of point estimates is the actual parameter value. To help you think about this idea, imagine a person, Arthur Dent, has lost his keys. The actual location of the keys, the Library, is akin to the population parameter. It is unknown to Arthur, but does have an actual value. To continue our metaphor, the locations where Arthur searches for his keys are like sample statistics. They are point estimates from different samples of data he has collected in his mind about where he may have lost the keys. Lastly, the area of the search defined by Arthur’s search location is akin to a compatibility interval.\nFigure 1: This figure is a metaphor for statistically unbiasedness estimates. Each ‘X’ represents a search location. The yellow area defined by the search locations represents the compatibility interval. The keys (the actual value of the parameter) are located at the average of all the search locations.\nIn our metaphor, unbiasedness would indicate that if we average across all the locations where Arthur searched, that average location (the expected value) would be the actual location of the keys (the unknown parameter). Another way of thinking about this, is that when we have an unbiased estimate, the compatibility interval includes the actual value of the unknown parameter. In other words, our compatibility interval has correctly “identified” the parameter value. Contrast this with an estimate that is statistically biased\nFigure 2: This figure is a metaphor for statistical biased estimates. Each ‘X’ represents a search location. The yellow area defined by the search locations represents the compatibility interval. The keys (the actual value of the parameter) are NOT located at the average of all the search locations.\nWhen the point estimates are statistically biased, Arthur’s search locations (sample statistics) are systematically in the wrong place. On average, where Arthur searched (the middle of the yellow circle) is not the actual location of the keys. Subsequently, the compatibility interval would not correctly identify the value of the parameter.",
    "crumbs": [
      "Estimating Uncertainty",
      "Statistical Unbiasedness and Statistical Precision"
    ]
  },
  {
    "objectID": "05-03-unbiasedness-and-precision.html#statistical-unbiasedness",
    "href": "05-03-unbiasedness-and-precision.html#statistical-unbiasedness",
    "title": "Statistical Unbiasedness and Statistical Precision",
    "section": "",
    "text": "Random Sampling: The Key to Statistically Unbiased Estimates\nThe key to whether or not the estimates are unbiased is whether the observed data were sampled randomly. (It is exactly the same question we asked about generalization.) Recall in the Gettysburg Address activity that when we used a non-random sampling method, the distribution of average word lengths was not at the true average word length of all the words in the Gettysburg Address. That is the expected value (average) of the different averages (point estimates) was not the same as the population parameter—the estimates were statistically biased.\nIn contrast, when we used a sampling method in which we randomly sampled words, the average of all the different averages was centered at the population mean—the samples produced unbiased estimates. Using random sampling produces unbiased estimates, which subsequently produce compatibility intervals that are much more likely to include the parameter value.\nWhy do we say “much more likely”? Shouldn’t a compatibility interval based on an unbiased estimate always include the value of the population parameter? Actually…no. In practice, we are centering our compatibility interval around the point estimate that comes from our observed data. In our key metaphor, this is akin to search on only one location (one “X”) and then building the search area (the yellow circle representing the compatibility interval) around that “X”. (We don’t have many X’s that we can average across!)\n\n\n\n\n\n\n\n\nFigure 3: In this figure the search location (‘X’) is an unbiased estimate because it was picked randomly, but the resulting compatibility interval (yellow area) based on that location did NOT include the keys (the actual value of the parameter).\n\n\n\n\n\nThis leads us to two important ideas about statistically unbiased estimates and their resulting compatibility intervals:\n\nWe build compatibility intervals off of a single point estimate. And, even when the data for that point estimate are selected randomly and the point estimate is an unbiased estimate, the resulting compatibility interval may not capture the value of the population parameter. What we know from theory is that 95% of all possible point estimates will produce a compatibility interval that includes the actual value of the population parameter. But this means that 5% of the possible point estimates will produce a compatibility interval that does not include the actual value of the population parameter.\nWe have no idea whether the our compatibility interval is one of the 95% “good” intervals or one of the 5% “bad” intervals. Our inferences are based on playing the odds that so long as we sampled our data randomly, the resulting interval is far more likely to be one of the “good” intervals that includes the actual value of the population parameter.",
    "crumbs": [
      "Estimating Uncertainty",
      "Statistical Unbiasedness and Statistical Precision"
    ]
  },
  {
    "objectID": "05-03-unbiasedness-and-precision.html#precision-of-the-estimate",
    "href": "05-03-unbiasedness-and-precision.html#precision-of-the-estimate",
    "title": "Statistical Unbiasedness and Statistical Precision",
    "section": "Precision of the Estimate",
    "text": "Precision of the Estimate\nThe second quality of a compatibility interval that we need to consider is statistical precision. Statistical precision indicates how precisely we can estimate the value of the unknown population parameter. Figure 4 illustrates this idea.\n\n\n\n\n\n\n\n\nFigure 4: LEFT: The search location (‘X’) produces a fairly precise search area (compatibility interval; yellow area) for the keys (parameter value). RIGHT: The search location (‘X’) produces a much less precise search area (compatibility interval; yellow area) for the keys (parameter value).\n\n\n\n\n\nAlthough the point estimate (the ‘X’) is in exactly the same location in both situations, the situation depicted on the left has a much smaller search area than the situation on the right. Another way of thinking about this is that the situation on the left depicts much less uncertainty about where we believe the keys are located (more precision about their location) than the situation on the right that depicts a good deal of uncertianty in the key’s location (less precision about their location). Because we measure uncertainty using the margin of error, this implies that statistical precision is a function of the margin of error. In general, smaller margin of errors produce more precise estimates for the parameter than larger margin of errors.\n\nFYI\nRemember from the Cuddling Preferences activity, you learned that the size of the margin of error is a function of the sample size. Thus, statistical precision is impacted by the sample size.\n\nIt is important to note that the precision of the estimate is completely unrelated to the unbiasedness/biasedness of the estimate. It is possible to have a statistically unbiased estimate that is not very precise or a statistically biased estimate that is incredibly precise. To illustrate, consider this example that came from the 1936 presidential election:\n\nThe Literary Digest, a highly regarded magazine at the time, had correctly predicted the outcomes of the 1916, 1920, 1924, 1928, and 1932 elections by conducting polls of their subscribers. (They included a post card in the magazine that subscribers mailed back to them.) In their October 31, 1936 issue, based on more than 2,000,000 returned post cards, the Literary Digest predicted that Republican presidential candidate Alfred Landon would win 57% of the popular vote (margin of error = 0.0657%) and 370 of the 531 electoral votes. After the official tally of the vote, Franklin D. Roosevelt (Landon’s oppnent) won 60.8% of the popular vote and and 523 of the electoral votes!\n\nEven though the poll had very high precision (their margin of error was .06 of a percentage point), this was a huge miss. In a post-mortem of their methods the problem was that their sampling method was statistically biased. It turned out that the vast majority of subscribers to the Literary Digest, and subsequently their poll responders, were Republicans. They has systematically under-polled the Democrats. High precision means very little unless the data used to create the estimate was sampled in an unbiased manner (i.e., randomly).",
    "crumbs": [
      "Estimating Uncertainty",
      "Statistical Unbiasedness and Statistical Precision"
    ]
  },
  {
    "objectID": "05-04-evaluating-hypotheses-with-compatibility-intervals.html",
    "href": "05-04-evaluating-hypotheses-with-compatibility-intervals.html",
    "title": "Evaluating Hypotheses with Compatibility Intervals",
    "section": "",
    "text": "Using Compatibility Intervals to Compare Groups\nAs you learned in the Helper or Hinderer: Revisited activity, we can use the compatibility interval to evaluate statistical hypotheses. The key to this is that the compatibility interval gives a range of plausible values for the parameter. Any value in the compatibility interval is a reasonable value for the parameter given the observed data we have. In other words, if we were to carry out a hypothesis test where the null hypothesis was that the parameter is equal to a value in the interval, we would fail to reject that null hypothesis based on our data.\nAs an example, let’s say we want to estimate the average price (in thousands of dollars) for a houses/condo in a neighborhood adjacent to the University of Minnesota Twin Cities campus (e.g., Marcy-Holmes, Cedar-Riverside, Como). On May 19, 2025, there were 40 houses/condos available for sale on Zillow.com in these neighborhoods. These data were entered into TinkerPlots and a bootstrap analysis was undertaken to estimate the standard error of the average price. Here are the resulting values:\nThe compatibility interval for the average house price is: \\(\\big[285.61, ~~385.97\\big]\\). We can interpret this as:\nWhat this implies is that:\nIn fact, any value in the interval would be a plausible value for the population average price of a house in a neighborhood near the University of Minnesota. When we evaluate a statistical null hypothesis, say:\n\\[\nH_0: \\mu =  \\$400k\n\\] we are really asking the question is the value expressed in the null hypothesis a plausible value given the data. In this case we are asking whether $400k is a plausible value for the population average given the data. When we reject the null hypothesis, that is indicating that the value is not plausible given the data. When we fail to reject the null hypothesis, it is indicating that the value is plausible given the data.\nIn the null hypothesis above, we would reject the null hypothesis since $400k is not a plausible value as evidenced in the compatibility interval. In other words, a population average house price of $400k is not compatible with the data we observed. This is why we call the interval a compatibility interval.\nNow say we want to estimate the average price (in thousands of dollars) for a houses/condo in a neighborhood adjacent to the University of Minnesota Duluth campus (e.g., Chester Park, Congdon Park, Hunter’s Park, Kenwood). On May 19, 2025, there were 25 houses/condos available for sale on Zillow.com in these neighborhoods. These data were entered into TinkerPlots and a bootstrap analysis was undertaken to estimate the standard error of the average price. Here are the resulting values:\nThe compatibility interval for the average house price is: \\(\\big[334.56, ~~658.00\\big]\\). We can interpret this as:\nIs the average house/condo price more expensive near the Duluth campus? Or near the Twin Cities campus? Based on the point estimates, the average house/condo price near the Duluth campus ($496.28k) is more expensive than that near the Twin Cities campus ($335.79). But what about after we account for uncertainty?\nWhen you compare compatibility intervals, it is often easiest to plot the compatibility intervals. Figure 1 shows an example of this for the compatibility intervals for Duluth and the Twin Cities.\nFigure 1: 95% compatibility intervals for the average price of a house/condo near the University of Minnesota Duluth and Twin Cities campuses. The dot indicates the point estimate.\nAs you interpret the compatibility intervals, remember that any value in the compatibility interval is a plausible value for the average. Because of this, if the intervals overlap, that suggests the parameter may be the same for the two groups. Based on our two intervals, it may be that:\nBecause of the overlap in the intervals, it is unclear which of these is the case. There is too much uncertainty to make a decision about which location has the higher average cost. In terms of our null hypothesis:\n\\[\nH_0: \\mu_{\\text{Duluth}} = \\mu_{\\text{Twin Cities}}\n\\]\nthe overlap would cause us to fail to reject the null hypothesis that they are the same. It may be that they are the same.\nIf the intervals did not overlap, then you could make a clear distinction about which location has a higher average price for a house/condo near the campus. For example, Figure 2 compares the average price of a house/condo near the University of Minnesota Duluth and Crookston campuses. (The Crookston CI = \\(\\big[222.72, ~~306.12\\big]\\).)\nFigure 2: 95% compatibility intervals for the average price of a house/condo near the University of Minnesota Duluth and Crookston campuses. The dot indicates the point estimate.\nEvery plausible value in the compatibility interval for the average price of a house/condo near the Duluth campus is higher than every plausible value in the compatibility interval for the average price of a house/condo near the Crookston campus. This implies that the average price of a house/condo near the Duluth campus is higher than for one near the Crookston campus. If we were testing the null hypothesis:\n\\[\nH_0: \\mu_{\\text{Duluth}} = \\mu_{\\text{Crookston}}\n\\]\nWe would reject the null hypothesis. The two averages being equal is not plausible given the compatibility intevrals. Although we can say that the average price is higher for Duluth than Crookston, it is a bit more unclear about how much higher a house/condo in Duluth is than Crookston since there is uncertainty in both averages.\nBased on the CIs, the average price of a house/condo in Duluth may be anywhere from $190.16k to $393.58k higher than the average price of a house/condo in Crookston.",
    "crumbs": [
      "Estimating Uncertainty",
      "Evaluating Hypotheses with Compatibility Intervals"
    ]
  },
  {
    "objectID": "05-04-evaluating-hypotheses-with-compatibility-intervals.html#using-compatibility-intervals-to-compare-groups",
    "href": "05-04-evaluating-hypotheses-with-compatibility-intervals.html#using-compatibility-intervals-to-compare-groups",
    "title": "Evaluating Hypotheses with Compatibility Intervals",
    "section": "",
    "text": "Point estimate: $496.28k\nStandard Error: $80.86k\nMargin of Error: $161.72k\n\n\n\nWe are 95% confident that the average house price for a house/condo in a neighborhood near the University of Minnesota Duluth is between $334.56k and $685.00k.\n\n\n\n\n\n\nThe average cost of a house/condo in a neighborhood near the Duluth campus is higher than the average cost of a house/condo in a neighborhood near the Twin Cities campus. For example, the average cost in Duluth may be $600k and the average price in the Twin Cities may be $300k (these are plausible values in the compatibility intervals).\nThe average cost of a house/condo in a neighborhood near the Duluth campus is the same as the average cost of a house/condo in a neighborhood near the Twin Cities campus. For example, the average cost in Duluth qnd the Twin Cities may be $350k (these are plausible values in the compatibility intervals).\nThe average cost of a house/condo in a neighborhood near the Twin Cities campus is higher than the average cost of a house/condo in a neighborhood near the Duluth campus. For example, the average cost in the Twin Cities may be $380k and the average price in Duluth may be $340k (these are plausible values in the compatibility intervals).\n\n\n\n\n\nPROTIP\nFailing to reject the null hypothesis does not imply you are saying that the two groups have an equal mean. It just means it is a possibility. As you saw with the compatibility intervals, it may be that they are the same, or that one is higher than the other…we just have too much uncertainty to make a decision.\n\n\n\n\n\n\n\nBased on the CIs, the largest plausible value for Duluth is $658k while the smallest possible value for Crookston is $264.42k. This results in a difference of $393.58k.\nAlso based on the CIs, the smallest plausible value for Duluth is $496.28k while the largest possible value for Crookston is $306.12k. This results in a difference of $190.16k.",
    "crumbs": [
      "Estimating Uncertainty",
      "Evaluating Hypotheses with Compatibility Intervals"
    ]
  }
]