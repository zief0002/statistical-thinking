[
["index.html", "Statistical Thinking: A Simulation Approach to Modeling Uncertainty Front Matter", " Statistical Thinking: A Simulation Approach to Modeling Uncertainty Front Matter This website is intended to serve as an organizational hub for the most current version of the CATALST Project’s Statistical Thinking: A Simulation Approach to Modeling Uncertainty. Here you will be able to access materials such as readings, data sets, and the lab manual. The website also includes helpful links and resources for each of the course topics. The resources that accompany this website include: A lab manual PDF copy available here Data sets used in the lab manual available here. You can read more about these resources in the introduction. Licensing and Attribution Copyright © 2019 Catalysts for Change PUBLISHED BY CATALYST PRESS This work is licensed under a Creative Commons Attribution 4.0 International License. You are free to share, remix, and make commercial use of the work under the condition that you provide proper attribution. To reference this work, use: Zieffler, A., &amp; Catalysts for Change. (2019). Statistical Thinking: A simulation approach to uncertainty (4.2th ed.). Minneapolis, MN: Catalyst Press. http://zief0002.github.io/statistical-thinking/ The work to create the material appearing in the book was made possible by the National Science Foundation (DUE–0814433). The material on this website and in the lab manual is a direct reflection of the ideas, work, and effort of several Catalysts for Change. They include (alphabetically): Ethan Brown, Jonathan Brown, Dan Butler, Beth Chance, George Cobb, Robert delMas, Katherine Edwards, Michelle Everson, Jeffrey Finholm, Chris Fiscus, Elizabeth Fry, Joan Garfield, Theresa Gieschen, Meg Goerdt, Robert Gould, Adam Gust, Melissa Hanson, John Holcomb, Michael Huberty, Rebekah R. Isaak, Kari Johnson, Nicola Justice, Laura Le, Suzanne Loch, Matthew Mullenbach, Michael Nguyen, Amy Okan, Allan Rossman, Anelise Sabbag, Andrew Zieffler, and Laura Ziegler Additionally, some of the activities presented in the lab manual were originally developed by Beth Chance, George Cobb, John Holcomb, and Allan Rossman as part of their NSF-funded project Concepts of Statistical Inference: A Randomization-Based Curriculum (NSF CCLI- DUE-0633349). Colophon There are two Google Fonts used on the website. The headings use Playfair Display and the display text is Alegreya. Icons used on the website are: Key by Iconic from the Noun Project "],
["introduction.html", "Introduction", " Introduction Learning statistics is sexy. Hal Varian, Google’s chief economist, believes this. During an interview in McKinsey Quarterly, Varian stated, “I keep saying the sexy job in the next ten years will be statisticians. People think I’m joking, but who would’ve guessed that computer engineers would’ve been the sexy job of the 1990s?” Varian is not the only person to express this sentiment either. Hans Rosling in the 2010 BBC documentary Joy of Stats1 referred to statistics as the “sexiest subject around”. Whether you believe it is the sexiest subject or not, it is incontrovertible that the use of statistics and data are prevalent in today’s information age. Almost every person on earth will benefit from learning some foundational ideas of statistics. This is true because statistics forms the basis of our everyday world just as much as do science, technology, and politics. Google, Netflix, Twitter, Facebook, OKCupid, Match.com, Amazon, iTunes, and the Federal Government are just a handful of the companies and organizations that use statistics on a daily basis. Journalism, political science, biology, sociology, psychology, graphic design, economics, sports science, and dance are all disciplines that have made use of statistical methodology. Course Material The materials on this website and in the lab manual will introduce you to the seminal ideas underlying the discipline of statistics. In addition, they have been designed with your learning in mind. For example, many of the class activities were developed using pedagogical principles, such as small group activities and discussion, that have been shown in research to improve student learning. Course readings should be completed outside of class and are intended to help you learn and extend the ideas, skills, and concepts you learn in the classroom. The readings themselves are not all “traditional” readings in the sense of words written on the screen, but instead often link to video clips, blogs and other multimedia material. TinkerPlots™ Software Much of the material presented in the lab manual requires the use of TinkerPlots™. This software can be downloaded (for Mac or PC), and a license can be purchased from http://www.tinkerplots.com/. Lab Manual and Data Sets You will work from the lab manual every day in class. As such, you will need to bring a copy of the lab manual (physical or electronic) with you to class every day. To download a PDF copy of the lab manual, click this link: https://github.com/zief0002/statistical-thinking/blob/master/statistical-thinking-v4.pdf?raw=true. There are several data sets used in the lab manual, as well as in EPsy 3264 assignments. To download a ZIP file to your computer that includes all the data sets, click this link: https://github.com/zief0002/statistical-thinking/blob/master/data.zip?raw=true. Once the ZIP file has been downloaded to your computer, double-click the ZIP file to unzip it and access the materials. Participation in the Learning Process The lab manual, instructors, and teaching assistants are all resources that are at your disposal to help you learn the material. In the end, however, you will have to do all of the hard work associated with actually learning that material. To successfully navigate this process, it is vital that you be an active participant in the learning process. Coming to class, participating in the activities and discussions, reading, completing the assignments, and asking questions are essential to successful learning. Learning anything new takes time and effort and this is especially true of learning statistics, as you are not just learning a set of methods, but rather a disciplined way of thinking about the world. Changing your habits of mind will take continual practice. It will also take a great deal of patience and persistence. As you engage in and use the skills, concepts and ideas introduced in the material, you will find yourself thinking about data and evidence in a different way. This may lead you to make different decisions or choices. But, even if this course does not change your world overnight, you will at the very least be able to critically think about inferences and conclusions drawn from data. Watch Joy of Stats online at http://www.gapminder.org/videos/the-joy-of-stats/↩ "],
["modeling-simulation.html", "Modeling &amp; Simulation", " Modeling &amp; Simulation There is mounting evidence that the “model-building era”\" that dominated the theoretical activities of the sciences for a long time is about to be succeeded or at least lastingly supplemented by the “simulation era”.2 Modeling is one of the most important topics you may ever learn. It is used in microbiology, macroeconomics, urban studies, sociology, psychology, public health, computer science, and of course, statistics. In fact, modeling is a method that is used in almost every discipline. Many think that it is an important skill to learn because it is so pervasive. While this is true, even more important is how closely the skills of modeling tie to the more general skills of problem solving. Starfield, Smith, and Bleloch (1994) summed this sentiment up nicely when they wrote, “learning to model is bound up with learning to solve problems and to think imaginatively and purposefully” (p. x).3 A model is a simplified representation of a system that can be used to promote an understanding of a more complex system. For example, meteorologists use computers to build models of the climate to understand and predict the weather. The computer model includes behaviors or properties which correspond, in some way, to the particular real-world system of climate. The computer models, however, do not include every possible detail about climate. All models leave things out and get some things—many things—wrong. This is because all models are simplifications of reality. Since all models are simplifications of reality there is always a trade-off as to what level of detail is included in the model. If too little detail is included in the model one runs the risk of missing relevant interactions and the resultant model does not promote understanding. If too much detail is included in the model, the model may become overly complicated and actually preclude the development of understanding. Models have many purposes, but are primarily used to better understand phenomena in the real-world. Common uses of models are for description, exploration, prediction, and classification. For example, Google builds models to understand and predict peoples’ internet searching tendencies. These models are then used to help Google carry out more efficient and better searches of information. As another example, Netflix builds models to understand the characteristics of movies that their customers have rated highly so that they can then recommend other movies that the person may enjoy. Amazon and Apple iTunes both use models in similar manners. Outline and Goals of Unit 1 The following schematic outlines the course readings, in-class activities, and assignments for Unit 1. In this unit, you will begin by exploring ideas of randomness. Randomness permeates, and is, in fact, fundamental to statistics. Then, you will learn how to use TinkerPlots™ to model several random processes and generate outcomes from those models. By generating data from different models, you will gain experience in considering the variation in outcomes that is produced by these random processes. This consideration will help you understand and overcome many misleading human intuitions about randomness. You will also be introduced to the Monte Carlo simulation process and learn how to carry out a Monte Carlo simulation using TinkerPlots™. This process allows you to quickly generate multiple data sets from a model in order to carry out hypothetical experiments. For example, we could ask the question: How likely is it to rain three out of the five days on my vacation given a particular forecast? By modeling the forecast and repeatedly generating data for the five days of vacation, we can then answer this question. As you progress through the unit, remember that the modeling process is a creative process that can often be very challenging. At times, this might lead to frustration as you are learning and practicing some of the material. But, as Mosteller et al. (1973) remind us, it is also a profitable experience since, “modeling is not only a technique of statistics…it is a method of study which can be applied in many other fields as well” (p. xii).4 Randomness One critical component of simulation is the random process used to generate data. To help you begin to understand randomness, watch the Random Sequences: Human vs. Coin YouTube video. Hartmann, S. (2005). The world as a process: Simulations in the natural and social sciences. http://philsci-archive.pitt.edu/2412/↩ Starfield, A. M., Smith, K. A., &amp; Bleloch, A. L. (1994). How to model it: Problem solving for the computer age. Edina, MN: Burgess International Group, Inc.↩ Mosteller, F., Kruskal, W. H., Link, R. F., Pieters, R. S., &amp; Rising, G. R. (1973). Statistics by example: Finding models. Reading, MA: Addison–Wesley.↩ "],
["generating-data-from-models.html", "Generating Data from Models", " Generating Data from Models One core skill of a practicing statistician is to be able to generate random data from a model. Most of the models you will encounter in this course are referred to as probability models. That is just a fancy way of associating probabilities with different events, or outcomes, in a model. For example, the model of flipping a “fair” coin is a probability model. There are two events/outcomes in the model: heads and tails. Each of these outcomes has a probability of 0.5 associated with it. (Note that although we could say 50%, that probabilities are on the scale from 0 to 1, so are defined using decimal values.) In the in-class activity, Generating Random Data—Cat Factory, you will create several probability models to generate data about cats. To prepare for this activity, watch the Probability Simulation TinkerPlots™ tutorial video. "],
["monte-carlo-simulation.html", "Monte Carlo Simulation", " Monte Carlo Simulation Monte Carlo simulation is one method that statisticians use to understand real-world phenomena. In Monte Carlo simulation, a model is used to generate multiple (sometimes millions) of data sets. By examining the data sets produced (or summaries of the data sets produced), researchers can draw insight about and predict what might happen in the real-world under a given set of circumstances. You can read about the fascinating origins of Monte Carlo simulation in the following article: The Beginning of the Monte Carlo Method Example of a Monte Carlo Simulation Study In 1978, China introduced the “one-child” policy in order to alleviate social, economic, and environmental problems in China. According to Wikipedia,5 The policy officially restricts the number of children married urban couples can have to one, although it allows exemptions for several cases, including rural couples, ethnic minorities, and parents without any siblings themselves. A spokesperson of the Committee on the One-Child Policy has said that approximately 35.9% of China’s population is currently subject to the one-child restriction. Although the Chinese government has suggested that the policy has prevented more than 250 million births from its implementation to 2000, the policy is controversial both within and outside of China because of the manner in which the policy has been implemented. There have also been concerns raised about potential negative economic and social consequences, in part because many families were determined to have a son. Scholars have wondered how things would change if instead of a one-child policy, a country adopted a “one son” policy. A “one son” policy would allow families to keep having children until they had a son. If a family’s first child is a boy, they would be restricted from having more children. If, however, the first child was a daughter, the family could continue having children until a son was born. For example, they might ask the question, If China adopted a “one son” policy, how would the policy affect the average number of children per family, which is currently 1.66? One way in which this question could be studied (without actually implementing the policy) would be to conduct a simulation study by modeling this situation and generating many data sets from the model. Consider for a minute how you might model the number of children a particular family would have. One way to model this is to write the word boy on one index card and the word girl on another index card and to place those two index cards in a hat. After mixing up the index cards, you could draw a single card from the hat. If the card has the word boy written on it, the simulated “family” would be reported to have one child. If the card has the word girl written on it, a tally mark could be recorded and the index card would be replaced in the hat. The cards could then be remixed and another card would be drawn. If the second card drawn has the word boy written on it, the simulated “family” would be reported to have two children. If the second card has the word girl written on it, another tally mark could be recorded and the index card would again be replaced in the hat. This process would continue until the boy card was drawn. The table below shows the results after carrying out this process for three simulated families. Table 1: The recorded number of girls and boys for three simulated families. Family Girl Boy Family #1 ✔ ✔ Family #2 ✔ Family #3 ✔✔ ✔ We could carry out this simulation for many families, say 500 families, and use the results to provide an answer to the research question. You can imagine that carrying out even this simple simulation would quickly become quite tedious. Simulation studies, such as this, are typically carried out using computer programs. In this unit, you will learn to use a computer program called TinkerPlots™ to model processes in the real-world and carry out simulation studies. Monte Carlo Simulation Assumptions “Wait,” you say. “Even if I carried out this simulation, I still would not be able to provide an answer to the research question! It doesn’t reflect reality! Some families may not want to have any children, while others might be happy to stop after a girl was born. What about multiple births?” Maybe you are even questioning whether the probability of having a boy or having a girl is really 50:50. These are all valid points, and all would likely affect the results of the simulation, which in turn affects the inferences and conclusions that are drawn. While the model used in the “one son” example is overly simplistic for drawing any sorts of meaningful conclusions about implementing such a policy in China, it could, however, provide a useful starting point for introducing additional complexity. Even in the most enormously complicated modeling problem, researchers often make many simplifying assumptions. (Remember that all models—even those that seem quite complex—are simplifications of reality and get many things wrong.) With enough simplification, a model can be constructed and studied. The model is evaluated and often revised or updated as certain assumptions are deemed tenable and others are not. Because of this process, simulation studies are generally iterative in their development. This iteration process continues until an adequate level of understanding is developed and the research question can be answered. Monte Carlo Simulation in Practice In practice, statisticians often use incredibly complex models to generate their data. As an example, Electronic Arts, the video game company behind titles such as Madden, NHL and FIFA, uses game telemetry (the transmission of data from a game executable for recording and analysis) to model the gameplay patterns of players and identify the elements of their games that are highly correlated with player retention.7 By understanding the behavior of players and the common patterns that are used, Electronic Arts game developers can focus their attention on more relevant features in future iterations of the game and ultimately reduce production costs. For example, in their examination of Madden NFL 11, Electronic Arts used 46 features to model players’ preferences, including their control usage, performance, and play-calling style. This is but one example of using simulation in video games. One-child policy. (2015, May 30). In Wikipedia, The Free Encyclopedia. Retrieved 18:02, June 1, 2015, from http://en.wikipedia.org/w/index.php?title=One-child_policy&amp;oldid=664745432↩ World Factbook↩ Weber, B. G., John, M., Mateas, M., &amp; Jhala, A. (2011). Modeling player retention in Madden NFL 11. Presented at Innovative Applications of Artificial Intelligence. http://users.soe.ucsc.edu/~bweber/pubs/madden11retention.pdf↩ "],
["modeling-sampling-variation.html", "Modeling Sampling Variation", " Modeling Sampling Variation In the course activities and homework assignments, you have been using probability models to generate random outcomes. You have also learned how to use Monte Carlo simulation to generate many data sets from a given model. This is the same kind of process that researchers, scientists, and statisticians engage in when they evaluate (or test) hypotheses about the world. To illustrate the ideas behind statistical hypothesis testing, consider how you might go about testing a coin for “fairness”. You might have suggested something along the lines of “flip the coin many times and keep track of the number of heads and tails”. Suppose you tossed the coin 100 times, which resulted in 53 heads and 47 tails. Would you say the coin is “unfair”? What if you had obtained 65 heads and 35 tails instead? Now would you say the coin is “unfair”? How about if you had gotten 84 heads and only 16 tails? The first result of 53 heads and 47 tails probably did not seem that far fetched to you, and you probably would feel at ease saying that the coin that produced such a result is most likely “fair”. On the other hand, the results of 65 heads and 35 tails—and especially 84 heads and 16 tails—likely made you feel uncomfortable about declaring the coin “fair”. Why is this? It is because you had a mental model of the distribution of heads and tails that you expect when the coin actually IS “fair”. For most people, this mental model encompasses a uniform distribution of the outcomes (e.g., a 50:50 split between heads and tails). If the observed result of the 100 coin flips is compatible with the model of a “fair” coin, you might conclude that the coin is “fair”. For example, the result of 53 heads from 100 flips is very close to the 50:50 split of heads and tails, and it is probably safe to say that a “fair” coin could have produced the set of flips in question. In this case, the data are compatible with the model of “fairness”. If the observed result deviates from what is expected under the model of a “fair” coin, for example the two results of 65 heads and 84 heads, you might end up rejecting the hypothesis that the coin was “fair”. In these two cases, the data are incompatible (or at least far less compatible) with the model of “fairness”. One thing you may have realized is that we expect variation in the results just because of chance (randomness). Even if the coin really was “fair”, we do not expect exactly 50 heads every time we carry out 100 flips of the coin. This variation in the number of heads we get each time we carry out 100 flips of the coin is referred to as sampling variation; it is the variation that arises because we are generating different samples from the model (or population). Knowing something about how much sampling variation is expected is how we can judge whether data are compatible or incompatible with the model; it is why you feel 53 heads would be compatible with a “fair” coin, but 65 heads feels less compatible with that model, and 84 heads even less so. Luckily, we can get an indication of how much sampling variation is expected using Monte Carlo simulation. Simulation Process for Evaluating Hypotheses The process we will use for evaluating a hypothesis is: Create a model that conforms to the hypothesis to be evaluated. Use the selected model to generate many, many sets of data (Monte Carlo simulation). The results you collect and pool together from these trials will give a picture of the variation you would expect under the hypothesized model. Evaluate whether the results observed in the actual data (not the simulated data) are compatible with the expected results produced from the model. This acts as evidence of support (or nonsupport) for the hypothesis. To help you remember this process, you can use the more simplistic mnemonic: Model Simulate Evaluate This may sound like a straight-forward process, but in practice it can actually be quite complex—especially as you are reading research articles and trying to interpret the findings. First off, the model that is selected is often not provided, nor described, explicitly within most research articles. It is often left to the reader to figure out what the assumed model was. At first, this may be quite difficult, but like most tasks, as you gain experience in this course and as you read more research, you find that there are a common set of models that are typically used by researchers. The model that you use in the Monte Carlo simulation is directly related to the hypothesis you make about a research question. Often researchers explicitly state hypotheses about their research questions. Hypotheses are simply statements of possible explanations for an observed set of data. For example, one possible explanation for the observed set of coin flips is: The coin used to generate the set of observed coin flips is a “fair” coin; which would produce (in the long run) a uniform distribution of heads and tails. One complication that you may encounter is that many statisticians and researchers write their hypotheses mathematically. The advantage to writing a hypothesis mathematically is that it explicitly defines the model that will be used in the Monte Carlo simulation. Consider the stated hypothesis that the coin used to generate the set of observed coin flips is a “fair” coin that produces a uniform distribution of heads and tails. Recall that producing a uniform distribution of heads and tails means that heads and tails are equally likely under this model (i.e., a 50:50 split). We could express this hypothesis more mathematically as: The model produces heads (and tails) with a probability of 0.5. Symbolically, we would express this hypothesis as: \\[ H_0: \\pi_{\\mathrm{Heads,~Tails}} = 0.5 \\] The symbol \\(H_0\\) is common and indicates a hypothesis about a model. Here, \\(\\pi\\) is the Greek letter pi and means “probability” or “proportion”. (Typically in symbolic notation for hypotheses, pi is not the mathematical constant of 3.14.) Statisticians typically use Greek letters to identify probabilities of the outcomes in a model.8 In this hypothesis, we are establishing that the model we are evaluating generates heads (and tails) with a probability of 0.5. Notice how the model is completely defined using the mathematical notation. The hypothesis states that the model has two potential outcomes (heads and tails), and the probability of each is 0.5. Pretty cool, huh? If this all seems like gibberish to you right now, do not worry about it. You can always write hypotheses descriptively, without resorting to the symbolic notation. Remember, we wrote the EXACT SAME hypothesis three different ways. If you are comfortable with the mathematical symbols, feel free to use it; the mathematical notation acts as a shorthand to quickly state a hypothesis and define the model used. As you read research articles or take other courses, you will see statistical hypotheses stated in many ways, so it is good to understand that there are many ways to express the same thing. Outline and Goals of Unit 2 The following schematic outlines the course readings, in-class activities, and assignments for Unit 2. In the readings, course activities, and assignments in Unit 2, you will explore the process of evaluating statistical hypotheses. You will be introduced to several common models that used by researchers and statisticians. You will also use TinkerPlots™ to generate simulated data to study the variation in results that would be expected under these models. Many of these models are directly related to the chance models that you have explored in the course to this point. For example, you should already be able to use TinkerPlots™ to produce results that would be expected from 100 flips of a “fair” coin. Aside from learning about some of the more common models used in research, you will also learn how to describe and formally quantify the variation in a distribution. This is helpful as we evaluate whether a particular result in observed data is compatible with results produced from the given model. Lastly, you will learn about common misconceptions regarding model evaluation (e.g., we can never say a model produced the data, only that it produces results compatible with the data), and how to use probabilistic language when providing an “answer” to a research question. Greek vs. Roman Letters Greek letters are used when the parameters of a model are being described. In contrast, Roman letters are used to describe observed results. For example, go back to the situation in which the observed data consisted of 53 heads and 47 tails from 100 flips of a coin. Here we would say \\(p_{\\mathrm{Heads}} = 0.53\\). The hypothesis about the model we are evaluating produces heads with a probability of 0.5, so \\(\\pi_{\\mathrm{Heads,~Tails}} = 0.5\\). Rather than use the Roman letters, some statisticians prefer to put a “hat” on the Greek letter to refer to the observed result. For example, \\(\\hat{\\pi}_{\\mathrm{Heads}}=0.53\\). In this course we are not as concerned about which notation you use to express the result observed in the actual data. In fact, it might be less confusing if you just write, the observed result is 0.53.↩ "],
["describing-distributions.html", "Describing Distributions", " Describing Distributions One of the important steps in any statistical analysis is that of summarizing data. It is good practice to examine both a graphical and a numerical summarization of your data. These summaries are often part of the evidence that researchers use to support any conclusions drawn from the data. They also allow researchers to discover structure that might have otherwise been overlooked in the raw data that was actually collected. Lastly, both graphical and numerical summaries of the data often point to other analyses that may be undertaken with the data. Once raw data has been collected in a study, it can be overwhelming to pull any kind of meaning out of it. For example, it is not uncommon for Google to be dealing with millions of cases. How can Google—or any researcher for that matter—go from all of that raw data to something that can help them answer their research questions? Rather than examining all of those cases individually, researchers examine the data collectively, often by plotting it. This is what is meant by a graphical summary of the data; it is quite literally, a picture of the distribution. There are many, many different types of plots that have been created to graphically summarize data. Each can provide a slightly different representation of the data. Metaphorically, you can imagine each of these different plot types as a different photo taken of the exact same person. Some may be color, others black and white. Some may be taken from different perspectives, angles or distances. While all photographs “summarize” the same person, you may notice characteristics of that person in some photos that are not evident in others. Many of the photos, however, will show the same thing. Shape The dot plot that TinkerPlots™ provides is a very useful plot.9 It allows us to summarize the shape of the distribution very easily. Shape is used to describe a distribution’s symmetry. As you might expect, symmetric distributions are shaped the same on either side of the center. (Another way of thinking about this is that if you folded the distribution at the center, the folded half of the distribution would align pretty well on top of the other half.) For example, “bell-shaped” (“approximately normal”) distributions are symmetric. When a distribution is asymmetric, it is referred to as a skewed distribution. The distribution shown in Figure 1 is a skewed distribution. In this distribution, there appears to be a longer tail on the right side of the distribution. Because the tail is on the right side of the distribution, statisticians would say it is “skewed to the right” or “positively skewed”. In a similar way, a distribution that tails to the left is “skewed to the left” or “negatively skewed”. Figure 1: This distribution is skewed to the right, or positively skewed. Location Aside from the overall shape of the distribution, it is also useful to summarize the location of the distribution. The location of the distribution provides a summarization of a so-called “typical” value for the data. A “typical” value can be estimated from the plot of the distribution. You can also use more formally calculated summaries of the location such as the mean, median, or mode. These values are easily calculated using TinkerPlots™. When looking at a plot of a distribution, data analysts often consider the number of modes or “humps” that are seen in a plot of the distribution. Here, the concept of mode is slightly different (although related) to the concept of mode that you may have learned in previous mathematics or statistics courses. The mode of a distribution gives a general sense of the values or measurements that occur frequently. This may be a single number, but many times is not. For example, the first hump of the distribution shown in the figure below suggests that values around nine are very common. The actual value of nine, however, may only show up once or twice in the data. Figure 2: A bimodal distribution showing two modes. One mode is around 9, and the other is near 12. A distribution can be unimodal (one mode), bimodal (two modes), multimodal (many modes), or uniform (no modes). The distribution shown above is bimodal—notice there are two humps. Uniform distributions have roughly the same frequency for all possible values (they look essentially flat) and thus have no modes. Variation A third characteristic of a distribution that should be summarized is the variation. Summarizing the variation gives an indication of how variable the data are. One method of numerically summarizing the variability in the data is to quantify how close the observations are relative to the “typical” value on average. Are the observations for the most part close to the “typical” value? Far away from the “typical” value? How close? It turns out, that the shape of the distribution also helps describe the variation in the data. For example, “bell-shaped” distributions have most observations close to the “typical” value, and more extreme observations show up both below and above the “typical” value (the variation is the same on both sides of the “typical” value). Whereas skewed distributions have many observations near the “typical” value, but extreme values only deviate from this value in one direction (there is more variation in the data on one side of the “typical” value than the other). Figure 3: Most of the observations in this distribution are clustered between 0 and 2. There are some observations greater than 2 (up to 10), although these are rare. One thing that affects the variation, and should be described is whether there are observations that stand out from the other observations. Often these observations have extremely large or small values relative to the other observations. These observations are referred to as potential outliers, or extreme cases. For example, in the positively skewed distribution shown previously, the observation that has a value near 10 would likely be considered a potential outlier. Putting It All Together Rotten Tomatoes is a website which aggregates movie critics’ reviews of films. The website marks each review as either positive or negative and then gives the film a score based on the percentage of positive reviews. In addition to the critics’ score, each film is also given a score based on reviews from the general public using the same methodology (reviews are tabulated so that the score represents the percentage of positive reviews from the general public). The plot shown below shows a dot plot of the distribution of the general public’s scores for 134 movies released in 2009. Figure 4: The scores for 134 movies released in 2009 based on the general public’s reviews. The scores represent the percentage of positive reviews for each movie. A written description of the distribution might read as follows: The distribution of scores for this sample of 134 movies is fairly symmetric. The median score for these movies is near 60, indicating that a typical movie released in 2009 is positively reviewed by about 60% of the public. The distribution also indicates that there is variation in the movies’ scores. Most of the movies in the sample have a score between 35 and 85, suggesting large differences in the public’s opinion of the quality of these movies. Notice that the description includes a description of the distribution’s shape, location, and variation. It also incorporates the context of the data, in this case film scores. This helps a reader to interpret the description. TinkerPlots™ also provides other types of plots, including the box plot (sometimes called the box-and-whiskers plot) and the hat plot (a variation of the box plot).↩ "],
["experimental-variation-and-the-randomization-test.html", "Experimental Variation and the Randomization Test", " Experimental Variation and the Randomization Test The nature of doing science, be it natural or social, inevitably calls for comparison. Statistical methods are at the heart of such comparison, for they not only help us gain understanding of the world around us but often define how our research is to be carried out.10 Drawing inferences about the differences between groups is an almost daily occurrence in the lives of most people. In any given hour of any given day, television, radio and social media abound with comparisons. For example, data scientists at OKCupid, an online dating site, examined whether frequent tweeters (users of Twitter) have shorter real-life relationships than others.11 Group comparisons are at the heart of many interesting questions addressed by psychologists, physicians, scientists, teachers, and engineers. Questions about group differences are often studied through scientific experiments. When considering a scientific experiment to examine group differences, the design of the study plays a very important role. To help understand this, think about a researcher who is studying the efficacy of a new cold medication. Let’s say that the researcher has 100 people (each with a cold) who volunteer to be a part of her study. Let’s consider how she might design her study. Design 1: She gives the cold medicine to all 100 volunteers. Design 2: She gives the cold medicine to the first 50 volunteers (treatment group) and nothing to the other 50 volunteers (control group). Design 3: She randomly picks 50 of the volunteers to whom she gives the cold medicine (treatment group), and she gives nothing to the other 50 volunteers (control group). All three designs have been used, and are still used, in research studies. There are pros and cons to each of the designs, and all are useful depending on what you want to know. In Design 1, it is hard to judge the efficacy of the medication. For example, what if 60 of the volunteers had no cold symptoms after four days? Did the medication work? You might be thinking, “what would have happened if they hadn’t received any medication?” That is a great question. In this design, we don’t know. Design 2 gives the researcher a comparison group. She can compare the number of volunteers in each group who have no cold symptoms after four days. This is a better design than Design 1 for examining efficacy. But, what if she found that after four days, 35 of the volunteers who got the medication had no symptoms, while only 25 of the volunteers who didn’t receive medication had no symptoms. Is this enough evidence for her to say the cold medication is effective? Probably not. Maybe most of the volunteers in the treatment group were already in later stages of their colds. Maybe they had more robust immune systems to begin with (e.g., due to differing exercise or nutrition habits) than the control group. You can imagine many such reasons that the treatment group would show quicker improvement than the control group. Design 3 has the same comparison group advantage as Design 2. The big difference, however, is that the volunteers were put into the groups at random. By assigning participants at random, the researcher “equalizes” the treatment and control groups. What this means is that the groups have, on average, the SAME nutritional habits, the SAME exercise habits, and the SAME everything-else. That means that the only thing that is different between the two groups is that the treatment group got the cold medication and the control group didn’t. If the researcher uses this type of design, she can draw much stronger inferences about WHY the treatment group improved: it was because of the cold medication! Experimental Variation Let’s say our hypothetical researcher used a strong design in which she randomly assigned her volunteers to treatment and control groups. After four days she found that the treatment group had 35 of the 50 volunteers with no symptoms, and the control group had 27 of 50 volunteers with no symptoms. Could she conclude that the cold medication is effective since 8 more volunteers had no symptoms in the treatment group? Actually no. And, the reason is because of experimental variation. Consider the situation where the treatment has absolutely NO EFFECT. In other words, it does nothing. Under that assumption, the treatment and the control groups should improve at about the same rate. Under the assumption of no treatment effect, differences between the treatment and control group are not a function of the cold medication. They are solely a function of random chance. Similar to the studies we looked at in Unit 2, we have to figure out how much chance variation is expected before we can say whether the difference of 8 volunteers is actually an improvement. One key difference between this type of study and those in Unit 2 is that the chance variation arises from the assignment to groups in these studies, whereas in Unit 2, the chance variation arose because of sampling from a larger population. When the chance variation is due to the assignment of participants to groups, it is referred to as experimental variation rather than sampling variation. Outline and Goals of Unit 3 The following schematic outlines the course readings, in-class activities, and assignments for Unit 3. In the readings, course activities, and assignments in Unit 3, you will explore the process of modeling experimental variation to be able to evaluate observed differences between groups. You will learn about the randomization test (a Monte Carlo method for evaluating whether an observed result in compatible with experimental variation from a hypothesized model) and how to carry out this test using TinkerPlots™. Liao, T. F. (2002). Statistical group comparison. New York: Wiley.↩ The website OKTrends includes an answer to this question, as well as many others.↩ "],
["quantifying-results-p-value.html", "Quantifying Results: p-Value", " Quantifying Results: p-Value In addition to computing the range of likely results from the model, statisticians also typically provide a quantification of the likelihood of the observed result given the hypothesized model. This quantification is referred to as a \\(p\\)-value (the \\(p\\) stands for probability). To compute a \\(p\\)-value, you count the number of results that are at least as extreme as the observed result, and divide this by the total number of results. \\[ p = \\frac{\\mathrm{number~of~results~at~least~as~extreme~as~observed~result}}{\\mathrm{total~number~of~simulated~results}} \\] This value is then reported as a decimal value. It quantifies the probability of observing a result at least as extreme as the observed result under the hypothesized model. To illustrate this, we will re-examine simulation results from the Sleep Deprivation study. Recall in that activity, the observed data had a difference in means of 15.9. Below is a plot of 100 differences in means simulated under the “no-effect”\" model. A vertical line is shown at the observed difference of 15.9. Since 15.9 is to the right of 0 (i.e., it is on the right-hand side of the plot) results that are more extreme than the observed result are to the right of 15.9. (If the observed result was to the left of 0, more extreme results would be those more negative than the observed result.) Here there are two simulated results out of 100 that are at least as extreme as 15.9 (\\(\\geq 15.9\\)). We would report the \\(p\\)-value as 0.02. Adjustment for Simulation Results In simulation studies, we make one small adjustment to the \\(p\\)-value computation; we add 1 to both the numerator and denominator: \\[ p = \\frac{\\mathrm{number~of~results~more~extreme~than~observed} + 1}{\\mathrm{total~number~of~simulated~results} + 1} \\] This adjustment assures that we never get a \\(p\\)-value of 0. Consider the \\(p\\)-value if our observed result would have been 18 (instead of 15.9). There are 0 results that are at least as extreme as 18 (\\(\\geq 18\\)). Without making the simulation adjustment, we would report a \\(p\\)-value of 0. This implies that seeing a result at least as extreme as 18 under the “no-effect” model is impossible. The problem is that we only ran 100 trials of the simulation. If we had run this simulation for all possible randomizations of the data, we would have seen results \\(\\geq 18\\). So, to report a \\(p\\)-value of 0 is misleading. The \\(p\\)-value we report should be, \\[ p = \\frac{0 + 1}{100 + 1} = 0.0099 \\] After the adjustment, the \\(p\\)-value is still quite small, indicating that had we seen an observed result of 18, we would say that it is inconsistent with the model of “no-effect”. In fact it is in the outer 0.01 (1%) of the results simulated from the hypothesized model. Going back to the \\(p\\)-value computed from the observed value of 15.9, \\[ p = \\frac{2 + 1}{100 + 1} = 0.0297 \\] We can interpret the \\(p\\)-value of 0.030 as indicating that the observed difference of 15.9 is in the outer 0.03 (3%) of results simulated from the hypothesized model. It is quite unlikely that we would see a result as extreme as 15.9, or more extreme, under the hypothesized model of “no effect”. p-Values as Evidence Large \\(p\\)-values indicate that the observed data are more compatible with the results from the model, while small \\(p\\)-values indicate that the observed data are not very compatible with the results from the model. As researchers, our goal is often to then translate this quantitative evidence into support for the hypothesized model. For example, in the Sleep Deprivation study, we obtained a \\(p\\)-value of 0.03. This suggests a low degree of compatibility between the observed data (our empirical evidence) and the hypothesized model of “no effect”. The broader scientific question about whether sleep deprivation has a harmful effect on learning, however is difficult to ascertain from a \\(p\\)-value. There are no hard-and-fast rules for gauging how strong the evidence is against the hypothesized model because what counts as evidence in one scientific discipline or context may not count in another scientific discipline or context. And, even if there were rules about the degree of evidence needed, you would still need to evaluate other criteria such as internal and external validity evidence, and sample size. Even then, the results from a single study are not typically convincing enough for most scientists to draw definitive conclusions. It is only through consistent findings that emerge from multiple studies (which may take decades) that we may have convincing evidence about the answer to the broader scientific question. Six Principles about p-Values Because they are so ubiquitous in the research literature for any field, and because they are often mis-interpreted (even by PhDs, researchers, and math teachers) it is important to be aware of what a \\(p\\)-value tells you, and more importantly, what it doesn’t tell you. To this end, the American Statistical Association released a statement on \\(p\\)-values in which it listed six principles:12 Principle 1: \\(P\\)-values can indicate how incompatible the data are with a specified statistical model. Principle 2: \\(P\\)-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. Principle 3: Scientific conclusions and business or policy decisions should not be based only on whether a \\(p\\)-value passes a specific threshold. Principle 4: Proper inference requires full reporting and transparency. Principle 5: A \\(p\\)-value does not measure the size of an effect or the importance of a result. Principle 6: By itself, a \\(p\\)-value does not provide a good measure of evidence regarding a model or hypothesis. Yaddanapudi (2016) published a paper in the Journal of Anaesthesiology, Clinical Pharmacology in which she explains each of these six principles for practicing physician-scientists using an example of treatment efficacy for a drug.↩ "],
["internal-validity-evidence-and-random-assignment.html", "Internal Validity Evidence and Random Assignment", " Internal Validity Evidence and Random Assignment Medical researchers may be interested in showing that a drug helps improve people’s health (the cause of improvement is the drug), while educational researchers may be interested in showing a curricular innovation improves students’ learning (the curricular innovation causes improved learning). To attribute a causal relationship, there are three criteria a researcher needs to establish: Temporal Precedence: The cause needs to happen BEFORE the effect. Covariation of the Cause and Effect: There needs to be a correlational relationship between the cause and effect. No Plausible Alternative Explanations: ALL other possible explanations for the effect need to be ruled out. Because of this third criteria, attributing a cause-and-effect relationship is very difficult. (You can read more about each of these criteria at the Web Center for Social Research Methods.) Experimental studies have their strength in meeting this third criteria. To rule out ALL other possible explanations for the effect, the control group and the treatment group need to be “identical” with respect to every possible characteristic (aside from the treatment) that could explain differences. This way the only characteristic that will be different is that the treatment group gets the treatment and the control group doesn’t. If there are differences in the outcome, then it must be attributable to the treatment, because the other possible explanations are ruled out. So, the key is to make the control and treatment groups “identical” when you are forming them. One thing that makes this task (slightly) easier is that they don’t have to be exactly identical, only probabilistically equivalent. This means, for example, that if you were matching groups on age that you don’t need the two groups to have identical age distributions; they would only need to have roughly the same AVERAGE age. Here roughly means “the average ages should be the same within what we expect because of sampling error.” Now we just need to create the groups so that they have, on average, the same characteristics … for EVERY POSSIBLE CHARCTERISTIC that could explain differences in the outcome. Zoinks!13 It turns out that creating probabilistically equivalent groups is a really difficult problem. One method that works pretty well for doing this is to randomly assign participants to the groups. This works best when you have large sample sizes, but even with small sample sizes random assignment has the advantage of at least removing the systematic bias between the two groups (any differences are due to chance and will probably even out between the groups). As Wikipedia’s page on random assignment points out, Random assignment of participants helps to ensure that any differences between and within the groups are not systematic at the outset of the experiment. Thus, any differences between groups recorded at the end of the experiment can be more confidently attributed to the experimental procedures or treatment. … Random assignment does not guarantee that the groups are matched or equivalent. The groups may still differ on some preexisting attribute due to chance. The use of random assignment cannot eliminate this possibility, but it greatly reduces it. Internal validity is the degree to which cause-and-effect inferences are accurate and meaningful. Causal attribution is the goal for many researchers. Thus, by using random assignment we have a pretty high degree of evidence for internal validity; we have a much higher belief in causal inferences. Much like evidence used in a court of law, it is useful to think about validity evidence on a continuum. We will visualize this continuum as a barometer. For example, a barometer visualizing the internal validity evidence for a study that employed random assignment in the design might be: The degree of internal validity evidence is high (in the upper-third). How high depends on other factors such as sample size. To learn more about random assignment, you can read the following: The research report, Random Assignment Evaluation Studies: A Guide for Out-of-School Time Program Practitioners According to Wiktionary the earliest usage of the work “zoinks” was by Norville “Shaggy” Rogers on the show Scooby-Doo.↩ "],
["sampling-variation-and-the-bootstrap-test.html", "Sampling Variation and the Bootstrap Test", " Sampling Variation and the Bootstrap Test In Unit 3, we discovered that, even under the null hypothesis of no group differences, group means from randomized studies vary because of experimental variation. That is, variation in the result occurs because of random assignment. Recall in Unit 2, that the chance variation was a function of the sampling process; different samples drawn from the population (model) produced different results. When results vary because of the sampling process, the chance variation is referred to as sampling variation. Sampling Variation Some designs for group comparisons also are affected by sampling variation. For example, a study design that employs random sampling to obtain observations would inherently be affected by sampling variation. Consider the following study that examined whether baby names are getting shorter over time. The Social Security Administration (SSA) provides historical data on names for every baby born in the United States. Researchers used the population of all names that were included at least five times in the SSA database to randomly sample 25 names from babies born in 1945 and 25 names from babies born in 1995. The length (in letters) of each name was computed, and the two samples were compared. The plot above shows the distribution of name length for the two samples. Babies born in 1995 have shorter names, on average, than babies born in 1945. Is this difference in means is 0.72. Is this difference evidence that baby names are getting shorter over time? In order to answer that question, we need to understand how much variation we expect in differences of means just because of chance. Here, chance is a function of the process of random sampling; note there is no random assignment to groups (year) in these data. Similar to the randomization test, we need to specify a “no difference” model and then simulate from it. But, in the simulation, we need to model the random sampling that was used to generate the data, not randomization to groups. Bootstrapping If we had the larger population of all baby names from 1945 and 1995, we could combine them all and draw two random samples of size 25 from this mega-population; one sample we label “1945”, and the other we label “1995”. (We combine the two populations because the hypothesized model of “no differences” implies that there is really only one population; no difference between the two populations). We could do this many times, each time collecting the difference in mean name lengths between the two samples. By plotting the difference in means, and computing the standard deviation of these differences, we could quantify the amount of variation we expect just because of sampling variation. Unfortunately, we do not have the population of baby names from 1945 and 1995. What we do have is a random sample of those names. So, we are going to combine the names from our two samples to form a “mega-population”. Then, we are going to draw two random samples of size 25 from this “mega-population”. Wait a minute. When we combined our two samples together, our “mega-population” was only 50 names big. If we draw two samples, each of size 25, from this “mega-population”, isn’t that all of the “mega-population”? If we do that, isn’t that the same thing as the randomization test? How does that allow us to model sampling error? After all, the randomization test helps model experimental error. All true. We can, however, model sampling error, with one twist. When we draw our 25 names for eah sample from our “mega-population”, we sample WITH REPLACEMENT. In this way, we mimic drawing random samples from a larger population without actually needing the larger population. It is a really nifty method called bootstrapping developed by Brad Efron in the late 1970’s. Efron’s big discovery was that by bootstrapping (sampling with replacement) from a random sample, a person could come up with a good estimate of the sampling variation. Outline and Goals of Unit 4 The following schematic outlines the course readings, in-class activities, and assignments for Unit 4. In the readings, course activities, and assignments in Unit 4, you will explore the process of modeling sampling variation to be able to evaluate observed differences between groups. You will learn about the bootstrap test (a Monte Carlo method for evaluating whether an observed result in compatible with sampling variation from a hypothesized model) and how to carry out this test using TinkerPlots™. You will also learn why random sampling helps provide validity evidence for generalizing results to the population (external validity evidence). Lastly, you will learn how to evaluate group differences from observational studies. "],
["external-validity-evidence-and-random-sampling.html", "External Validity Evidence and Random Sampling", " External Validity Evidence and Random Sampling In statistical inference, generalization refers to the process of using sample data to draw conclusions about the larger population from which the sample was drawn. The sample data provides statisticians with an estimate of the exact “truth” about the population. For example, data collected from 1,000 Americans about their voting preferences may be used to infer the voting preferences of Americans in general. Statisticians are typically concerned with making inferences about some summary measure of the population, a mean or population percentage. (Some vocabulary: Population summary measures are called parameters. Sample estimates of parameters are referred to as statistics.) How useful is a sample statistic when it comes to estimating a population parameter? Can we draw reasonable inference about a population from sample data? This question is at the heart of weighing evidence about external validity. External validity is the degree to which generalizations to the larger population are accurate and meaningful. There are two statistical aspects we need to consider when we evaluate evidence for external validity: sampling variation and bias. Sampling variation is the idea that statistics from different samples vary. For example, to use the earlier example, different samples of 1,000 Americans would produce different estimates of voting preferences. This variation needs to be accounted for when estimates are given. One way of thinking about sampling variation is to map it to the quality of precision. (We will focus more on this in Unit 5.) The second statistical characteristic we need to attend to is bias. Statistical bias is when sample statistics differ systematically from the population parameter. The key here is the word “systematically”. This implies that there is something in the underlying process (aside from random variation) that is affecting the estimation process. Statistical Bias To help you think about bias, imagine a person, Arthur Dent, has lost his keys. The actual location of the keys, the Library, is akin to the population parameter. Arthur believes he lost his keys at the Supermarket and searches several places around the Supermarket. The locations where Arthur searches are like sample statistics. Figure 5: This figure is a metaphor for statistical bias. Figure 5 is a metaphor for the concept of statistical bias. Arthur’s search locations (sample statistics) are systematically in the wrong place. On average, where Arthur searched (the middle of the yellow circle) is not the actual location of the keys. Compare this with the search locations in Figure 6. Figure 6: This figure is a metaphor for unbiasedness. Figure 6 is a metaphor for unbiasedness. On average, where Arthur searched is the location of the keys. There are a couple of other concepts that this metaphor can help us think about. Even in Figure 6, none of the actual search locations were right at the keys. Some of the locations were too far to the left of the keys, and others were too far to the right of the keys. However, ON AVERAGE, the search locations “found” the keys. The way we define unbiased is that the AVERAGE of the statistics is at the population parameter. Average has nothing to do with the size of the yellow circle. (The size of the circle is related to the amount of sampling variation, a concept we will deal with in Unit 5.) The two figures below also illustrate unbiasedness (left) and bias (right), despite the size of the yellow circle. The last concept about bias to point out is that bias (or unbiasedness) is a property of the sampling method. The reason the search locations were not in the right place is because the method Arthur used to pick the search locations was biased. He thought he lost his keys in the Supermarket, so that is where he looked. An unbiased sampling method is random sampling. Random sampling uses chance to select the sampling units (participants) from the larger population. When random sampling has been employed in a study, the unbiasedness of the sampling method is strong evidence for external validity; we have a much higher belief in generalizations to the larger population. In our validity barometer we would be in the upper-third (depending on other factors such as sample size). In this course, we will discuss and utilize simple random sampling. To draw a simple random sample we need a list of EVERY member of the population. This list is called the sampling frame. (Obtaining a sampling frame can be very difficult. Try obtaining a list of everyone who lives in the United States!) Then we employ randomness to draw out sampling units, with the caveat that each unit in the sampling frame has an equal chance of being drawn. "],
["validity-evidence-and-inferences.html", "Validity Evidence and Inferences", " Validity Evidence and Inferences How subjects are selected from the larger population (sampling) and how those selected subjects are then assigned to experimental conditions (design) both play a large role in the inferences that can be drawn. These two facets are directly related to questions about external validity and internal validity, respectively. Validity is the degree to which inferences and conclusions are meaningful and accurate. It is important that you note that the validation is about the inferences, not the study. (You should never claim a study is valid.) Unfortunately, it is almost impossible to know whether the inferences we draw are valid or not. Therefore, the best we can do is to provide evidence that supports the claims of validity and present that evidence to people. In research studies there are two major types of validity evidence that should be considered: Internal Validity Evidence: This is evidence that supports the drawing of cause-and-effect conclusions. Generally these types of conclusions are only appropriate when a study has employed random assignment. Read more at https://www.socialresearchmethods.net/kb/intval.php. External Validity Evidence: This is evidence that supports the drawing of generalizable conclusions. Generally these types of conclusions are only appropriate when a study has employed random sampling. Read more at https://www.socialresearchmethods.net/kb/external.php. Much like evidence used in a court of law, it is useful to think about validity evidence on a continuum. For example, after reading about the sampling and design of a particular study you might envision the following two “barometers” of the internal and external validity evidence for a study’s inferences: The inferences drawn from this study may generalize, but are probably not causal. As you read research studies (and work on the class activities) you will learn how to situate the yellow dot along these two continuums by considering the sampling and design plans used in the study. Studies of Peanut Allergies Below, you will be presented with excerpts of the research design from three different studies of peanut allergies. The researchers who conducted these studies all used different study designs. After reading the excerpts of each study, try to create the two barometers to situate the internal and external validity evidence. This will help us consider the inferences and conclusions that can be appropriately drawn. Remember there are two primary questions that you should ask when evaluating a study’s design: (1) How were the study participants selected from the population? This helps us think about the degree of external validity evidence, and (2) How were the selected study participants assigned to conditions? This helps us situate the internal validity evidence. Study Design #1 Consider the research design of this study, published in the New England Journal of Medicine.14 Randomized Trial of Peanut Consumption in Infants at Risk for Peanut Allergy To study whether peanut consumption was related to peanut allergies, researchers enrolled 640 infants (aged 4 to 11 months) with severe eczema, egg allergy, or both into the LEAP study. These infants were all enrolled from a single site in the United Kingdom. Participants were stratified into two study cohorts (on the basis of the results of a skin-prick test for peanut allergy) and then participants in each study cohort were randomly assigned to a group in which dietary peanut would be consumed or a group in which its consumption would be avoided. The primary outcome was the proportion of participants with peanut allergy at 60 months of age. Among the 530 infants in one cohort, the prevalence of peanut allergy at 60 months was 13.7% in the avoidance group and 1.9% in the consumption group. The absolute difference in risk of 11.8% represents an 86.1% relative reduction in the prevalence of peanut allergy. In the other cohort (98 infants), the prevalence of peanut allergy was 35.3% in the avoidance group and 10.6% in the consumption group. These findings suggest that high-risk children who consumed peanut products from infancy until they were 5 years old are less likely to develop a peanut allergy than those who avoided peanuts, according to the LEAP randomized trial. When you read about a study design, try to initially identify The population the researchers would like to generalize their results to. The sample used in the study AND how participants were sampled. Any control/treatment groups AND how participants were assigned to these groups. Response/outcome variable AND how this variable was measured. Results and statistical evidence Based on the excerpt, The population the researcher would like to generalize results to encompasses high-risk children who consume peanut products from infancy until they were 5 years old. The sample included 640 infants (aged 4 to 11 months at enrollment) from the United Kingdom. All 640 infants had severe eczema, egg allergy, or both. They were all enrolled at a single site. These infants were NOT randomly sampled. Although the excerpt doesn’t say, they were likely volunteered (by their parents) to be a part of the study. The study employed a cohort design (a replication study) in which infants in each cohort were randomly assigned to treatment (consume a peanut protein–containing bar) or control (avoid peanuts). The first cohort included 540 infants and the replication cohort included 98 infants. The response/outcome being measured is the prevalence of peanut allergy after 60 months of the treatment. In other words, it was the percentage of the treatment (or control) group that had a peanut allergy after 5 years. In the first cohort, the treatment group had a LOWER percentage of infants who developed a peanut allergy than the control group (1.9% vs. 13.7%). This finding was replicated in the replication cohort which found that only 10.6% of the treatment group had a peanut allergy after 60 months versus 35.3% of the control group. No p-values or other inferential evidence are provided. Given this information, what would your validity evidence barometers look like? The level of internal validity evidence is pretty high. The study used random assignment to place the infants in to the control and treatment groups. Furthermore, it replicated this design using a cohort study. Lastly, the sample sizes were pretty large, so the random assignment probably worked pretty well to make the groups statistically identical. Based on all of this, the researchers have a pretty good claim for cause-and-effect about the consumption of peanuts reducing the prevalence of peanut allergies. The degree of external validity evidence, however, is pretty low. Infants were not randomly sampled from the population of high-risk infants. They constitute a convenience sample. This makes it likely that the sample does not represent the population on all characteristics. For example, to volunteer for the study, the infants might have been identified as high-risk at the same medical center. This might mean their parents are of a similar socio-economic status, or live in similar neighborhoods, etc. which may not represent the larger population. This severely limits the generalizability of these results to the larger population of high-risk infants. Study Design #2 Consider the research design of this study, published in the Journal of Allergy and Clinical Immunology.15 Maternal Consumption of Peanut during Pregnancy is Associated with Peanut Sensitization in Atopic Infants To identify factors associated with peanut sensitization, we evaluated 503 infants between 3 to 15 months of age with likely milk or egg allergy but no previous diagnosis of peanut allergy. The infants were enrolled in the study from five sites: Mount Sinai School of Medicine, New York; Duke University Medical Center, Durham, NC; Johns Hopkins University School of Medicine, Baltimore, MD; National Jewish Health, Denver, CO, and the Arkansas Children’s Hospital, Little Rock, AR. The infants’ mothers were queried about the frequency of their peanut ingestion during each trimester of pregnancy, as well as during breastfeeding. The research found that frequent consumption of peanut during pregnancy was strongly associated with peanut sensitization/allergy (p &lt; .001). The also did analyses in which they controlled for infants’ gender and race, two factors thought to be associated with peanut allergies. The analyses indicated that even after controlling for these factors, frequency of peanut consumption during pregnancy was strongly, positively associated with peanut sensitization/allergy (p &lt; .001). Based on the excerpt about this study design, try to identify The population the researchers would like to generalize their results to. The sample used in the study AND how participants were sampled. Any control/treatment groups AND how participants were assigned to these groups. Response/outcome variable AND how this variable was measured. Results and statistical evidence. (Hint: There are several comparisons reported.) Click Here toShow/Hide Responses. The population the researchers would like to generalize their results to is all atopic infants. The sample included 503 atopic infants (aged 3 to 15 months) with no previous peanut allergies who were enrolled from one of the five sites. Although the excerpt doesn’t say, they were likely volunteered (by their parents) to be a part of the study. The study was observational in nature; the researchers did NOT assign the mothers to eat a certain amount of peanuts during their pregnancies. (You will read more about oservational studies in Observational Studies and the Bootstrap Test.) The response/outcome being measured in each comparison is whether or not the infant had a peanut sensitization/allergy. The researchers found statistical evidence of an association between maternal consumption of peanuts during pregnancy and peanut allergies in infants. This association was not very compatible with the hypothesis of no association between the frequency of peanut consumption and peanut allergies (p &lt; .001). This result was also seen after the accounted for gender and racial differences. Given this information, what would your validity evidence barometers look like? Click Here toShow/Hide Barometers. The level of internal validity evidence is low. The study did not use random assignment to assign the frequency of peanuts that mothers would eat during their pregnancies. The researchers improved the level of their internal validity evidence by controlling for infant sex and race. Although this is stronger evidence for cause-and-effect than not controlling for anything, there are still many alternative factors (aside from frequency of maternal peanut ingestion) that might explain the differences in infants’ peanut allergies. The degree of external validity evidence is also pretty low. Infants were not randomly sampled from the population of atopic infants. They weren’t even randomly sampled from the five sites. They constitute a convenience sample. This makes it likely that the sample does not represent the population. (It is likely systematically biased.) This severely limits the generalizability of these results to the larger population of atopic infants. Study Design #3 Lastly, consider the research design of this study, published in the Journal of Allergy and Clinical Immunology.16 Prevalence of Peanut and Tree Nut Allergy in the United States Determined by Means of a Random Digit Dial Telephone Survey: A 5-Year Follow-Up Study To study whether the prevalence of peanut allergy among the general population of the United States has changed over time, researchers contacted 9252 households by telephone in 2002 to conduct a survey about peanut allergies. (These telephone numbers constituted a random sample of phone numbers and excluded nonresidential numbers. Households were called at different times of the day and on different days to optimize contact with a resident. At least 10 attempts were made to contact a resident of each household.) Of the households contacted, 4855 agreed to participate. These households represented a census of 13,493 individuals. The results from this survey were compared to the results from a comparable survey carried out in 1997. These comparisons indicated that the rate of peanut allergy among adults reported in 1997 (0.4%) was not statistically different from the rate reported in 2002 (0.8%). In children the researchers found a difference between the reported rates in 1997 (0.6%) and those in 2002 (1.2%; p = 0.05). The researchers concluded that the rate of peanut allergy among children has doubled from 1997 to 2002. Based on the excerpt about this study design, try to identify The population the researchers would like to generalize their results to. The sample used in the study AND how participants were sampled. Any control/treatment groups AND how participants were assigned to these groups. Response/outcome variable AND how this variable was measured. Results and statistical evidence. (Hint: There are several comparisons reported.) Click Here toShow/Hide Responses. The population the researchers would like to generalize their results to is all household in the United States. The sample included 4855 households. The study was observational in nature; the researchers did NOT assign households to have peanut allergies. The response/outcome being measured in each comparison is the number of adults and children in the household with a peanut allergy. The researchers found statistical evidence of a difference in rates of peanut allergies for children between 1997 and 2002. This difference was not very compatible with the hypothesized model of no difference between rates in 1997 and 2002 (p = .05). Given this information, what would your validity evidence barometers look like? Click Here toShow/Hide Barometers. The level of internal validity evidence is low. Since the researchers are not trying to attribute a causal reason for any differences in peanut allergies, this is not much of a concern. The degree of external validity evidence is also pretty low. Although telephone numbers were initially randomly sampled, many households did not respond. This wrecks the randomness. Also, many people in the U.S. do not have telephones, or are on a no-call list. This severely limits the generalizations. Du Toit, G., et al. (2015). Randomized trial of peanut consumption in infants at risk for peanut allergy. New England Journal of Medicine, 372(9), 803–813.↩ Sicherer, S. H., Wood, R. A., Stablein, D., Lindblad, R., Burks, A. W., Liu, A. H., Jones, S. M., Fleischer, D. M., Leung, D. Y., &amp; Sampson, H. A. (2010). Maternal consumption of peanut during pregnancy is associated with peanut sensitization in atopic infants. Journal of Allergy and Clinical Immunology, 126(6), 1191–1197.↩ Sicherer, S. H., Muñoz-Furlong, A., &amp; Sampson, H. A. (2003). Prevalence of peanut and tree nut allergy in the United States determined by means of a random digit dial telephone survey: A 5-year follow-up study. Journal of Allergy and Clinical Immunology, 112(6), 1203–1207.↩ "],
["observational-studies-and-the-bootstrap-test.html", "Observational Studies and the Bootstrap Test", " Observational Studies and the Bootstrap Test In some studies, researchers do not assign study participants to groups/conditions. One example of this is the second study described in Validity Evidence and Conclusions: Peanut Allergies. In this study, the two groups being compared, mothers that ate peanuts during pregnancy and mothers that did not eat peanuts during pregnancy, were not assigned by the researchers — they “self-selected” into the groups based on whether or not they ate peanuts during pregnancy. When the study participants are not assigned to conditions by a researcher the study is referred to as an observational study. Inferences a researcher can draw from an observational study are much weaker. Typically cause-and-effect conclusions cannot be made based on data collected from observational studies. This is because the possibility of alternative explanations always exists. Drawing causal conclusions from an observational study is misleading and can even be construed as unethical. In 1988, results released to the public from the National Household Survey on Drug Abuse created the false perception that crack cocaine smoking was related to ethnicity. The analysis, which was based on observational data (researchers cannot assign race) showed that the rates of crack use among blacks and Hispanics were twice as high as among whites. The data were re-analyzed in 1992 by researchers from Johns Hopkins University to take into account social factors such as where the users lived and how easily the drug could be obtained. They found that after adjusting for these factors, there were no differences among blacks, Hispanics and whites in the use of crack cocaine. Analyzing Data from Observational Studies Although you probably won’t be able to draw a cause-and-effect inference, it is still useful to study and analyze observational data. Observational studies are often precursors to randomized studies— recall that one criteria of cause is that correlation exists, and observational data is a good for examining correlation. To analyze data from an observational study, researchers use similar methods as they use for data from a study in which the study participants were randomly assigned to groups. The key difference, however, is that the variation in results is not due to experimental variation. It turns out that the variation is more akin to sampling variation. This means we need to adapt our randomization test to account for sampling variation rather than experimental variation. To account for sampling variation in the randomization test, we change replacement option so that the outcome variable is sampled with replacement. Note that the group labels should still be sampled without replacement. (We want to model the same number of participants in each group as in any observational data.) Sampling the outcome values with replacement will increase the variation in the simulated results (larger standard deviation of the mean values). This increased variation is compatible with sampling from a larger population. Sampling with replacement from the observed data is called bootstrapping. Subsequently, this test is referred to as a bootstrap test. The table below shows a comparison of the randomization test and the bootstrap test. Test Variation Modeled TinkerPlots™ Randomization Experimental variation Two sampling devices: (1) outcomes (sampled without replacement); and (2) group labels (sampled without replacement) Bootstrap Sampling variation Two sampling devices: (1) outcomes (sampled with replacement); and (2) group labels (sampled without replacement) "],
["estimating-uncertainty.html", "Estimating Uncertainty", " Estimating Uncertainty Far better an approximate answer to the right question … than an exact answer to the wrong question.17 Aside from hypothesis testing, one of the most common uses of statistical inference is the estimation of unknown parameters using sample data. Polling is one application where statistical estimation is used. For example, Gallup and the Pew Research Center are organizations that use statistical estimation to provide snapshots of public attitudes and opinions on topics from politics and the economy, to social awareness and health and well-being. The results of their polls are seen on a daily basis in almost every newspaper, news blog and website across the world. Statistical estimation is used by more than pollsters. Biologists, social scientists, and medical researchers use statistical estimation to quantify population characteristics. For example, each year the Minnesota Department of Natural Resources estimates the populations of various species of animal, bird, and fish. These estimates are used to help set hunting and fishing regulations, as well as to allocate resources.18 Quantifying Uncertainty: Compatibility Intervals When statisticians report sample estimates, they typically provide the value of the estimate along with the quantification of uncertainty in this estimate. This uncertainty measurement gives us an indication of the “precision” of the estimate. For example, consider again the example of the person who lost his keys. In those pictures, recall, the places where he searched for his keys (the red x’s) represents that sample data. Figure 7: This figure is a metaphor for statistical uncertainty. One estimate that can be made from that data is the “mean” location.19 This location is marked with a big blue dot. This estimate is the one-location best guess about where the actual keys (the population parameter) are located. The yellow area represents the uncertainty; other places where the keys might be aside from the on-location best guess. Interpreting this uncertainty gives him a range of locations where the actual keys might be; the compatibility interval. Quantification of Uncertainty: Margin of Error Compatibility intervals are often reported using the sample estimate (statistic) and a margin of error. For example, consider the following poll results reported in the New York Times on June 30, 2011: As the housing market slumped over the last few years with a speed and magnitude not seen since the Great Depression, aspects of homeownership have been debated as never before. There are tough questions about the role the government should take…includ[ing] how much of a down payment lenders should demand. Whether buyers need to come up with a 20 percent down payment—the standard for decades, but beyond the reach of many families now—is hotly debated. Fifty-eight percent of respondents say lenders should require this, while 36 percent say they should not. The nationwide telephone poll was conducted June 24–28 with 979 adults and has a margin of sampling error of plus or minus three percentage points for all adults. In this article, the percentage of all adults in the United States who believe that lenders should require a 20% down payment on a house was reported as: Sample estimate: 58% Margin of error: 3% Although the compatibility interval isn’t directly reported, it can be computed using the sample estimate and margin of error as, \\[ \\mathrm{Sample~Estimate} \\pm \\mathrm{Margin~of~Error} \\] In this example, \\[ 58\\% \\pm 3\\% = \\left[55\\%,~61\\%\\right] \\] Statisticians refer to this as a compatibility interval because it gives an interval of plausible values for the percentage of all adults in the United States who believe that lenders should require a 20% down payment on a house that are compatible with the observed data. Based on the observed data, the best estimate for the “truth” (the population parameter) is that 58% of all adults in the United States believe that lenders should require a 20% down payment on a house. However, because of the uncertainty associated with random sampling, it may be that anywhere between 55% and 61% of all adults in the United States believe that lenders should require a 20% down payment on a house. In the interpretation above, we associated the amount of uncertainty with variation because of random sampling. It turns out that the margin of error is directly related to the quantification of sampling error. In fact, \\[ \\mathrm{Margin~of~Error} = 2 \\times \\mathrm{Standard~Error} \\] While statistians tend to use two standard errors to compute the uncertainty estimate, this is a somewhat arbitrary choice. Some researchers choose one or three standard errors. 0.0.0.1 What is the Standard Error? The standard error is just a fancy name for the standard deviation when the distribution is composed of statistics (e.g., means, percentages). In other words, the standard deviation of a set of simulated results. That means, in practice, we can use bootstrapping to model the sampling variation, obtain the standard deviation of the bootstrapped results, and use that along with the estimate from the observed data to obtain a compatibility interval. As you interpret compatibility intervals, there are a couple things to keep in mind. The compatibility interval is being used to estimate the population parameter. The compatibility interval gives a range of compatible values for the population parameter. Each value in the range is not equally compatible with the data (Values in the middle of the range are more compatible with the data than values at the ends of the range.) Moreover, the values outside the range are not incompatibile with the data; just far, far less compatible. To further help you understand ideas related to the standard error and the margin of error, we would like you to read Chapter 10 (What is a Margin of Error?) from a short pamphlet put together by the American Statistical Association’s Section on Survey Research. It is available at http://www.prm.nau.edu/prm447/asa%20brochures/margin.pdf. Outline and Goals of Unit 5 The following schematic outlines the course readings, in-class activities, and assignments for Unit 5. In the readings, course activities, and assignments in Unit 5, you will explore the use of bootstrapping to make statistical estimates using TinkerPlots™. You will also learn about how sample size affects the precision/uncertainty of statistical estimates. Lastly, you will learn about the connections between hypothesis testing and estimation. Tukey, J. (1962). The future of data analysis. Annals of Mathematical Statistics 33(1), 1–67.↩ Here is the Wolf Population report for 2016.↩ In two-dimensional space, like on a map, the “mean” is referred to as a centroid.↩ "],
["uncertainty-and-bias.html", "Uncertainty and Bias", " Uncertainty and Bias Since statistical estimation is a method of inference, we need to weigh validity evidence the same way we did for tests of hypotheses. Whether the estimate is a “good” estimate depends on how the sample was drawn. If a biased method of drawing the sample was used, the compatibility interval will likely be biased. Let’s again illustrate using our key example. The figure on the left is a metaphor for the compatibility interval resulting from an unbiased sampling method, such as random sampling. The figure on the right is a metaphor for the compatibility interval resulting from an biased sampling method, such as convenience sampling. Remember the goal of estimation is to give a range of likely values for the population parameter (the keys). If a biased sampling method is used the resulting compatibility interval will likely be wrong (biased). Don’t confuse uncertainty with bias. In our key example, remember the amount of uncertainty is shown by the size of the yellow circle. In the figures above, the uncertainty for both the biased and unbiased sampling methods was the same. Now, consider the following figures. The figure on the left is a metaphor for a compatibility interval that has a small amount of uncertainty (large sample size) resulting from a biased sampling method. The figure on the right is a metaphor for a compatibility interval that has a large amount of uncertainty (small sample size) resulting from an unbiased sampling method. In terms of estimating where the actual keys are, the compatibility interval represented in the right-hand figure would be better. This is an important lesson: Sample size only matters if the sampling method is unbiased. "]
]
